Running evaluation with 1822 queries
Modes: ['dense', 'sparse', 'hybrid', 'openai']
Top-K: 10
Reranker: disabled
Initializing retriever...
2026-02-19 15:11:13 | INFO     | arxiv-rag.supabase | SupabaseClient initialized: https://wfkectgpoifwbgyjslcl.supabase.co...

============================================================
Evaluating mode: DENSE
============================================================

[1/1822] enhancing large language model reasoning capabilities throug...
2026-02-19 15:11:14 | INFO     | arxiv-rag.bge_embedder | Loading BGE-M3 model on cuda...
Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 171897.70it/s]
2026-02-19 15:11:19 | INFO     | arxiv-rag.bge_embedder | BGE-M3 loaded in 5.1s
You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 7494ms

[2/1822] using reinforcement learning to develop emergent self reflec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 173ms

[3/1822] How to implement test-time scaling in large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 123ms

[4/1822] Improving language model reasoning performance on competitio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1165ms

[5/1822] Scaling reinforcement learning for large language models usi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1873ms

[6/1822] How to use long chain of thought training techniques to impr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 614ms

[7/1822] open-source world foundation models for physical AI developm...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 867ms

[8/1822] how to use general-purpose world models for training physica...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 98ms

[9/1822] How can agentic search workflows and reason-in-documents mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 162ms

[10/1822] Improving the trustworthiness of large reasoning models usin...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 766ms

[11/1822] End-to-end native GUI agents that use vision-only screenshot...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 149ms

[12/1822] How does system-2 reasoning and iterative training with refl...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 72ms

[13/1822] challenging multi-modal benchmark for evaluating large langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1001ms

[14/1822] dataset for testing state of the art models on advanced acad...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 855ms

[15/1822] survey of collaboration mechanisms and coordination protocol...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.803 | Time: 521ms

[16/1822] what are the key dimensions and strategies for organizing la...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1449ms

[17/1822] how to effectively use llm-as-a-judge and consensus filterin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1206ms

[18/1822] identifying and mitigating biases in best-of-n evaluation st...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 866ms

[19/1822] improving small language model math reasoning using monte ca...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 701ms

[20/1822] how to scale mathematical reasoning in small models through ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 119ms

[21/1822] autonomous LLM agent framework for end-to-end scientific dis...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 302ms

[22/1822] how to use large language model agents as research assistant...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1041ms

[23/1822] how to reduce the inference overhead and reasoning length of...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 228ms

[24/1822] RL-style fine-tuning for pruning reasoning steps and optimiz...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 172ms

[25/1822] comprehensive survey on reinforcement learning methods for t...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 677ms

[26/1822] the impact of test-time scaling and reinforced reasoning on ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 178ms

[27/1822] survey of autonomous AI agents in retrieval augmented genera...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.935 | Time: 1497ms

[28/1822] how do agentic design patterns like reflection and planning ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 236ms

[29/1822] How to address the optimization trade-off between reconstruc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 199ms

[30/1822] Training efficient Diffusion Transformers using VA-VAE and v...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 187ms

[31/1822] How to address underthinking and frequent thought switching ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 1108ms

[32/1822] Improving the performance of reasoning models by using a dec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 366ms

[33/1822] How can multimodal large language models use visual reasonin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 170ms

[34/1822] Improving spatial reasoning in multimodal models by generati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 206ms

[35/1822] How to apply direct preference optimization to rectified flo...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 213ms

[36/1822] Multi-dimensional video reward models for aligning flow-base...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 228ms

[37/1822] benchmark for evaluating large multimodal models on expert k...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.883 | Time: 190ms

[38/1822] measuring knowledge gain in multimodal learning through perc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 141ms

[39/1822] How does the Qwen2.5-1M model achieve a one million token co...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1067ms

[40/1822] Inference optimization methods for one million token context...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1323ms

[41/1822] benchmarking different large language model steering methods...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 604ms

[42/1822] performance of sparse autoencoders versus rank-1 representat...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 299ms

[43/1822] inference-time steering of diffusion models using Feynman-Ka...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 273ms

[44/1822] how to control diffusion model generation with reward functi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.705 | Time: 206ms

[45/1822] How can large multimodal models maintain performance while c...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 1531ms

[46/1822] Efficient large multimodal model architecture that uses moda...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 1378ms

[47/1822] Applying chain of thought reasoning and step by step verific...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.933 | Time: 184ms

[48/1822] Using the potential assessment reward model and direct prefe...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.938 | Time: 170ms

[49/1822] How does the lightning attention mechanism combined with mix...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1251ms

[50/1822] Scaling large vision-language and text models using efficien...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 204ms

[51/1822] benchmarking expert-level medical reasoning and multimodal u...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1560ms

[52/1822] high difficulty medical question answering dataset with clin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 286ms

[53/1822] How to protect large language models from universal jailbrea...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 120ms

[54/1822] Evaluating the effectiveness of constitutional classifiers a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 566ms

[55/1822] evaluating large language models on their ability to navigat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 588ms

[56/1822] a multi-agent framework using an explore-critic paradigm to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 169ms

[57/1822] How to use 3D tracking videos as control signals in diffusio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1606ms

[58/1822] A unified video diffusion framework that leverages 3D contro...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 161ms

[59/1822] How to implement System 2 reasoning in large language models...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 906ms

[60/1822] Training language models to perform meta-reasoning about the...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1200ms

[61/1822] hierarchical multi-agent framework for mobile task automatio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1210ms

[62/1822] how can mobile agents learn from past experiences using tips...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 271ms

[63/1822] benchmark for evaluating multi-turn conversation capabilitie...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 664ms

[64/1822] realistic multi-turn evaluation for large language models us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 193ms

[65/1822] transformer-based diffusion models for closed-loop autonomou...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1100ms

[66/1822] achieving human-like driving behaviors and joint prediction-...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1243ms

[67/1822] benchmarking large language model performance on competitive...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1195ms

[68/1822] how to measure the reasoning and coding abilities of LLMs th...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 555ms

[69/1822] How does graph-based retrieval-augmented generation solve th...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 494ms

[70/1822] Systematic review of GraphRAG technical foundations includin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 988ms

[71/1822] benchmarks for evaluating expert level reasoning and domain ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 416ms

[72/1822] multimodal foundation model evaluation datasets with human e...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1483ms

[73/1822] how to use process reward models with speculative decoding t...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.761 | Time: 187ms

[74/1822] efficient large language model inference using a draft model...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1148ms

[75/1822] benchmarking vision language models on physical world unders...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 258ms

[76/1822] how to improve physical reasoning in vision language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1566ms

[77/1822] Technical details of omni-modal models using multi-stage tra...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 199ms

[78/1822] How does the Baichuan-Audio-Tokenizer capture semantic and a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1371ms

[79/1822] How to implement a temporally-aware knowledge graph system f...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 351ms

[80/1822] Benchmarking Zep against MemGPT using the Deep Memory Retrie...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 1516ms

[81/1822] human annotated datasets for large language model safety ali...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1494ms

[82/1822] how to train lightweight LLM safety guardrails using a hybri...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1262ms

[83/1822] Multimodal large language models for real-time full-duplex v...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 1057ms

[84/1822] How can multimodal LLMs achieve seamless two-way voice commu...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.626 | Time: 1221ms

[85/1822] How to synthesize agent interaction data using backward cons...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 266ms

[86/1822] Data-centric framework for adapting autonomous agents to dig...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1003ms

[87/1822] how can reinforcement learning and oversampling for increase...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1182ms

[88/1822] strategies for enabling inference scaling in large language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 422ms

[89/1822] How can multiagent systems and independent model specializat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[90/1822] Finetuning language models on synthetic data from multiagent...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 193ms

[91/1822] preference optimization algorithm for Thinking-LLM-as-a-Judg...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1316ms

[92/1822] how does separating evaluation planning from execution in ch...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 253ms

[93/1822] How can reasoning-based supervised fine-tuning and hard samp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 272ms

[94/1822] A large-scale dataset with detailed reasoning steps for trai...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1558ms

[95/1822] Can large language models articulate their own learned behav...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 359ms

[96/1822] Research on behavioral self-awareness in LLMs and whether mo...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 1058ms

[97/1822] How are large language models being used across the differen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1108ms

[98/1822] A comprehensive review of the roles and methodologies of lar...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1308ms

[99/1822] How to use open-source large language models for efficient G...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 1012ms

[100/1822] State of the art open-source LLM framework for SWE-bench Lit...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.680 | Time: 1452ms

[101/1822] reproducing o1 style slow-thinking in multimodal models by f...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 318ms

[102/1822] research on transferring long-form reasoning capabilities fr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 99ms

[103/1822] How does pre-training data influence the emergence of Chain ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 334ms

[104/1822] Survey of large language model architectures and scaling mec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1541ms

[105/1822] How does increasing inference-time compute in reasoning mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 178ms

[106/1822] Investigating the impact of test-time compute scaling on the...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 149ms

[107/1822] How to improve multimodal GUI agents using a two-stage fine-...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 938ms

[108/1822] Developing generalist GUI agents with hierarchical reasoning...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 148ms

[109/1822] open source multi-modal reward models for aligning large vis...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 143ms

[110/1822] using reward models for reinforcement learning and test-time...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 409ms

[111/1822] Recent advancements in large vision-language models for deta...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1593ms

[112/1822] Large multi-modal models that outperform GPT-4o and Gemini 1...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1356ms

[113/1822] How can Monte Carlo Tree Search be integrated with large lan...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 333ms

[114/1822] Improving the exploration of LLM-based heuristic generation ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[115/1822] statistical methods for justifying the replacement of human ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 545ms

[116/1822] how to evaluate if an LLM is a reliable substitute for human...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 455ms

[117/1822] How can large language model agents be designed to support l...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 1129ms

[118/1822] A comprehensive survey of perception, memory, and action mod...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 285ms

[119/1822] Multimodal foundation models for computational pathology pre...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1605ms

[120/1822] How does incorporating transcriptomic data into the pre-trai...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 238ms

[121/1822] training-free test-time alignment of diffusion models using ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1168ms

[122/1822] how to optimize diffusion models for multiple reward objecti...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 447ms

[123/1822] Inference-time alignment techniques for diffusion models usi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 481ms

[124/1822] Guide to reward-guided generation and inference-time guidanc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 271ms

[125/1822] How to use multimodal large language models to convert chart...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 1743ms

[126/1822] Multimodal LLM trained on Chart2Code-160k dataset for genera...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 413ms

[127/1822] how to use dynamic trend representation transformers and cro...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 460ms

[128/1822] fusing dynamic trends and static graph attributes for traffi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 293ms

[129/1822] How can learnable orthogonal and scaling transformations imp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 414ms

[130/1822] Post-training quantization methods using Quantization Space ...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 522ms

[131/1822] Scalable parallel transformer architecture for video diffusi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1088ms

[132/1822] Memory-efficient training frameworks using hybrid parallelis...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 309ms

[133/1822] How can evolutionary search strategies be used to scale infe...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1049ms

[134/1822] Using language models to generate and recombine candidate re...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 528ms

[135/1822] How to improve low-level spatial understanding in vision-lan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1569ms

[136/1822] Recent advancements in training VLA models that integrate se...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 311ms

[137/1822] How can self-play between a conjecturer and a prover LLM imp...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1342ms

[138/1822] Training large language models for formal mathematical verif...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1357ms

[139/1822] self-supervised music representation learning models that ut...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 1044ms

[140/1822] how to train a joint music-text embedding model using contra...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1141ms

[141/1822] How to improve large language model reasoning through test-t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 1079ms

[142/1822] Efficient test-time computation methods for LLMs that levera...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 623ms

[143/1822] How does scaling long chain of thought data to one million s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 130ms

[144/1822] Investigating the impact of long-CoT dataset scaling and rei...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 122ms

[145/1822] How can chain-of-thought reasoning and spatial coordinate al...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 118ms

[146/1822] Advanced spatial reasoning in vision-language models using b...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1543ms

[147/1822] How to improve language model reasoning by training models t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1228ms

[148/1822] Comparison between critique fine-tuning and imitation learni...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1209ms

[149/1822] What are the primary challenges and open problems in using m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1217ms

[150/1822] Investigating the limitations of machine unlearning for cybe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 170ms

[151/1822] How can multimodal large language models use hidden latent s...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 1474ms

[152/1822] Compressing textual reasoning chains into compact thinking t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 1518ms

[153/1822] best practices and data-centric strategies for post-training...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1023ms

[154/1822] detailed implementation of post-training data strategies and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 67ms

[155/1822] benchmarks for evaluating temporal awareness and real-time r...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1182ms

[156/1822] how to evaluate the ability of video LLMs to handle incremen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 284ms

[157/1822] How can large language model agents be trained with reinforc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1601ms

[158/1822] LLM-based autonomous agents for comprehensive literature ret...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 170ms

[159/1822] Are chain of thought explanations in reasoning models like D...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 148ms

[160/1822] Measuring if large language models can accurately describe h...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 182ms

[161/1822] benchmarking negation understanding in vision language model...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.977 | Time: 1419ms

[162/1822] improving CLIP model performance on negated text queries thr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 652ms

[163/1822] multimodal dataset for building damage assessment combining ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.786 | Time: 615ms

[164/1822] globally distributed dataset for training AI models in build...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 286ms

[165/1822] comprehensive survey of parameter-efficient fine-tuning tech...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 121ms

[166/1822] recent developments and systematic review of PEFT methods ac...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1188ms

[167/1822] evaluation framework for assessing the functional correctnes...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 1209ms

[168/1822] multilingual benchmarks for measuring whether AI-generated c...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.899 | Time: 1192ms

[169/1822] How do large language models improve cold-start recommendati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 500ms

[170/1822] Survey of recent advances and future research directions in ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 959ms

[171/1822] How to use Monte Carlo Tree Search and iterative self-traini...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 140ms

[172/1822] Training language model agents to perform self-reflection an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1147ms

[173/1822] How to maintain safety alignment and prevent jailbreaking ri...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 316ms

[174/1822] Parameter efficient fine-tuning methods that preserve safety...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 316ms

[175/1822] Investigating the stability of features extracted by sparse ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 458ms

[176/1822] Do TopK sparse autoencoders identify consistent features acr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 154ms

[177/1822] Scaling vision language model pretraining with massive high ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 131ms

[178/1822] How does progressively scaling SFT data quantity and complex...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 212ms

[179/1822] comprehensive survey of large language models used for autom...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1206ms

[180/1822] recent advancements and challenges in using large language m...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 170ms

[181/1822] Interpretable machine unlearning in diffusion models using s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 142ms

[182/1822] Applying sparse autoencoders to diffusion model activations ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 197ms

[183/1822] interactive benchmark for evaluating multimodal large langua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 542ms

[184/1822] How do state of the art multimodal large language models per...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1837ms

[185/1822] comprehensive architectural framework and modular blueprint ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 260ms

[186/1822] how to implement reasoning language models using process-bas...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1135ms

[187/1822] training-free methods for consistent character identity in t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 207ms

[188/1822] how to achieve consistent identity in diffusion models using...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 317ms

[189/1822] Recent survey on the performance and training techniques of ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1124ms

[190/1822] Comparing the efficiency and scalability of task-specific sm...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1061ms

[191/1822] How to align large language model outputs with human prefere...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 319ms

[192/1822] Techniques for test-time preference optimization that use na...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 354ms

[193/1822] large scale multimodal benchmark for evaluating cultural bia...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 117ms

[194/1822] fine-tuning vision language models on the CultureVerse datas...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 167ms

[195/1822] speculative decoding techniques that use a judge model to ac...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 98ms

[196/1822] how to increase the inference speed of large language models...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 85ms

[197/1822] How can tensor product decomposition be used to reduce KV ca...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 203ms

[198/1822] Efficient attention mechanisms using contextual low-rank rep...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 1308ms

[199/1822] Comparing the impact of conversational XAI interfaces versus...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1164ms

[200/1822] How does integrating large language model agents into conver...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 132ms

[201/1822] How are large language models being applied to automate and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1636ms

[202/1822] A review of current research on using large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1149ms

[203/1822] rehearsal-free class incremental learning using decoupled lo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 333ms

[204/1822] how to achieve a stable stability-plasticity trade-off in co...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 171ms

[205/1822] How to train large language models using FP4 quantization wh...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 1103ms

[206/1822] Techniques for ultra-low precision training of LLMs using di...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1554ms

[207/1822] How can instruction tuning with environment-based self-refin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 91ms

[208/1822] Training LLM agents to correct their own mistakes using envi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 199ms

[209/1822] How does inference-time scaling and extended reasoning chain...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1541ms

[210/1822] Applying journey learning and systematic clinical reasoning ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 950ms

[211/1822] comprehensive benchmark for evaluating large language model ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 692ms

[212/1822] how to assess tool-augmented large language models using cat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 294ms

[213/1822] how can we align time series data with linguistic logic and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 449ms

[214/1822] using dual scale context alignment graph neural networks to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 160ms

[215/1822] benchmarking large language models for factual grounding and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 554ms

[216/1822] automated evaluation methods and leaderboards for testing wh...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1092ms

[217/1822] how to implement retrieval augmented generation over a large...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 212ms

[218/1822] framework for dynamic video retrieval and informative frame ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 251ms

[219/1822] benchmarking multi-turn retrieval augmented generation syste...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1532ms

[220/1822] how do state-of-the-art RAG systems handle multi-turn conver...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 323ms

[221/1822] How can process reward models be used to improve multimodal ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 788ms

[222/1822] Training multimodal large language models for mathematical r...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1194ms

[223/1822] How do large language models use sparse autoencoders to repr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1611ms

[224/1822] Investigation into whether multilingual LLMs encode universa...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 515ms

[225/1822] methods for scaling serial and parallel test-time compute to...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 1177ms

[226/1822] using model-generated test voting and multi-turn selection t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1043ms

[227/1822] Is the performance drop in continual learning of large langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 160ms

[228/1822] How does freezing bottom layers of large language models hel...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 1572ms

[229/1822] comparing the safety and alignment levels of DeepSeek-R1 and...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1031ms

[230/1822] systematic evaluation of DeepSeek-R1 and o3-mini reasoning m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 124ms

[231/1822] techniques for maintaining response diversity in large langu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1517ms

[232/1822] how to optimize language models to generate more diverse per...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1244ms

[233/1822] how to bridge the multilingual performance gap in mathematic...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1299ms

[234/1822] improving Korean language math reasoning capabilities in LLM...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1280ms

[235/1822] How does scaling the input vocabulary size using multi-gram ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1519ms

[236/1822] Investigating the impact of decoupling input and output voca...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 924ms

[237/1822] How can hierarchical memory systems be used to enable traini...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1112ms

[238/1822] Frameworks for real-time video reasoning that use parallel s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 246ms

[239/1822] how does the inconsistency between comprehension and safety ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 1552ms

[240/1822] using shuffle inconsistency and query-based black-box optimi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1218ms

[241/1822] How does the accuracy of frequent ChatGPT users compare to a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1068ms

[242/1822] Can experienced LLM users identify AI-written articles that ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 407ms

[243/1822] Theoretical analysis of gradient descent dynamics and loss t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 292ms

[244/1822] How does the parametrization of key and query weights affect...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 614ms

[245/1822] How does the level of sparsity in mixture-of-experts models ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 79ms

[246/1822] Investigating the optimal balance between computational effi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 81ms

[247/1822] Evaluating large language models on multi-step and constrain...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1007ms

[248/1822] How to measure the performance of LLMs in complex tool use s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 972ms

[249/1822] How can large language model-based generative agents be used...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 142ms

[250/1822] Using LLM-powered learner simulators with memory and reflect...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1640ms

[251/1822] How does global batch load balancing loss improve expert spe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 783ms

[252/1822] Training Mixture-of-Experts models with global frequency syn...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 319ms

[253/1822] Best practices and design considerations for conducting exte...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 1221ms

[254/1822] How can AI developers implement external red teaming framewo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 551ms

[255/1822] Unified generative model for speech and singing voice enhanc...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1461ms

[256/1822] How can masked generative models be used for zero-shot voice...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 146ms

[257/1822] Scalable deep graph neural networks for crystal property pre...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 669ms

[258/1822] Advanced graph neural network models for molecules and mater...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 200ms

[259/1822] How to map hidden knowledge of neural networks into the mult...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 439ms

[260/1822] Scalable method for validating AI models and identifying neu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.982 | Time: 536ms

[261/1822] Methods for training retrieval augmented generation models t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1745ms

[262/1822] Improving RAG performance through test-time compute scaling ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 925ms

[263/1822] comprehensive review of the methods and algorithms used to e...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 755ms

[264/1822] how are multi-turn interactions in large language models eva...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.891 | Time: 698ms

[265/1822] How to use tree search algorithms like MCTS to mitigate hall...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 269ms

[266/1822] Applying dual process theory and slow thinking generation wi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1016ms

[267/1822] collaborative framework for large language models and small ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1322ms

[268/1822] how to integrate cloud-based LLMs with on-device small recom...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1249ms

[269/1822] Best practices for optimizing Retrieval-Augmented Generation...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1481ms

[270/1822] How do factors like document chunk size, retrieval stride, a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 243ms

[271/1822] How can self-updating libraries and task decomposition impro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1172ms

[272/1822] Frameworks for large language models that use dynamic memory...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 642ms

[273/1822] benchmarking text to image diffusion models for toxicity fai...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 268ms

[274/1822] comprehensive safety assessment framework for evaluating bia...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 296ms

[275/1822] Do different large language models produce similar creative ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1004ms

[276/1822] Study comparing the population-level diversity and homogenei...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 438ms

[277/1822] how effective are LLM-based software repair agents at fixing...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 839ms

[278/1822] comparison of agentic program repair performance between ope...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 546ms

[279/1822] benchmark for evaluating large language model hallucinations...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 415ms

[280/1822] new taxonomy for large language model hallucinations disting...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 205ms

[281/1822] how to improve automated feature interpretability in large l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1505ms

[282/1822] natural language descriptions for llm features that capture ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1590ms

[283/1822] How to improve multi-step reasoning in RAG systems using pro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 983ms

[284/1822] Addressing early-step bias in process reward models through ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1195ms

[285/1822] How do advanced second-order optimizers like Self-Scaled BFG...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1124ms

[286/1822] Performance of self-scaled quasi-Newton methods like SSBFGS ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 504ms

[287/1822] How to use adaptive projective gradient descent and shared s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 985ms

[288/1822] Constrained optimization methods for multi-task model mergin...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 794ms

[289/1822] How does displaying AI confidence levels influence human sel...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 977ms

[290/1822] Research on the alignment between artificial intelligence co...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 557ms

[291/1822] Performance comparison between large language models and tra...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 959ms

[292/1822] Analyzing the trade-off between F1-score and inference time ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 357ms

[293/1822] comprehensive review of techniques for distinguishing betwee...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 523ms

[294/1822] comparison of probabilistic methods and ensemble learning fo...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1023ms

[295/1822] discrete diffusion models for multi-task drug discovery usin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 313ms

[296/1822] how to use non-autoregressive bidirectional parallel decodin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 171ms

[297/1822] semi-supervised split learning frameworks for addressing int...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 191ms

[298/1822] how to implement split learning for resource-constrained LEO...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 319ms

[299/1822] How to adapt ConvNeXt architectures for facial emotion recog...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 1461ms

[300/1822] Deep learning frameworks for facial expression recognition t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 437ms

[301/1822] Reducing hallucinations in legal question answering systems ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 1331ms

[302/1822] Benchmarking and evaluating large language model factuality ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 1257ms

[303/1822] How can modified harmful datasets bypass moderation guardrai...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 919ms

[304/1822] Evaluating the effectiveness of guardrail moderation in prev...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 1005ms

[305/1822] How to integrate natural language, algorithmic, and symbolic...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 130ms

[306/1822] Progressive paradigm training strategies for unifying multip...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 177ms

[307/1822] training large language models to adaptively allocate infere...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1095ms

[308/1822] Inference Budget-Constrained Policy Optimization for improvi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1301ms

[309/1822] What are the common limitations and reliability issues of us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1029ms

[310/1822] Frameworks and algorithms for improving the alignment and cr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 792ms

[311/1822] synthetic benchmarks for evaluating the long-context reasoni...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 90ms

[312/1822] how do large language models perform on long-context logical...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1396ms

[313/1822] How can sparse autoencoder features be optimized for precise...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1072ms

[314/1822] Comparison of Feature Guided Activation Additions with Contr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1622ms

[315/1822] how to automatically convert open ended visual question answ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1577ms

[316/1822] agent based framework for generating challenging distractors...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 72ms

[317/1822] How to adapt a small auto-regressive model like Qwen2 for mu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1385ms

[318/1822] What are the most effective training data cleaning and sampl...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 987ms

[319/1822] How can Kolmogorov-Arnold Networks be combined with recurren...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 154ms

[320/1822] Applying learnable temporal spline functions and edge-based ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.793 | Time: 84ms

[321/1822] how to use causal reward modeling and counterfactual invaria...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 1065ms

[322/1822] improving the fairness and reliability of large language mod...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.591 | Time: 414ms

[323/1822] comprehensive review of mitigation strategies for large lang...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 1413ms

[324/1822] recent advancements in responsible AI for enhancing large la...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 914ms

[325/1822] How can matching user queries against pre-generated question...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 598ms

[326/1822] Techniques for reducing information dilution in RAG systems ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 347ms

[327/1822] vision transformer framework using foundation models for ene...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1404ms

[328/1822] automated generation of training data from immunofluorescenc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1377ms

[329/1822] How to prevent attention distribution flattening in long con...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 725ms

[330/1822] Improving length generalization and key information retrieva...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 962ms

[331/1822] How can external document knowledge be integrated directly i...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1273ms

[332/1822] Techniques for parameterizing retrieved knowledge into model...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 667ms

[333/1822] What are the limitations of graph neural networks for solvin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1220ms

[334/1822] Investigating the computational power of message passing GNN...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 496ms

[335/1822] How to implement dynamic workflow adjustment and modular sub...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1300ms

[336/1822] Research on enhancing multi-agent framework performance thro...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1025ms

[337/1822] How can developers build a structured safety case to prove t...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 263ms

[338/1822] Evaluating AI control safety by using red teaming and conser...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 1039ms

[339/1822] Comprehensive survey on explainable artificial intelligence ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 571ms

[340/1822] How can large language models and vision-language frameworks...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.890 | Time: 1336ms

[341/1822] benchmarking the performance of constrained decoding framewo...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 1015ms

[342/1822] how do different constrained decoding tools compare in terms...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 426ms

[343/1822] how to merge multiple deep learning models sequentially with...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 458ms

[344/1822] training-free methods for scalable continual model merging t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 1103ms

[345/1822] How to use audio large language models for natural language-...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 110ms

[346/1822] Alignment approach with LLM distillation for enhancing audio...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 101ms

[347/1822] benchmarking page-level and layout-level retrieval systems f...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 104ms

[348/1822] evaluation of visual retrievers versus text-based retrievers...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 88ms

[349/1822] How to improve multilingual reasoning in large language mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 71ms

[350/1822] Efficient methods for multilingual reasoning alignment that ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 345ms

[351/1822] How can large language models be trained to reason over user...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 793ms

[352/1822] Self-training frameworks for personalized LLMs that utilize ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 151ms

[353/1822] methods for reducing hallucinations in multimodal large lang...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 1075ms

[354/1822] improving multimodal LLM alignment through cross-modal prefe...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.840 | Time: 1156ms

[355/1822] How does Meta use Large Language Models for mutation-guided ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 260ms

[356/1822] LLM-based test generation framework for hardening Android Ko...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 546ms

[357/1822] How to implement a neuro-fuzzy system on an FPGA for real-ti...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[358/1822] FPGA-based intelligent sensor for personalizing time headway...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 1482ms

[359/1822] impact of AWQ and GPTQ low-bit quantization on the mathemati...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 1163ms

[360/1822] effective fine-tuning strategies to restore mathematical rea...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 345ms

[361/1822] systematic review of large language models for natural disas...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 549ms

[362/1822] how are generative artificial intelligence and large languag...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[363/1822] Does OpenAI's o3 model represent true artificial general int...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 702ms

[364/1822] Critique of massive trialling of predefined operations as a ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 949ms

[365/1822] Evaluating implicit sociodemographic bias in large language ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 462ms

[366/1822] Do advanced large language models exhibit greater implicit b...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1078ms

[367/1822] Systematic literature review of large language models in CHI...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 119ms

[368/1822] How are large language models being used as research tools a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 765ms

[369/1822] Evaluating personalized long-form text generation using larg...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 767ms

[370/1822] How to extract atomic aspects and evidence from LLM generate...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 474ms

[371/1822] large scale dataset of high quality science problem solution...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 802ms

[372/1822] automated extraction pipeline for building scientific reason...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 152ms

[373/1822] comprehensive survey of gradient-based multi-objective optim...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 210ms

[374/1822] recent advancements in learning continuous Pareto sets and f...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 998ms

[375/1822] Comprehensive survey of deep reinforcement learning algorith...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1404ms

[376/1822] How does deep reinforcement learning compare to traditional ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1219ms

[377/1822] Survey of model optimization and system architecture strateg...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 707ms

[378/1822] How can cognitive edge computing frameworks balance latency,...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 1281ms

[379/1822] how to generate 360 panoramas using multi-view diffusion mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1319ms

[380/1822] diffusion based method for high resolution 360 degree panora...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1488ms

[381/1822] How to improve concept erasure in diffusion models by dynami...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 96ms

[382/1822] Adaptive Guided Erasure method for selectively removing harm...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 999ms

[383/1822] How does a hybrid attention mechanism combining global and l...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 310ms

[384/1822] Comparison of RoPE, NoPE, and QK-Normalization patterns in t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 1304ms

[385/1822] How to design a multi-agent framework using large language m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 156ms

[386/1822] LLM-based approach for mapping learner goals to skills and o...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[387/1822] how do large language models exhibit conformity bias and gro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1308ms

[388/1822] benchmarking social influence in AI agents using BenchForm t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1407ms

[389/1822] benchmarking framework for evaluating large language model b...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 935ms

[390/1822] holistic platform for testing ai agents using microservice f...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 279ms

[391/1822] How can multimodal prompts like images videos and humming be...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 121ms

[392/1822] A generalized framework for symbolic music generation that u...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 85ms

[393/1822] how to implement training-free visual token pruning for mult...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 377ms

[394/1822] reducing visual redundancy in MLLMs like LLaVA-NeXT through ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 138ms

[395/1822] distributed training of large language models using asynchro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 583ms

[396/1822] how to improve DiLoCo for distributed training by overlappin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 467ms

[397/1822] How can count-based exploration and optimistic reward estima...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 1230ms

[398/1822] A practical algorithm for online RLHF that uses a coin-flip ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.911 | Time: 1205ms

[399/1822] How can large language models be used to identify adversaria...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 755ms

[400/1822] Improving the safety and robustness of autonomous vehicles t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 1573ms

[401/1822] Observational study on how programming students use generati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.957 | Time: 1028ms

[402/1822] How do computer science students interact with large languag...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 709ms

[403/1822] Video reasoning segmentation using multimodal large language...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 76ms

[404/1822] Improving video reasoning segmentation accuracy on ReVOS ben...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 129ms

[405/1822] How can concept activation vectors be used to steer large la...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1174ms

[406/1822] Lightweight framework for granular control of LLM outputs by...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 72ms

[407/1822] How does GPT-4o perform on multimodal physics concept invent...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 244ms

[408/1822] Evaluating the multimodal and multilingual capabilities of l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1043ms

[409/1822] Evaluation of large language models for translating natural ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 892ms

[410/1822] Fine-tuning small language models with distilled high-qualit...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 863ms

[411/1822] Fine-grained complexity analysis of visual autoregressive mo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 355ms

[412/1822] What are the computational limits and efficiency criteria fo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 962ms

[413/1822] best practices and lessons learned from red teaming generati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 719ms

[414/1822] what are the key methodologies and threat model ontologies u...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 531ms

[415/1822] How can large language models be fine-tuned to improve their...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 363ms

[416/1822] Techniques for training large language models to ignore coun...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 714ms

[417/1822] relationship between non-smooth convex optimization theory a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 204ms

[418/1822] how to use optimization theory bounds to transfer optimal le...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 927ms

[419/1822] How can multi-modal large language models be improved for fi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 1172ms

[420/1822] Enhancing fine-grained recognition in MLLMs using contrastiv...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 197ms

[421/1822] enhancing knowledge base question answering through agentic ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 155ms

[422/1822] improving low resource kbqa performance with mcts guided exp...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 260ms

[423/1822] mathematical analysis of transformer layer dynamics using Vl...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 949ms

[424/1822] understanding the evolution of data anisotropy and clusterin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 852ms

[425/1822] adaptive retrieval methods for overcoming bounded recall in ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 276ms

[426/1822] improving retrieval recall by using listwise LLM rankers to ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 1164ms

[427/1822] How can large language model agents be used for goal-driven ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 63ms

[428/1822] Evaluation framework and novel dataset for assessing the qua...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 86ms

[429/1822] The relationship between softmax numerical stability and the...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 275ms

[430/1822] Achieving grokking without regularization by mitigating soft...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 936ms

[431/1822] automated techniques for optimizing prompts to improve test ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 696ms

[432/1822] how to automatically generate model-specific prompts for sof...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 144ms

[433/1822] How can interdisciplinary collaboration between physicists a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 782ms

[434/1822] A roadmap for creating large physics models using foundation...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 146ms

[435/1822] How can small language models achieve high performance in re...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1171ms

[436/1822] Efficient retrieval augmented generation framework for resou...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 749ms

[437/1822] How to reduce error accumulation in online test-time prompt ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 206ms

[438/1822] Techniques for adaptive prompt selection based on prediction...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 185ms

[439/1822] Multi-modal sequential recommendation models using hierarchi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 140ms

[440/1822] How can hierarchical mixture of experts and contrastive lear...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 151ms

[441/1822] A comprehensive review of Mixture of Experts architectures, ...
2026-02-19 15:16:34 | WARNING  | arxiv-rag.retriever | Dense search returned no results
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 139ms

[442/1822] How does the Mixture of Experts framework improve the perfor...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 519ms

[443/1822] comprehensive guide to pre-training generative models and al...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.500 | Time: 391ms

[444/1822] academic reference for understanding the fundamental pillars...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 126ms

[445/1822] training free inference time methods for mitigating hallucin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 133ms

[446/1822] How can contrastive decoding mechanisms and masking signific...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 137ms

[447/1822] How does the extended context window of large language model...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 1223ms

[448/1822] Leveraging long context large language models for text to SQ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 385ms

[449/1822] enhancing the performance of LLM input guardrails through ch...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 911ms

[450/1822] how can fine-tuning large language models as judges with cha...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 281ms

[451/1822] comprehensive review of text data augmentation methods using...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 667ms

[452/1822] current challenges and future opportunities in using generat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1158ms

[453/1822] How can deep learning models combining LSTM and CNN with att...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 83ms

[454/1822] Using synthetic minority over-sampling technique and hybrid ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 878ms

[455/1822] state of the art 8B small language model as a judge for gene...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 891ms

[456/1822] techniques for training small language models for automated ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 297ms

[457/1822] How can self-reflection frameworks and simulated psychologic...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 770ms

[458/1822] Investigating the inconsistency between explicit and implici...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1157ms

[459/1822] How does GPU dynamic voltage and frequency scaling impact th...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 284ms

[460/1822] Analyzing the relationship between input sequence characteri...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 200ms

[461/1822] How can transformers perform full Bayesian inference using i...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 245ms

[462/1822] Comparing transformer in-context learning with Markov Chain ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 1165ms

[463/1822] evaluating the effectiveness and limitations of reinforcemen...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 315ms

[464/1822] hybrid training approaches combining RL and SFT to address r...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 483ms

[465/1822] How to apply speculative sampling techniques to accelerate i...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 1097ms

[466/1822] Speeding up diffusion model generation using speculative dec...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 68ms

[467/1822] How to use synthetic persona data from Persona Hub to improv...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 67ms

[468/1822] Large-scale synthetic data generation strategies for trainin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1085ms

[469/1822] How can temperature sensitivity be used to detect if instruc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[470/1822] Assessing the vulnerability of vision-language models to mem...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 437ms

[471/1822] How has constructionism evolved as an educational framework ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 133ms

[472/1822] Applying constructionist principles to smart education model...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 870ms

[473/1822] agentic workflows for program synthesis using LLM quality ch...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 661ms

[474/1822] how to implement a dynamic multi-agent system for program sy...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[475/1822] How to use hierarchical feature trees and high-level abstrac...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1163ms

[476/1822] Iterative feature tree synthesis framework for generating hi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1148ms

[477/1822] Recent surveys on large vision-language model alignment and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 589ms

[478/1822] What are the primary causes of multimodal misalignment in vi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 881ms

[479/1822] performance comparison of GPT-4o and Claude 3.5 against trad...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 84ms

[480/1822] evaluating the accuracy of large language models for line-by...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 770ms

[481/1822] benchmarking the security and vulnerability of retrieval-aug...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 300ms

[482/1822] how do external knowledge injections and unverified context ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 1062ms

[483/1822] automated extraction of olympiad level math problems from on...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 869ms

[484/1822] benchmarking mathematical reasoning in large language models...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 83ms

[485/1822] benchmarking the performance of large language model agents ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 721ms

[486/1822] standardized evaluation framework for testing the planning a...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.912 | Time: 483ms

[487/1822] benchmarking framework for evaluating the effectiveness of h...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 816ms

[488/1822] how well do modern hate speech detection systems perform aga...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 799ms

[489/1822] comprehensive survey of generative artificial intelligence t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1276ms

[490/1822] how are diffusion models and multimodal AI being applied to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 90ms

[491/1822] How can diffusion priors be used to balance perceptual quali...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1190ms

[492/1822] A unified image restoration model using diffusion priors wit...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 673ms

[493/1822] validated tools and metrics for evaluating the quality and a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 153ms

[494/1822] how to assess the structural and substantive validity of aut...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 390ms

[495/1822] investigating the emergence and formation of localized task ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1112ms

[496/1822] using task vector prompting loss to enhance task representat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1206ms

[497/1822] How can large language models achieve higher knowledge densi...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 1195ms

[498/1822] Machine writing frameworks that simulate human-like cognitiv...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 816ms

[499/1822] How can modified Algorithm-of-Thoughts techniques like AoT+ ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 342ms

[500/1822] Investigating the use of enhanced Algorithm-of-Thoughts fram...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 1502ms

[501/1822] How can multi-agent systems and agentic AI frameworks like O...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 79ms

[502/1822] Evaluation of hallucination mitigation strategies using hier...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.946 | Time: 68ms

[503/1822] How can knowledge distillation and conditional variational a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 980ms

[504/1822] Adaptive diversity distillation techniques for math word pro...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.853 | Time: 847ms

[505/1822] How can large language models be used to generate functional...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 71ms

[506/1822] Leveraging LLMs and design layout graphs to automate the cre...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 1112ms

[507/1822] How well do large language models perform on Allen's interva...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 130ms

[508/1822] Evaluating large language model capabilities in temporal und...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 72ms

[509/1822] segment-level direct preference optimization for improving m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 65ms

[510/1822] how to optimize social agents using segment-based direct pre...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.807 | Time: 995ms

[511/1822] How to improve RAG systems for industrial applications using...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.889 | Time: 1177ms

[512/1822] Techniques for knowledge atomizing and knowledge-aware task ...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 1548ms

[513/1822] How can large language models be integrated into an analytic...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1185ms

[514/1822] A framework for analyzing multi-user XR sessions using a pla...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 520ms

[515/1822] generating diverse and customizable synthetic Q&A pairs for ...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 250ms

[516/1822] a two-stage framework for producing lexically and semantical...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 487ms

[517/1822] How can large language models like fine-tuned BART and BERT ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 108ms

[518/1822] Proactive intrusion prediction framework for IoT security us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 588ms

[519/1822] Evaluating the performance of machine-generated text detecto...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 106ms

[520/1822] Can machine learning models robustly detect AI-generated con...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 250ms

[521/1822] How to improve safety visual reasoning in large vision-langu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 714ms

[522/1822] Reducing attack success rate in vision-language models by ad...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 921ms

[523/1822] How to achieve temporally consistent video relighting using ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 914ms

[524/1822] Frameworks for video relighting that preserve illumination p...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 266ms

[525/1822] How to improve Sharpness-Aware Minimization performance by e...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 631ms

[526/1822] Analysis of SAM training dynamics using third-order stochast...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 778ms

[527/1822] evaluating uncertainty estimation versus self-knowledge for ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 409ms

[528/1822] comprehensive analysis of uncertainty estimation techniques ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 170ms

[529/1822] improving mathematical reasoning in large language models us...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 1028ms

[530/1822] enhancing LLM math capabilities with a first-try strategy an...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 807ms

[531/1822] benchmarks for evaluating long-context language models on co...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.484 | Time: 666ms

[532/1822] how do current large language models perform on tasks requir...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1047ms

[533/1822] How can domain prompts and semantic prototypes be used in a ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 824ms

[534/1822] Diffusion-based time series generation using prototype assig...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 107ms

[535/1822] How can we effectively detect AI-generated text that has bee...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 65ms

[536/1822] A data-centric augmentation approach for building robust mod...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 550ms

[537/1822] How does the presence of citations in large language model r...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 617ms

[538/1822] Experimental research examining if users trust LLM generated...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 392ms

[539/1822] How can retrieval-augmented dynamic prompt tuning improve th...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 88ms

[540/1822] A framework for incomplete multimodal learning using retriev...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 245ms

[541/1822] How can model editing and attention head analysis be used to...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 707ms

[542/1822] Improving large language model robustness by identifying key...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 575ms

[543/1822] evaluating large language models on everyday moral dilemmas ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 706ms

[544/1822] how do large language models perform on complex social ethic...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 676ms

[545/1822] Effective post-training quantization methods for Mamba archi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 937ms

[546/1822] How to use variance aligned rotation and Karhunen-Loeve Tran...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 389ms

[547/1822] How can black-box adversarial attacks disrupt decision-makin...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 733ms

[548/1822] Evaluating the robustness of vision-language models in auton...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.853 | Time: 813ms

[549/1822] How to use optimal transport and Monge-Kantorovich vector ra...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1100ms

[550/1822] Improving conformal prediction for multi-output regression a...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 103ms

[551/1822] How can direct preference optimization be used to personaliz...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 62ms

[552/1822] Aligning diffusion models with multiple individual user rewa...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 861ms

[553/1822] Survey of recent research on how embodiment, symbol groundin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.852 | Time: 671ms

[554/1822] How are researchers addressing the limitations of large lang...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 74ms

[555/1822] academic benchmark for evaluating multi-hop tool use in larg...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 387ms

[556/1822] evaluation dataset for testing complex function calling and ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.679 | Time: 544ms

[557/1822] benchmarking cultural bias towards Western entities in Arabi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 86ms

[558/1822] impact of pre-training data frequency and subword tokenizati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 164ms

[559/1822] How can internal activation steering improve the safety and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 699ms

[560/1822] Methods for revising multimodal model activations during gen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 581ms

[561/1822] How can multi-agent large language model frameworks improve ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 113ms

[562/1822] Implementing multi-agent orchestration for geospatial tasks ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 168ms

[563/1822] How can we design just-in-time and human-verifiable security...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 670ms

[564/1822] A framework for generating contextual security policies for ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 759ms

[565/1822] External safety evaluation results and red teaming findings ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 585ms

[566/1822] Systematic generation of unsafe test inputs to assess the pr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 920ms

[567/1822] flexible and scalable training system for sparse mixture of ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 159ms

[568/1822] optimizing task scheduling and communication overhead in lar...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 563ms

[569/1822] How can retrieval augmented generation and large language mo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 942ms

[570/1822] Implementing RAG-based systems for real-time phone call frau...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.901 | Time: 606ms

[571/1822] benchmarking the performance of multimodal large language mo...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 160ms

[572/1822] evaluating the ability of MLLMs to determine chronological e...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 311ms

[573/1822] Improving quantum machine learning performance by training t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1158ms

[574/1822] End-to-end differentiable framework for learning parameteriz...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1509ms

[575/1822] Best practices and training recipes for domain-adaptive post...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 256ms

[576/1822] How does combining continual pre-training with instruction-f...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 274ms

[577/1822] how to model long-range dependencies in brain networks using...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 745ms

[578/1822] using biased random walks in brain graph transformers to mod...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1139ms

[579/1822] adaptive perturbation methods for mitigating harmful fine-tu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 429ms

[580/1822] how to recover large language model safety alignment after h...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.871 | Time: 129ms

[581/1822] comprehensive benchmark for evaluating undergraduate level m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[582/1822] evaluating large reasoning models using metrics like effecti...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 78ms

[583/1822] How to improve table understanding in language models using ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 149ms

[584/1822] A framework for enhancing table reasoning in LLMs by extract...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 118ms

[585/1822] comprehensive review of foundation models in computational p...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 397ms

[586/1822] what are the current challenges and methodologies for buildi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 547ms

[587/1822] adaptive world model reinforcement learning for autonomous d...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 999ms

[588/1822] how to address distribution shift in world model based plann...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 133ms

[589/1822] A comprehensive review of recent deep learning architectures...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 613ms

[590/1822] How do modern deep learning foundation models categorize the...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 147ms

[591/1822] evaluation of explainability methods for encoder based langu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 171ms

[592/1822] comparative analysis of lime shap and lrp techniques for tra...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 280ms

[593/1822] How do attention modules in different transformer layers con...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 234ms

[594/1822] Analyzing the role of early versus late transformer layers i...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1183ms

[595/1822] evaluating the reliability of large language models as judge...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 508ms

[596/1822] comparing human rater performance with LLM as judge models u...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 840ms

[597/1822] How to generate task-specific LoRA weights using Conditional...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 277ms

[598/1822] Using meta-learning and CVAE generators to produce task-awar...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 665ms

[599/1822] how to use a world knowledge tree and self-reflection refine...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 522ms

[600/1822] framework for scaling supervised fine-tuning data through kn...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 338ms

[601/1822] Challenges and future research directions for integrating mu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 724ms

[602/1822] What are the essential requirements and desiderata for devel...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 721ms

[603/1822] how to use prioritized depth-first search and large language...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 81ms

[604/1822] combining embedding based retrieval with search heuristics t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 146ms

[605/1822] taxonomy of interaction types between software developers an...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 203ms

[606/1822] how do developers interact with generative AI and large lang...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 760ms

[607/1822] A comprehensive analysis and taxonomy of common error types ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 715ms

[608/1822] New methods for efficient error detection and automated repa...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 760ms

[609/1822] comprehensive survey of large language models in bioinformat...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 196ms

[610/1822] what are the current challenges and future directions for us...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 200ms

[611/1822] comprehensive review of test-time compute scaling methods fo...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 259ms

[612/1822] survey on inference time computation techniques such as self...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[613/1822] integrating large language model multi-agent frameworks with...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 1107ms

[614/1822] performance evaluation of Autogen based multi-agent systems ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 267ms

[615/1822] How do large language models respond to malicious jailbreaki...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 904ms

[616/1822] Investigating the impact of using fabricated scientific argu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 741ms

[617/1822] factors influencing high school students' acceptance of gene...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 696ms

[618/1822] research on the role of perceived enjoyment and compatibilit...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 438ms

[619/1822] dataset for evaluating vision language models on handwritten...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 176ms

[620/1822] how well do vision language models perform on reasoning task...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 156ms

[621/1822] adaptive interpolation methods for knowledge distillation to...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 150ms

[622/1822] how to prevent mode collapse and mode averaging in language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 113ms

[623/1822] automated methods for optimizing large language model pretra...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1098ms

[624/1822] how to improve LLM training efficiency with UtiliMax and MED...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 82ms

[625/1822] Diffusion transformer models for joint image and video virtu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 123ms

[626/1822] How to maintain temporal consistency in long video virtual t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.933 | Time: 202ms

[627/1822] how to mitigate systematic misalignment in reinforcement lea...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 525ms

[628/1822] the impact of providing evaluators with simulated future con...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 338ms

[629/1822] How to perform precise free-form grounding across multiple i...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 185ms

[630/1822] Improving multi-image grounding capabilities in MLLMs throug...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 402ms

[631/1822] How can large language models be integrated with high-order ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 155ms

[632/1822] Framework for mixed-type data imputation using bidirectional...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 155ms

[633/1822] How can large language models and CTGANs be used to generate...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 143ms

[634/1822] Evaluating the performance of GPT-based models and CTGAN in ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1087ms

[635/1822] How can optimized soft prompts in the textual embedding spac...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 979ms

[636/1822] Effective safety alignment for diffusion models using catego...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 714ms

[637/1822] using reinforcement learning and representation space guidan...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 806ms

[638/1822] interpretable reinforcement learning methods for LLM jailbre...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.945 | Time: 816ms

[639/1822] how to improve diversity in mixture of experts for low-rank ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 732ms

[640/1822] using the Gram-Schmidt process and Stiefel manifold to enhan...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 625ms

[641/1822] How can hierarchical autoregressive transformers combine cha...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 225ms

[642/1822] Large language models using character-to-word hierarchical a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 116ms

[643/1822] How does the increase in energy loss in the final layer of l...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 630ms

[644/1822] Effective methods for mitigating reward hacking in RLHF by p...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 717ms

[645/1822] certified robustness of large language models using randomiz...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 612ms

[646/1822] how to calculate tight lower bounds for the worst-case robus...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 624ms

[647/1822] How do scaling laws for large language models change when in...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 805ms

[648/1822] Empirical analysis of scaling laws for training differential...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.687 | Time: 821ms

[649/1822] How consistent are large language models like GPT-4 and Clau...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 832ms

[650/1822] Measuring the instability of LLM outputs for legal decision ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 722ms

[651/1822] Assessing the ability of large language models to trace exec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 797ms

[652/1822] Measuring the gap between code generation performance and st...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 688ms

[653/1822] impact of using problem-solving data versus general mathemat...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 887ms

[654/1822] comparing mathematical reasoning performance of LLMs trained...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 565ms

[655/1822] How to modify Chinchilla scaling laws to include inference l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1071ms

[656/1822] Training language models for inference efficiency by co-opti...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1058ms

[657/1822] How to use a hierarchical mixture-of-experts framework to mo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 345ms

[658/1822] Advanced multimodal fake news detection methods focusing on ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 759ms

[659/1822] Evaluating instruction-following large language models for z...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 826ms

[660/1822] How can large language models leverage natural language infe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 564ms

[661/1822] accelerating diffusion model inference by exploiting tempora...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 598ms

[662/1822] how to design a hardware accelerator that leverages temporal...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 189ms

[663/1822] A comprehensive survey of five hundred seventy two code benc...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 610ms

[664/1822] The HOW2BENCH framework and checklists for improving the qua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 406ms

[665/1822] How does complexity control and neuron condensation influenc...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 253ms

[666/1822] Investigating the internal information circuits and complexi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 675ms

[667/1822] using fine-tuned small language models like Llama 3 to gener...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 575ms

[668/1822] evaluating the effectiveness of quantized small language mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 528ms

[669/1822] Evaluating end-to-end spoken language models on knowledge un...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 703ms

[670/1822] A benchmark for assessing the robustness and world knowledge...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1560ms

[671/1822] dynamic self-adaptation of large language models through sin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 769ms

[672/1822] efficient alternative to LoRA for real-time task specific ad...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 690ms

[673/1822] fine-tuning T5-small for scalable and topic-controlled quest...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.798 | Time: 696ms

[674/1822] how to generate semantically aligned and topic-specific ques...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 303ms

[675/1822] Open source TypeScript framework for building autonomous AI ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 511ms

[676/1822] How to integrate large language models with web3 application...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 704ms

[677/1822] Theoretical analysis of prompt optimization as an alternativ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 273ms

[678/1822] How can prompt optimization be formulated as an optimization...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 652ms

[679/1822] adaptive sublayer skipping techniques to accelerate prefilli...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 242ms

[680/1822] how to improve long-context LLM inference efficiency by iden...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 1045ms

[681/1822] Dynamic structured pruning for large language models that ad...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 513ms

[682/1822] Using a sparse mask predictor to dynamically select relevant...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 271ms

[683/1822] efficient large language model tool learning methods using p...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 794ms

[684/1822] how to improve LLM tool calling efficiency by dividing compl...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1125ms

[685/1822] How does in-execution self-debugging using intermediate stat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 709ms

[686/1822] Improving large language model programming performance throu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 438ms

[687/1822] how to implement curriculum learning for large language mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 701ms

[688/1822] dynamic pretraining data selection strategies based on chang...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 128ms

[689/1822] How well do large language models like GPT-4 and Claude perf...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 105ms

[690/1822] Benchmarking different large language models and prompt engi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 668ms

[691/1822] How can we identify and edit specific gender neurons in larg...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 670ms

[692/1822] A method for mitigating gender bias in LLMs through interpre...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 618ms

[693/1822] Strategies for accelerating deep learning inference on resou...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1426ms

[694/1822] A comprehensive review of techniques like pruning, quantizat...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 470ms

[695/1822] How to improve text-to-CAD generation using large language m...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 174ms

[696/1822] Training large language models for CAD model creation throug...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 1099ms

[697/1822] How can internal representations and hidden states of large ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 430ms

[698/1822] Evaluating LLM code generation quality by analyzing latent s...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 252ms

[699/1822] how to measure and detect conversational bias in multi-agent...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.449 | Time: 140ms

[700/1822] evaluating why traditional questionnaire-based bias detectio...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 701ms

[701/1822] enhancing graph retrieval-augmented generation for medical r...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 986ms

[702/1822] improving large language model explainability in high-stakes...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 77ms

[703/1822] How can attackers manipulate voting-based LLM leaderboards l...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.712 | Time: 655ms

[704/1822] Security vulnerabilities and mitigation strategies for crowd...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 619ms

[705/1822] improving mathematical reasoning in large language models by...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 71ms

[706/1822] methods for optimizing the intermediate steps of chain-of-th...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 676ms

[707/1822] critic-free reinforcement learning from human feedback using...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 496ms

[708/1822] comparison of local versus global advantage normalization in...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 441ms

[709/1822] How to implement multimodal large language model multi-agent...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 667ms

[710/1822] Designing no-code frameworks for multimodal multi-agent syst...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1078ms

[711/1822] foundation model for automating systematic reviews through h...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 467ms

[712/1822] performance of large language models compared to clinicians ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 413ms

[713/1822] How to train neural networks with brain-like topographic org...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 724ms

[714/1822] Developing spatially organized artificial neural networks th...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.907 | Time: 632ms

[715/1822] Expert annotated datasets for legal information retrieval an...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 572ms

[716/1822] What are the available benchmarks for evaluating retrieval s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 573ms

[717/1822] decentralized framework for specialized large language model...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 199ms

[718/1822] how can blockchain technology be integrated with fine-tuned ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 140ms

[719/1822] Multi-agent system for question answering using routing and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 143ms

[720/1822] How can routing and planning in multi-agent RAG systems impr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 139ms

[721/1822] How can crowdsourced LLM leaderboards like Chatbot Arena be ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.949 | Time: 568ms

[722/1822] Analysis of the vulnerability of Elo rating systems in large...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 311ms

[723/1822] How to improve the transparency and verification of intermed...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 954ms

[724/1822] Segmenting the chain of thought reasoning process into layer...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 458ms

[725/1822] using crowdsourced metaphors to analyze public perception of...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 254ms

[726/1822] how do open-ended mental models and metaphors predict trust ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 233ms

[727/1822] how to automatically verify the factual accuracy of large la...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 137ms

[728/1822] benchmarking automated systems for fact-checking medical dis...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 108ms

[729/1822] enhancing clinical reasoning in small language models throug...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 78ms

[730/1822] self-evolving framework for medical reasoning using soft dua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 80ms

[731/1822] how to improve the diversity of language model outputs in RL...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 1022ms

[732/1822] techniques for balancing human preference alignment and resp...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 211ms

[733/1822] Synthesizing long-context training data for large language m...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 389ms

[734/1822] How can we effectively train long-context large language mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 452ms

[735/1822] How do next-generation intelligent tutoring systems like Soc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 624ms

[736/1822] Using generative AI and JSON-based prompts to create adaptiv...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 311ms

[737/1822] How to perform hierarchical code summarization for entire so...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 188ms

[738/1822] Improving repository-level software documentation through sy...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 148ms

[739/1822] How can Simulation Theory and task decomposition be used to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 671ms

[740/1822] Enhancing LLM performance on higher-order Theory of Mind tas...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 488ms

[741/1822] How can a taxonomy of user information needs guide the integ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 161ms

[742/1822] Synergies between large language models and knowledge graphs...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 175ms

[743/1822] how to use large language models and constraint logic progra...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 592ms

[744/1822] combining LLMs with logic programs to improve the accuracy a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 175ms

[745/1822] Controllable video generation using blob representations and...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 180ms

[746/1822] Methods for enhancing compositional text-to-video generation...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 205ms

[747/1822] Empirical study evaluating the proficiency of code large lan...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 445ms

[748/1822] How do the internal biases of code LLMs affect their ability...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 170ms

[749/1822] How to achieve faster LLM inference on mobile devices by usi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1050ms

[750/1822] Optimization techniques for on-device large language model i...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 997ms

[751/1822] applying superstatistical methods and q-Gaussian distributio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 164ms

[752/1822] using the Informer transformer model and LightGBM for long-t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1018ms

[753/1822] Multi-agent systems using small language models and retrieva...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 284ms

[754/1822] Using fine-tuned language models and RAG to democratize bioi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 186ms

[755/1822] semi-supervised learning methods for fine-grained action rec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 184ms

[756/1822] how to improve fine-grained action recognition with limited ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 631ms

[757/1822] Scalable graph neural network framework for recommendation u...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 686ms

[758/1822] How to improve the efficiency and robustness of graph based ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 745ms

[759/1822] evaluating the reliability and cultural sensitivity of large...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 218ms

[760/1822] benchmarking value misalignment in open-source large languag...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 170ms

[761/1822] how to improve multimodal large language model performance o...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 658ms

[762/1822] benchmarking numerical reasoning and structure recognition i...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 178ms

[763/1822] How can internal chain of thought reasoning steps in customi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 856ms

[764/1822] Stealthy backdoor attacks on large language models that use ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 196ms

[765/1822] how to improve contextual faithfulness in retrieval-augmente...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 755ms

[766/1822] enhancing faithfulness in long-form question answering by tr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 321ms

[767/1822] How to use language-guided cross-attention mechanisms to pru...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.939 | Time: 1034ms

[768/1822] A simple plug-and-play method for vision token pruning in ML...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 368ms

[769/1822] How to use Transformer models for controllable multitrack MI...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 596ms

[770/1822] Generative music models for computer-assisted composition th...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 102ms

[771/1822] How can structured prompt design and in-context learning tec...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 271ms

[772/1822] Evaluating the effectiveness of general-purpose large langua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 342ms

[773/1822] research on context-aware safety benchmarks for large langua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 132ms

[774/1822] how to evaluate large language model safety by considering c...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 385ms

[775/1822] how to use knowledge graph retrieval augmented generation to...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 120ms

[776/1822] graph based retrieval methods for resolving semantic ambigui...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 142ms

[777/1822] How to measure and reduce redundancy in multi-modality large...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 406ms

[778/1822] Quantitative analysis of redundant test questions and overla...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 612ms

[779/1822] How does changing the reward function shape with an alpha pa...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 573ms

[780/1822] Using AlphaPO to mitigate likelihood displacement and over-o...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 605ms

[781/1822] Recent survey of foundation model based agents capable of co...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 315ms

[782/1822] What are the current research gaps and taxonomies for digita...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 615ms

[783/1822] machine learning models and explainable AI techniques for de...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 267ms

[784/1822] comparison of XGBoost and Random Forest performance against ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 209ms

[785/1822] How can a multi-agent LLM system provide decision interpreta...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 628ms

[786/1822] Improving the reliability of LLM-based RTL code generation t...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 717ms

[787/1822] efficient document compression methods for retrieval augment...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 571ms

[788/1822] how to achieve high compression rates for RAG context window...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 226ms

[789/1822] how does using chain of thought reasoning influence the conf...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.761 | Time: 430ms

[790/1822] impact of providing reasoning steps on the overconfidence of...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 545ms

[791/1822] how can chain of thought prompting be used to generate empat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 193ms

[792/1822] two-stage training approach for speech-based large language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 302ms

[793/1822] evaluating instruction tuning data quality by measuring the ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 488ms

[794/1822] how to improve synthetic dataset integrity by filtering out ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 249ms

[795/1822] Evaluating the performance of causal sequence decoding model...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 355ms

[796/1822] How do language model decoders trained with cross-entropy lo...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 380ms

[797/1822] automatic prompt engineering for multi-step LLM pipelines us...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 509ms

[798/1822] how to optimize complex LLM workflows with feedback-based pr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 405ms

[799/1822] training-free approach to long video understanding using con...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 236ms

[800/1822] how to process arbitrarily long videos in video question ans...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1084ms

[801/1822] How can large language model serving systems achieve both pr...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 633ms

[802/1822] The impact of Deficit Longest Prefix Match and D2LPM on thro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 688ms

[803/1822] using entropy-based selective classifiers to estimate confid...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 235ms

[804/1822] comparing model calibration and error detection performance ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 341ms

[805/1822] how to improve retrieval-augmented generation performance us...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 981ms

[806/1822] impact of multi-granular and self-contained document chunkin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 185ms

[807/1822] Performance comparison of monolingual versus multilingual BE...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 393ms

[808/1822] Introduction of a public UPOS-tagged dataset and evaluation ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 165ms

[809/1822] AI-powered learning platform using Retrieval-Augmented Gener...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.611 | Time: 516ms

[810/1822] How can agentic Large Language Model assistants provide pers...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 148ms

[811/1822] How does the Graph-PReFLexOR framework use graph reasoning a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 628ms

[812/1822] Integrating category theory and knowledge graph growth strat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 586ms

[813/1822] high quality Chinese datasets for large language model pretr...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 157ms

[814/1822] curated Chinese language model training corpora for improvin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 232ms

[815/1822] How to improve many-shot in-context learning performance in ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 496ms

[816/1822] Techniques for addressing performance degradation in many-sh...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.543 | Time: 374ms

[817/1822] How to improve context selection in multimodal RAG using rel...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 303ms

[818/1822] Adaptive context selection and re-ranking methods for improv...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 421ms

[819/1822] benchmarks for evaluating adversarial robustness and composi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 323ms

[820/1822] how to improve the reliability of audio-visual LLMs using ca...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1016ms

[821/1822] How to improve GUI action grounding in novel environments us...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 428ms

[822/1822] Using MLLM based agents and Q-value-Incentive In-Context Rei...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 601ms

[823/1822] impact of learning rates and training data size on the out-o...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 277ms

[824/1822] how to optimize instruction tuning for table tasks while mai...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 405ms

[825/1822] How to use rank-wise mixture of experts in LoRA to improve m...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.571 | Time: 587ms

[826/1822] Efficient parameter fine-tuning for multiple tasks by treati...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 572ms

[827/1822] evaluation datasets for measuring the instruction following ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 441ms

[828/1822] how do multilingual retrieval models perform when given comp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 147ms

[829/1822] Applying social choice theory and maximal lotteries to impro...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.908 | Time: 536ms

[830/1822] How does Nash Learning from Human Feedback approximate maxim...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 167ms

[831/1822] LLM retrieval methods that align complex questions with data...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 157ms

[832/1822] How to use relationship exploration between data objects to ...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 267ms

[833/1822] How can decoder-only LLMs be used for extractive schema link...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 187ms

[834/1822] Improving schema linking accuracy and computational efficien...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.871 | Time: 365ms

[835/1822] graph prompt tuning for heterophily graphs using distributio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 175ms

[836/1822] how to use hop-specific prompts and generalized low-rank ada...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 146ms

[837/1822] How to implement hierarchical backpressure for autoscaling l...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.901 | Time: 588ms

[838/1822] Improving GPU utilization and SLO attainment in LLM inferenc...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 992ms

[839/1822] zero-shot hallucination detection in large language models u...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 597ms

[840/1822] how can attention weights and query categorization be used t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 743ms

[841/1822] How vulnerable is GraphRAG to data poisoning attacks compare...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1005ms

[842/1822] Evaluation of poisoning attacks on knowledge graph based RAG...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 389ms

[843/1822] How to evaluate the coverage of diverse factual information ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[844/1822] Automated framework for measuring information diversity and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 168ms

[845/1822] theoretical framework for contrastive pre-training using app...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 213ms

[846/1822] sample complexity guarantees and joint generative hierarchic...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 491ms

[847/1822] How can retrieval-augmented dialogue knowledge aggregation i...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 180ms

[848/1822] A multi-granularity graph-based approach for aggregating sem...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[849/1822] how can large language models be used for zero-shot and few-...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 557ms

[850/1822] large scale source code authorship identification using a to...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 974ms

[851/1822] Auto-regressive transformer models for graph generation usin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 539ms

[852/1822] How to use pre-trained transformers for graph property predi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 482ms

[853/1822] framework for evaluating multimodal retrieval augmented gene...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.798 | Time: 289ms

[854/1822] measuring the reliability of multimodal RAG systems by asses...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 212ms

[855/1822] detailed training logs and implementation strategies for bui...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 78ms

[856/1822] open source resources and technical documentation for addres...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 211ms

[857/1822] hybrid framework for automated log analysis using uncertaint...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 123ms

[858/1822] how to improve log analysis performance using large language...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 78ms

[859/1822] self-supervised quantization methods for integrating knowled...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 132ms

[860/1822] how to use quantized entity representations to improve large...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 146ms

[861/1822] generalizing the logistic loss function for language modelin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 178ms

[862/1822] evaluating alpha-divergence based loss functions and paralle...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 196ms

[863/1822] How do current large language models perform on tasks evalua...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 183ms

[864/1822] A comprehensive evaluation framework and synthetic benchmark...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 292ms

[865/1822] How do function encoders using least-squares optimization co...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 471ms

[866/1822] A geometric approach to transfer learning characterizing int...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 114ms

[867/1822] How does a block causal transformer architecture improve nex...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 865ms

[868/1822] Foundation models for fluid dynamics using block causal tran...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 455ms

[869/1822] how to obtain valid confidence intervals when using machine ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 526ms

[870/1822] prediction powered inference bootstrap methods for debiasing...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 150ms

[871/1822] How can mixture of experts architectures improve the perform...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 98ms

[872/1822] Using rectified flow pose diffusion and multi-modal LLMs for...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 222ms

[873/1822] using parameter trust regions to mitigate knowledge conflict...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 913ms

[874/1822] training-free techniques for multi-task model merging that p...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 151ms

[875/1822] How to use probabilistic federated search to improve retriev...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 375ms

[876/1822] Improving RAG performance for multi-product QA by aggregatin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 244ms

[877/1822] multilingual dataset for evaluating consistency of large lan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 210ms

[878/1822] methodology for comparing health-related inquiry consistency...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 88ms

[879/1822] How can LLM-based multi-agent systems automate the entire fi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 316ms

[880/1822] A collaborative framework using multiple language model agen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 150ms

[881/1822] How to precisely control camera extrinsic and intrinsic para...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 110ms

[882/1822] Methods for adjusting camera angles and lens distortions in ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 79ms

[883/1822] How can attackers use training loss information from proprie...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 246ms

[884/1822] Vulnerability of Google Gemini models to adversarial prompt ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 93ms

[885/1822] Diffusion models for large scale neural network parameter ge...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 326ms

[886/1822] Techniques for generating full network parameters for vision...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 90ms

[887/1822] How can abductive reasoning be used to infer user personas f...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 538ms

[888/1822] Improving LLM personalization by training on preference data...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 296ms

[889/1822] How can hierarchical attention mechanisms be used to improve...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 108ms

[890/1822] Recent approaches to zero-shot video-to-music generation usi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 454ms

[891/1822] LLM text-to-SQL framework using statistical conformal predic...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 236ms

[892/1822] How can human-in-the-loop and branching point prediction imp...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.525 | Time: 123ms

[893/1822] How can optimal transport-based alignment loss and attention...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 474ms

[894/1822] State-of-the-art methods for integrating visual cues into au...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 197ms

[895/1822] How are large language models being used to detect hate spee...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.944 | Time: 318ms

[896/1822] Recent advancements in using cutting edge language models fo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 202ms

[897/1822] how to generate counterfactual and contrastive explanations ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 165ms

[898/1822] model intrusive methods for interpreting DCNN image classifi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 160ms

[899/1822] How do dimensionality reduction techniques like PCA and UMAP...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 202ms

[900/1822] Analyzing the multidimensional and layer-wise characteristic...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 437ms

[901/1822] how to optimize the scaling factor in low-rank adaptation to...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 417ms

[902/1822] efficient methods for accuracy recovery when fine-tuning pru...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 682ms

[903/1822] Dataset for long-form video understanding instruction tuning...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 187ms

[904/1822] How to improve video large language model performance on lon...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 288ms

[905/1822] evaluation of ChatGPT's ability to generate FEniCS and MATLA...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 933ms

[906/1822] using prompt engineering to implement numerical models for u...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 271ms

[907/1822] comprehensive survey of recent advances in deep learning for...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 311ms

[908/1822] review of foundation models and specialized transformer arch...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 256ms

[909/1822] How can image-to-text conversion and chain-of-thought improv...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.482 | Time: 654ms

[910/1822] Methods to mitigate modality imbalance in vision language mo...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 403ms

[911/1822] How can deep neural decision trees and forests be used to im...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 305ms

[912/1822] A comparative study on using deep neural decision forests an...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 196ms

[913/1822] how to use program-driven verification and dual refinement t...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 800ms

[914/1822] enhancing large language model self-correction using self-ge...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 442ms

[915/1822] How can large language models be integrated with symbolic ac...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 605ms

[916/1822] Using action languages to bridge the gap between natural lan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 529ms

[917/1822] how to improve the robustness of retrieval augmented generat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[918/1822] a training free plug and play framework for filtering malici...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 610ms

[919/1822] How can large language models use iterative self-questioning...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 390ms

[920/1822] State of the art methods for generating chronological news s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[921/1822] How to perform efficient stylized question answering in larg...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 400ms

[922/1822] Lightweight and train-free methods for controlling LLM respo...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 235ms

[923/1822] How can large language models resolve conflicts between edit...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 158ms

[924/1822] A retrieval-based framework for updating large language mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 307ms

[925/1822] How to generate efficient Shapley value explanations for tim...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[926/1822] Time-series transformer architectures that use Shapley-based...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 636ms

[927/1822] speculative decoding for large language models using draft a...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 348ms

[928/1822] how to implement lossless speculative decoding for accelerat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 290ms

[929/1822] enhancing neural theorem proving with large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 268ms

[930/1822] efficient recursive proving algorithms for automated theorem...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 190ms

[931/1822] autonomous red teaming of multi-host networks using large la...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 392ms

[932/1822] evaluating the effectiveness of LLM-based penetration testin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 206ms

[933/1822] dataset for panoptic segmentation-captioning with instance-s...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 164ms

[934/1822] improving region-level comprehension and language generation...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 179ms

[935/1822] how do large language models handle time-sensitive factual k...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 305ms

[936/1822] improving the accuracy and consistency of large language mod...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 138ms

[937/1822] How to evaluate large language models in personalized recomm...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 245ms

[938/1822] Assessing the capability of large language models to capture...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 223ms

[939/1822] applying test-time training to large language models for imp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 361ms

[940/1822] how to improve medical reasoning in LLMs using high quality ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 230ms

[941/1822] training medical patient simulators using dialogue strategie...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 460ms

[942/1822] investigating the relationship between medical inquiry quali...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 558ms

[943/1822] Fine-grained medical vision-language pre-training using larg...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 135ms

[944/1822] How to improve medical image analysis through knowledge inje...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.907 | Time: 320ms

[945/1822] Using LLaVA and multimodal-to-text prompt engineering for id...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 708ms

[946/1822] How can large language models and feature embeddings be appl...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 517ms

[947/1822] evaluating sparse autoencoders for llm interpretability usin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 189ms

[948/1822] how do sparse autoencoders distinguish different meanings of...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 601ms

[949/1822] how to improve self-adaptation in configurable systems using...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 419ms

[950/1822] techniques for continuous configuration optimization in inte...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 327ms

[951/1822] How to use large-scale synthetic data to improve spoken dial...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.876 | Time: 184ms

[952/1822] Multi-turn spoken dialogue systems utilizing heterogeneous f...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 629ms

[953/1822] multi-agent conversational bandit framework for online large...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.624 | Time: 223ms

[954/1822] how to improve online evaluation and filtering of LLM respon...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 213ms

[955/1822] How to evaluate and improve long-context language models for...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 437ms

[956/1822] Improving retrieval performance in long-context LLMs through...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 262ms

[957/1822] comprehensive benchmarks for evaluating multi-modal large la...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 439ms

[958/1822] research on the performance of mainstream multi-modal assist...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 254ms

[959/1822] comprehensive benchmark for evaluating large language models...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 195ms

[960/1822] standardized evaluation framework for testing the performanc...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 183ms

[961/1822] How to use unlearning strategies and layer-level patching to...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 308ms

[962/1822] Defending against LLM jailbreak attacks by identifying vulne...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1661ms

[963/1822] Integrating domain knowledge from large language models with...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 280ms

[964/1822] How can large language models be used to guide Bayesian opti...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 170ms

[965/1822] synthetic data generation using multi-hop reasoning on conte...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 608ms

[966/1822] improving document-level fact checking and grounded factuali...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 450ms

[967/1822] How to implement ultra-low latency deep learning inference o...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 427ms

[968/1822] Optimizing lookup table based neural networks on FPGAs using...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 200ms

[969/1822] Improving Tip-Adapter for vision-language models by incorpor...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 258ms

[970/1822] Theoretical understanding of training-free few-shot CLIP ada...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 351ms

[971/1822] framework for quantifying the extent of model distillation a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 177ms

[972/1822] how to evaluate the degree of knowledge distillation from te...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[973/1822] Improving cross-lingual knowledge consistency in large langu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 199ms

[974/1822] How to use self-consistent responses across different langua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 175ms

[975/1822] How can in-context learning and retrieval-augmented generati...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 222ms

[976/1822] Comparing the performance of few-shot in-context learning an...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 411ms

[977/1822] How to combine slot attention with pre-trained diffusion mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 231ms

[978/1822] Improving compositional image generation and object discover...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 156ms

[979/1822] How effective is fine-tuning large language models for the a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 148ms

[980/1822] Investigating the relationship between reasoning complexity ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 220ms

[981/1822] How can multi-listwise preference optimization be used to im...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 404ms

[982/1822] Finetuning protein large language models for multi-attribute...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 180ms

[983/1822] evaluating the safety and vulnerability of large audio langu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 167ms

[984/1822] benchmarking large audio language models for safety alignmen...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 176ms

[985/1822] learning-based multi-turn jailbreak attack framework for lar...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.913 | Time: 253ms

[986/1822] How can multi-turn red-teaming strategies using turn-level L...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 292ms

[987/1822] What are the primary technical challenges in developing agen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 282ms

[988/1822] How to integrate large language models with neural graph dat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[989/1822] structured prompt engineering frameworks for developing task...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 564ms

[990/1822] using natural language specifications to design conversation...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 210ms

[991/1822] How to use hybrid attention mechanisms and large language mo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 258ms

[992/1822] Academic papers on combining textual statistical features wi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 161ms

[993/1822] How to improve large language model performance by aggregati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 305ms

[994/1822] Fine-tuning techniques for large language models to synthesi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 149ms

[995/1822] Can Looped Transformers perform neural algorithmic reasoning...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[996/1822] Extending the neural algorithmic reasoning capabilities of L...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 348ms

[997/1822] How to improve text-to-video generation models using text em...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 216ms

[998/1822] Enhancing text-to-video synthesis by identifying optimal tex...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.948 | Time: 1254ms

[999/1822] multilingual dataset for hate speech and abusive language de...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 224ms

[1000/1822] benchmarking machine learning models for hate speech classif...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 343ms

[1001/1822] Bridging the gap between instruction tuning and pre-training...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 257ms

[1002/1822] Using adaptive data selection and controlled rewriting of pr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 256ms

[1003/1822] How do padding tokens in text encoders affect the image gene...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.940 | Time: 119ms

[1004/1822] Causal analysis of padding token representations in text-to-...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 84ms

[1005/1822] How does adding random punctuation to mathematical prompts a...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 254ms

[1006/1822] Evaluating the vulnerability of math-specialized large langu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 455ms

[1007/1822] benchmarking multimodal large language models for complex ge...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 63ms

[1008/1822] agent based framework for improving MLLM performance on geol...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.940 | Time: 1128ms

[1009/1822] How can adaptive projector fusion driven by user instruction...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 185ms

[1010/1822] Video large language models using instruction-based dynamic ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 171ms

[1011/1822] how does inter-model response agreement and focal loss impro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 67ms

[1012/1822] improving large language model calibration using auxiliary m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 64ms

[1013/1822] How to implement black-box watermarking for retrieval augmen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 479ms

[1014/1822] Robust knowledge-based watermarking methods for LLM retrieva...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 96ms

[1015/1822] How effective is ChatGPT-4o at generating WCAG compliant web...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 88ms

[1016/1822] Evaluating the utility of large language models in identifyi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 81ms

[1017/1822] How can goal-conditioned reinforcement learning policies tra...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 182ms

[1018/1822] Achieving horizon generalization in RL through planning inva...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 117ms

[1019/1822] How can segment-level reward models improve reinforcement le...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 438ms

[1020/1822] Using dynamic text segmentation and location-aware normalize...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 181ms

[1021/1822] how to integrate AI-driven intrusion detection systems with ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.871 | Time: 133ms

[1022/1822] multilevel defense strategies combining artificial intellige...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 111ms

[1023/1822] how to add early exit branches to pre-trained deep neural ne...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[1024/1822] optimizing the speed accuracy tradeoff in deep learning usin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 235ms

[1025/1822] how to improve the robustness of large language models again...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 113ms

[1026/1822] techniques for maintaining faithful integrity in LLMs to pre...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1192ms

[1027/1822] How can we evaluate the ability of large language models to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 179ms

[1028/1822] Benchmarks and datasets for assessing and improving the mark...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 221ms

[1029/1822] how to improve retrieval augmented generation for scientific...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 200ms

[1030/1822] utilizing contextualized graph representations and dense-spa...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 164ms

[1031/1822] How can 2D Gaussian splatting be integrated with vector quan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1191ms

[1032/1822] Using flexible 2D Gaussian features and splatting operations...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 311ms

[1033/1822] How can overlapping messages in text-based human-AI interact...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[1034/1822] Designing conversational AI systems that support concurrent ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 789ms

[1035/1822] How can neural language models be used to prioritize configu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1184ms

[1036/1822] Techniques for accelerating configuration performance bug te...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1420ms

[1037/1822] synthetic benchmarks for evaluating deductive reasoning in l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 145ms

[1038/1822] how do state of the art reasoning models perform compared to...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 168ms

[1039/1822] flexible modular framework for knowledge graph retrieval aug...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 399ms

[1040/1822] improving knowledge graph retrieval augmented generation by ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 86ms

[1041/1822] evaluating theory of mind in large language models using ver...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 279ms

[1042/1822] a multi-choice question answering benchmark for assessing fi...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 104ms

[1043/1822] interpretable multiple instance learning for whole slide ima...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 101ms

[1044/1822] how to use clinical concepts and vision language models for ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 73ms

[1045/1822] how to map multimodal llm hidden states to interpretable vis...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 892ms

[1046/1822] training free method for steering multimodal large language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 132ms

[1047/1822] How to predict the accuracy of black-box language models by ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 791ms

[1048/1822] Using follow-up query responses to identify misrepresented m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 122ms

[1049/1822] How to adaptively select semantically similar translation de...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 102ms

[1050/1822] Improving neural machine translation in large language model...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 108ms

[1051/1822] How to improve Transformer attention mechanisms by incorpora...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.888 | Time: 403ms

[1052/1822] Efficient fine-tuning of foundation models using sparse GIN-...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 99ms

[1053/1822] How can transformer architectures be optimized for generaliz...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 141ms

[1054/1822] Improving the out-of-distribution generalization of transfor...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 133ms

[1055/1822] How can multimodal large language models be used to help end...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 119ms

[1056/1822] Interactive systems for authoring custom AI vision sensors u...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 845ms

[1057/1822] How can multi-agent large language model frameworks be used ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 298ms

[1058/1822] Framework for implementing collaborative LLM agents with spe...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 770ms

[1059/1822] How do language and text-to-image models exhibit religious b...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 503ms

[1060/1822] Measuring religious stereotypes in generative AI using natur...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 394ms

[1061/1822] benchmarking generative AI models versus traditional ion exc...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 148ms

[1062/1822] how to improve generative materials discovery using post-gen...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 188ms

[1063/1822] large time series models with billion scale parameters incor...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 956ms

[1064/1822] how to use patch convolutional embedding and human feedback ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 372ms

[1065/1822] Hierarchical tree-structured recommendation system using ret...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 171ms

[1066/1822] How can RAG-enhanced hierarchical models improve the accurac...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 297ms

[1067/1822] How to use self-questioning techniques to reduce hallucinati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 136ms

[1068/1822] Multi-round training framework for MLLMs that uses heuristic...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 291ms

[1069/1822] Analyzing phase transitions and emergent capabilities in lar...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 204ms

[1070/1822] How to estimate the internal dimension and parameter suffici...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 163ms

[1071/1822] How can digital twins and ray tracing be used together with ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1169ms

[1072/1822] Using a multi-step tuning process with AI to bridge the gap ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 303ms

[1073/1822] How can large language models be used to automatically gener...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[1074/1822] Automating the transformation of REST APIs into AI compatibl...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.807 | Time: 721ms

[1075/1822] robust nonlinear subspace clustering using data-driven kerne...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 327ms

[1076/1822] how to improve kernel-based subspace clustering by learning ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 523ms

[1077/1822] Evaluation of fine-tuned GPT-4o-mini for cost-effective and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 218ms

[1078/1822] How can fine-tuned large language models improve de-identifi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 208ms

[1079/1822] How to improve document retrieval accuracy using zero-shot r...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 150ms

[1080/1822] Techniques for zero-shot document re-ranking that use pre-tr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 162ms

[1081/1822] How can natural language inference models be improved to rec...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 268ms

[1082/1822] Evaluating the performance of large language models on impli...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 153ms

[1083/1822] Research comparing the emotional variance and sentiment posi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1240ms

[1084/1822] How does the EmoXpt framework analyze differences in sentime...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 172ms

[1085/1822] current research roadmap and challenges for advancing large ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 168ms

[1086/1822] six-layer vision framework for analyzing orchestration and v...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 170ms

[1087/1822] graph-based framework for generating stealthy jailbreak prom...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 512ms

[1088/1822] how can interconnected graph structures with pruning improve...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1125ms

[1089/1822] How can zero-shot large language models and prompt engineeri...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 181ms

[1090/1822] Effective frameworks for using LLMs to evaluate student comp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 707ms

[1091/1822] How does the AIN bilingual multimodal model achieve state-of...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 182ms

[1092/1822] Development of a large multimodal model for Arabic and Engli...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 164ms

[1093/1822] mathematical framework that unifies preference optimization ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 304ms

[1094/1822] impact of optimization objectives and explicit reward models...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 186ms

[1095/1822] how to reduce activation memory during transformer fine-tuni...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 992ms

[1096/1822] efficient fine-tuning methods for large language models that...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 179ms

[1097/1822] Bayes-optimal generalisation error for shallow two-layer neu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 248ms

[1098/1822] Phase transition from universal to specialisation learning i...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.819 | Time: 359ms

[1099/1822] How can RAG and chain-of-thought reasoning be used with larg...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1143ms

[1100/1822] Using RAG-based agentic LLMs and chain-of-thought prompting ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 183ms

[1101/1822] evaluating large language models for longitudinal clinical s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 410ms

[1102/1822] performance of retrieval augmented generation and chain of t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 477ms

[1103/1822] How to improve large language model reasoning accuracy by co...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[1104/1822] Research on formalizing natural language reasoning tasks for...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 541ms

[1105/1822] How to improve resource efficiency in compound AI systems us...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 201ms

[1106/1822] Architectural designs for decoupling orchestration and resou...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 157ms

[1107/1822] Using pre-trained foundational vision transformers like DINO...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 173ms

[1108/1822] Machine learning of material properties from microstructures...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 168ms

[1109/1822] how to perform knowledge distillation from large transformer...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 180ms

[1110/1822] distilling large scale transformer language models into rwkv...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 256ms

[1111/1822] how to trigger hallucinations in multimodal large language m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 218ms

[1112/1822] transferable visual adversarial attacks against multimodal m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 136ms

[1113/1822] how to combine large language models with symbolic solvers u...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 216ms

[1114/1822] neurosymbolic framework for improving large language model p...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 197ms

[1115/1822] survey of AI researchers' opinions on existential risk and t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1046ms

[1116/1822] empirical study on why AI experts disagree about catastrophi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 76ms

[1117/1822] preference datasets and benchmarks for evaluating reward mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 93ms

[1118/1822] how to develop reward models for trustworthy long-context ge...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 80ms

[1119/1822] How can legal concept generation and Determinantal Point Pro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 389ms

[1120/1822] Improving legal document retrieval by augmenting query facts...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 99ms

[1121/1822] AI storytelling system using character symbol manipulation a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 94ms

[1122/1822] How can symbolic motions from toy-playing be used to guide l...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 98ms

[1123/1822] how to optimize large language model serving for multiple se...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 94ms

[1124/1822] efficient LLM inference systems that utilize hardware-aware ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1820ms

[1125/1822] how to use agentic frameworks and character knowledge graphs...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 509ms

[1126/1822] improving the factual accuracy of large language model summa...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 573ms

[1127/1822] How to optimize confidence thresholds in large language mode...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 1113ms

[1128/1822] Probabilistic modeling of joint error distributions to tune ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1027ms

[1129/1822] Efficient LLM serving systems that co-locate latency-sensiti...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 719ms

[1130/1822] How can interference-aware scheduling and latency prediction...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1423ms

[1131/1822] How to reduce communication overhead in distributed large la...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 982ms

[1132/1822] Techniques for overlapping communication with computation in...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 898ms

[1133/1822] instruction tuning for video facial expression captioning an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 341ms

[1134/1822] how can multimodal large language models be trained to provi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 1544ms

[1135/1822] How can multi-neuromodulatory systems like dopamine and nora...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 294ms

[1136/1822] Bio-inspired learning rules using multi-scale neuromodulatio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 770ms

[1137/1822] How to effectively integrate multiple vision encoders in mul...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 130ms

[1138/1822] Advanced fusion strategies for hybrid multimodal large langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 94ms

[1139/1822] What are the current state of the art techniques for early e...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.951 | Time: 129ms

[1140/1822] Comprehensive review of methodologies using intermediate lay...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1641ms

[1141/1822] how to implement remote and automatic cognitive impairment s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 263ms

[1142/1822] performance of large language models like DistilBERT in clas...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 252ms

[1143/1822] How do large language models reflect and amplify stereotypes...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 530ms

[1144/1822] Measuring representational harms and social bias against non...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 156ms

[1145/1822] how to exploit inter-iteration and intra-iteration output sp...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.382 | Time: 182ms

[1146/1822] software-hardware co-design for diffusion models using ffn r...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 141ms

[1147/1822] how to achieve low-bit quantization of text-to-image diffusi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 1379ms

[1148/1822] addressing activation outliers and cross-attention distribut...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 557ms

[1149/1822] How to combine spatial layout information and semantic text ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 395ms

[1150/1822] A hybrid method for document segmentation using bounding box...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 939ms

[1151/1822] improving visual reasoning in large vision-language models u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 216ms

[1152/1822] DrivingVQA dataset for complex visual question answering in ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 187ms

[1153/1822] Using explainable AI and machine learning to distinguish bet...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 211ms

[1154/1822] Differentiating between multiple LLM sources using deep lear...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 225ms

[1155/1822] How can large language models be used for document-level tex...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 234ms

[1156/1822] Improving document simplification performance in large langu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 179ms

[1157/1822] how can we evaluate the clinical appropriateness of conversa...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 160ms

[1158/1822] benchmarking large language model based psychiatric agents t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 239ms

[1159/1822] How can transformer-based models like mT5 and BanglaT5 be ap...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 177ms

[1160/1822] Fine-tuning pre-trained language models for solving Bengali ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 458ms

[1161/1822] How to improve text-image alignment in diffusion-based style...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 159ms

[1162/1822] Recent methods for balancing textual semantics and stylistic...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 443ms

[1163/1822] adversarial attacks on LLM routing systems to manipulate mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 698ms

[1164/1822] how can an adversary use confounder gadgets to compromise th...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 258ms

[1165/1822] A large-scale dataset of cybersecurity-specific prompts for ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 171ms

[1166/1822] Evaluating large language model security using close-ended p...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.829 | Time: 189ms

[1167/1822] How can Large Language Models be used to perform interpretab...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 162ms

[1168/1822] Using adaptive retrieval-augmented generation to bridge soci...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 149ms

[1169/1822] How can retrieval augmented generation and in-context learni...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.571 | Time: 176ms

[1170/1822] A framework for dynamic retrieval of regional cultural value...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[1171/1822] How does information bottleneck theory explain the internal ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 171ms

[1172/1822] Improving large language model reasoning and inference effic...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 982ms

[1173/1822] How does data contamination in pre-training sets affect the ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 171ms

[1174/1822] Controlled study measuring the impact of source and target d...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 224ms

[1175/1822] Why do adaptive optimizers like Adam outperform SGD in trans...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[1176/1822] Analysis of how layer normalization placement affects gradie...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 183ms

[1177/1822] How can Generative Adversarial Networks be used to predict s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 456ms

[1178/1822] Detecting smart grid instability and adversarial attacks usi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 277ms

[1179/1822] how to erase NSFW concepts from text to image diffusion mode...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 352ms

[1180/1822] efficient concept erasure for diffusion models using a seman...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 191ms

[1181/1822] benchmark for evaluating the effectiveness of large language...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.860 | Time: 169ms

[1182/1822] comparing advanced reasoning models and classical LLMs on th...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 168ms

[1183/1822] generative multimodal large language models for explicit sem...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 263ms

[1184/1822] large scale dataset and framework for training event-based v...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.422 | Time: 275ms

[1185/1822] How can we automatically generate high-quality dialogue benc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 160ms

[1186/1822] Using query-based subgraph retrieval and multi-stage LLM pip...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 891ms

[1187/1822] How effective are vision-language models like GPT and Gemini...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 572ms

[1188/1822] A study on using VLMs for the automated assessment of AR sce...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 172ms

[1189/1822] How to use ensemble models and committee voting strategies t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 190ms

[1190/1822] Improving dataset distillation performance by leveraging col...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 259ms

[1191/1822] How to use ChatGPT and tag-based data analysis to generate p...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 225ms

[1192/1822] A method for converting student learning behavior data into ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 278ms

[1193/1822] retrosynthesis prediction using dual graph representations f...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 274ms

[1194/1822] how to improve retrosynthesis prediction by combining dual g...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 169ms

[1195/1822] How can reconfigurable optical circuit switching improve com...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.571 | Time: 741ms

[1196/1822] Improving training cost efficiency of MoE models using a reg...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 154ms

[1197/1822] how to use gpt models and hierarchical summarization for the...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 522ms

[1198/1822] leveraging large language models and prompt engineering for ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 327ms

[1199/1822] automated neural architecture search and compression techniq...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 841ms

[1200/1822] how to design low latency neural networks for bragg peak fin...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 830ms

[1201/1822] How can large language models be used as automated simulator...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 281ms

[1202/1822] Evaluating concept-based explanation methods through automat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 260ms

[1203/1822] How to reduce hallucinations in multimodal large language mo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 93ms

[1204/1822] A post-pretraining method for improving visual representatio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 72ms

[1205/1822] How to use multiple large language models to generate high q...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 78ms

[1206/1822] Improving scientific figure captioning by using multimodal L...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 78ms

[1207/1822] How can we identify and neutralize backdoor trigger tokens i...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 88ms

[1208/1822] Defending against backdoor attacks in natural language proce...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.798 | Time: 89ms

[1209/1822] How do large language models simplify or omit representation...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 155ms

[1210/1822] Evaluating cultural representational gaps and power inequiti...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 758ms

[1211/1822] How can semantic graphs and uncertainty propagation between ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 136ms

[1212/1822] Improving uncertainty-based hallucination detection by model...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 246ms

[1213/1822] how to generate counterfactual examples for natural language...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 95ms

[1214/1822] framework for automatic counterfactual generation using labe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 74ms

[1215/1822] autonomous driving framework using dual-process decision mak...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[1216/1822] improving autonomous vehicle navigation through cognitive pe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 236ms

[1217/1822] how to improve video large language models by integrating mu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 134ms

[1218/1822] leveraging multiple frozen vision backbones to create unifie...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 97ms

[1219/1822] lightweight and explainable intrusion detection systems for ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 97ms

[1220/1822] how to improve transparency and computational efficiency in ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 82ms

[1221/1822] applying compositional diffusion models for 6 degree-of-free...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 63ms

[1222/1822] how can generative diffusion policies be used for few-shot a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 68ms

[1223/1822] How can block-wise mixed format quantization using FP4 diale...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 245ms

[1224/1822] Fine-grained block-level quantization techniques for LLMs th...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 127ms

[1225/1822] evaluating multi-step tool use reasoning in large language m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 106ms

[1226/1822] comparison of process supervised reward models and outcome s...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 99ms

[1227/1822] how to use hadamard rotations to mitigate activation and wei...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 75ms

[1228/1822] low-precision fine-tuning of transformers using hadamard-ass...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 90ms

[1229/1822] How to implement efficient context pruning using sequence la...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 107ms

[1230/1822] A robust approach for combining context reranking and sequen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 84ms

[1231/1822] How can large language models and knowledge graphs be used t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 85ms

[1232/1822] Multi-branched reaction pathway search algorithm for identif...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 86ms

[1233/1822] datasets for training foundation models on linked business t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 108ms

[1234/1822] what datasets are available for research in multi-table repr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 332ms

[1235/1822] hybrid retrieval-augmented generation framework for universi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 66ms

[1236/1822] effective strategies for implementing unified RAG systems in...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 250ms

[1237/1822] How to achieve zero-shot vision to speech generalization in ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 124ms

[1238/1822] Open-source omnimodal models for real-time emotional speech ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 113ms

[1239/1822] How can large language models improve their own critique and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 830ms

[1240/1822] Enhancing LLM self-critique capabilities through self-genera...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 1632ms

[1241/1822] How can large language models improve long-term memory in vo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1249ms

[1242/1822] Enhancing in-car voice assistant memory by using LLMs to ext...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 802ms

[1243/1822] How to use Conditional Value at Risk in reinforcement learni...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 357ms

[1244/1822] Risk-averse fine-tuning methods for large language models to...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 92ms

[1245/1822] How to use large language models for lexicon-based text embe...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 82ms

[1246/1822] Improving sparse or lexicon-based embedding performance on t...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1536ms

[1247/1822] How can domain-adversarial fine-tuning improve the generaliz...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 229ms

[1248/1822] Enhancing small language model chain of thought performance ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 200ms

[1249/1822] using large language models for automated regression test ge...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 88ms

[1250/1822] feedback directed zero shot LLM approach for generating repr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 85ms

[1251/1822] How to address influence score bias in data selection for in...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 420ms

[1252/1822] Balanced and influential data selection techniques for instr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 100ms

[1253/1822] how to use large language models for knowledge graph complet...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 65ms

[1254/1822] automated curriculum modeling and topic extraction from lect...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 214ms

[1255/1822] applying large language models to few-shot multivariate time...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 99ms

[1256/1822] how to leverage pre-trained large language models for few-sh...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 78ms

[1257/1822] Evaluating the effectiveness of open-source and commercial l...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 85ms

[1258/1822] How do large language models compare to human teaching assis...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 102ms

[1259/1822] evaluating the fidelity and constraint satisfaction of large...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 88ms

[1260/1822] systematic framework and error-correction mechanisms for ass...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 89ms

[1261/1822] how can large language models perform zero-shot image and au...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 96ms

[1262/1822] a training-free approach using gradient-free optimization an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 86ms

[1263/1822] methods for separating motion concepts from appearance in te...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 186ms

[1264/1822] using temporal attention purification and appearance highway...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 115ms

[1265/1822] How can chain-of-thought reasoning be applied to improve lar...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 262ms

[1266/1822] Using proactive intent analysis and reasoning steps to enhan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 285ms

[1267/1822] fine-tuning large language models using ensembles of LoRA ex...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 90ms

[1268/1822] how can clustering training data by gradient directions and ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 234ms

[1269/1822] How can trie-based prefix tree structures be used to optimiz...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 282ms

[1270/1822] Efficient beam search algorithms for LLMs that use shared pr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 88ms

[1271/1822] how effective are large language models at extracting specie...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 117ms

[1272/1822] using large language models for automated extraction of spec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 151ms

[1273/1822] limitations of current large language model cybersecurity ev...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 125ms

[1274/1822] comprehensive risk assessment framework for LLM cyber capabi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 315ms

[1275/1822] How do one-layer attention-only transformers develop interna...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 117ms

[1276/1822] Understanding how training data features shape the internal ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 115ms

[1277/1822] How to use preference optimization to intrinsically reduce h...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.807 | Time: 116ms

[1278/1822] Improving the reliability of LLM machine translation by fine...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 144ms

[1279/1822] comparing the performance of LSTM deep learning models and A...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 122ms

[1280/1822] how do Long Short-Term Memory networks compare to traditiona...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 370ms

[1281/1822] How does the integration of generative AI tools and experien...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 231ms

[1282/1822] Research on human-AI collaboration in business education foc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 226ms

[1283/1822] How can multimodal large language models perform GUI groundi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 110ms

[1284/1822] State of the art methods for visual GUI agent grounding and ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 248ms

[1285/1822] How to improve the safety of large language model reward mod...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 154ms

[1286/1822] Dynamic selection of the most important safety rules to maxi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 201ms

[1287/1822] How to optimize the order of in-context learning examples fo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 141ms

[1288/1822] Dataset-free methods for finding the best sequence of few-sh...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 197ms

[1289/1822] systematic hyperparameter tuning for LLM-as-a-judge using mu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 182ms

[1290/1822] how to find cost-efficient open-weight LLM judges through mu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 803ms

[1291/1822] how to integrate domain specific large language models into ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 220ms

[1292/1822] leveraging a suite of general and specialized LLMs within an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 170ms

[1293/1822] Evaluating large language models and artificial intelligence...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 192ms

[1294/1822] Implications of AI personhood and moral status for the ethic...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 161ms

[1295/1822] how to improve large language model reasoning and explainabi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 158ms

[1296/1822] using selective tree exploration and supervised fine tuning ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 146ms

[1297/1822] how to perform open-set test-time adaptation for multimodal ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 521ms

[1298/1822] robust multimodal test-time adaptation methods for identifyi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 456ms

[1299/1822] evaluating the correlation between cross-attention weights i...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 158ms

[1300/1822] investigating the plausibility of using cross-attention weig...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 179ms

[1301/1822] How can specific attention heads in LLMs be used for trainin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 193ms

[1302/1822] Techniques for improving large language model performance on...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.907 | Time: 784ms

[1303/1822] How can Mixture-of-Experts models be improved by removing ro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 155ms

[1304/1822] Research on expert self-selection mechanisms in language mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 239ms

[1305/1822] How to evaluate fact-conflicting hallucinations in small lan...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.845 | Time: 185ms

[1306/1822] Benchmarking the factual analysis and context reasoning capa...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 158ms

[1307/1822] using direct preference optimization to generate multiple ch...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 158ms

[1308/1822] training a model to predict student choices for generating m...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 157ms

[1309/1822] How can large language models be jailbroken using positive s...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 163ms

[1310/1822] Effective jailbreak attack methods for large language models...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.578 | Time: 165ms

[1311/1822] benchmark for text-driven image editing evaluation with huma...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 177ms

[1312/1822] multi-modality source-aware quality assessment metric for ev...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 212ms

[1313/1822] How can Pointwise V-Information based fine-tuning improve th...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 149ms

[1314/1822] A comprehensive dataset and evaluation framework for trainin...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 182ms

[1315/1822] Scalable signature-based algorithm for path-dependent hedgin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 474ms

[1316/1822] Mathematical framework for deep hedging alternatives using u...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 771ms

[1317/1822] alternative memory architectures for AI inference that optim...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 516ms

[1318/1822] how does managed-retention memory improve energy efficiency ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 718ms

[1319/1822] How to improve short text classification using graph learnin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 278ms

[1320/1822] Novel methods for addressing semantic sparsity in short text...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 522ms

[1321/1822] How to use class-specific visual prompts to improve the inte...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 110ms

[1322/1822] Improving saliency maps in Vision Transformers for fine-grai...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.422 | Time: 104ms

[1323/1822] How can an autonomous multi-agent framework using dynamic ro...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 113ms

[1324/1822] Large language model based multi-agent systems for psycholog...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 101ms

[1325/1822] iterative reinforced fine tuning using Monte Carlo Tree Sear...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 108ms

[1326/1822] addressing fragment deficiency and parameter errors in LLM t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 299ms

[1327/1822] how to use large language models and graph embedding techniq...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 106ms

[1328/1822] fine-tuning pre-trained large language models with graph and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 348ms

[1329/1822] architectural methods for overlapping communication and comp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 81ms

[1330/1822] improving distributed transformer inference speed by decoupl...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 663ms

[1331/1822] how to improve large language model performance on complex c...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[1332/1822] effective agentic frameworks for overcoming reasoning and lo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 312ms

[1333/1822] How do large language models perform in detecting smart cont...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 110ms

[1334/1822] Reducing false positive rates in LLM-based smart contract se...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 116ms

[1335/1822] How to use model weight averaging during sequential fine-tun...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 223ms

[1336/1822] Mitigating forgetting in diverse domain continual learning b...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 102ms

[1337/1822] How can influence functions be used to identify labeler bias...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 101ms

[1338/1822] Efficient methods for measuring the impact of specific human...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 274ms

[1339/1822] comparing the performance of large language models and encod...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 91ms

[1340/1822] how well do large language models perform at segment-level q...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 83ms

[1341/1822] comprehensive evaluation framework for assessing large langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 91ms

[1342/1822] how to benchmark financial large language models on professi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 121ms

[1343/1822] systematic mapping study on ethical risks and mitigation str...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 85ms

[1344/1822] current challenges and frameworks for mitigating ethical con...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[1345/1822] Open source Python library for evaluating bias and fairness ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 129ms

[1346/1822] How can I assess algorithmic bias in LLM responses using an ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 123ms

[1347/1822] How to use visual Hopfield networks and associative memory t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 97ms

[1348/1822] Improving LLM-based medical report generation by mining dise...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 165ms

[1349/1822] Using zero-shot prompting with open-source large language mo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 171ms

[1350/1822] Evaluation of large language models for scoring transcribed ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 123ms

[1351/1822] How to improve the inference efficiency of LLM-based recomme...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 605ms

[1352/1822] Optimizing accuracy and latency in large language model reco...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 119ms

[1353/1822] How can explicit visual prompts like markers and pointers be...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 95ms

[1354/1822] Framework for integrating medical entity extraction and visu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 194ms

[1355/1822] How can model predictive control frameworks improve the plan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 828ms

[1356/1822] Using large language models as implicit cost function minimi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 161ms

[1357/1822] How does transfer learning work in variational quantum circu...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 159ms

[1358/1822] Analytical fine-tuning methods for adapting pretrained varia...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 487ms

[1359/1822] benchmarks for evaluating multimodal large language models o...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 163ms

[1360/1822] large scale dataset for cross-event and within-event reasoni...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 155ms

[1361/1822] How can image-based multimodal large language models be used...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 156ms

[1362/1822] Techniques for improving the black-box transferability of ad...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 149ms

[1363/1822] comprehensive review of methodologies for applying large lan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 135ms

[1364/1822] what are the differences between graph2text and graph2token ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 148ms

[1365/1822] How can token-level uncertainty and attention mechanisms be ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 140ms

[1366/1822] Lightweight decoding strategies for improving faithfulness t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 150ms

[1367/1822] impact of explicit symmetry breaking and geometric reference...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 149ms

[1368/1822] benchmarking trade-offs between group equivariance and symme...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 392ms

[1369/1822] How can the IDADP framework help large language models detec...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 150ms

[1370/1822] Effective prompting strategies for zero-shot irony comprehen...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 146ms

[1371/1822] How do large language models compare to humans in open-ended...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 142ms

[1372/1822] Analyzing the limitations of LLM exploration through sparse ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.853 | Time: 145ms

[1373/1822] Systematization of knowledge on security vulnerabilities and...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 145ms

[1374/1822] A comprehensive study on the risks of code hallucinations an...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 1226ms

[1375/1822] evaluating the fairness and robustness of commercial AI safe...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 170ms

[1376/1822] how do safety moderation classifiers used as LLM guardrails ...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.389 | Time: 162ms

[1377/1822] how to improve multimodal language models for mental health ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[1378/1822] multimodal framework for fine grained anxiety symptom detect...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 572ms

[1379/1822] enhancing retrieval augmented generation for complex reasoni...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 231ms

[1380/1822] how to improve agentic RAG performance with self-consistency...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[1381/1822] how can multimodal large language models use image represent...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 70ms

[1382/1822] using spatial intelligence of multimodal LLMs and visual gra...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 153ms

[1383/1822] how to use large language models and retrieval augmented gen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 405ms

[1384/1822] enhancing automated short answer grading reliability with RA...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 84ms

[1385/1822] How to improve membership inference attack accuracy in large...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 101ms

[1386/1822] Detecting pretraining data leakage in large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 83ms

[1387/1822] How can lightweight models be used for evidence extraction t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 103ms

[1388/1822] Comparison of quote-first-then-answer strategies versus full...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 118ms

[1389/1822] meta-learning for parameter initialization in variational qu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 85ms

[1390/1822] how to apply MAML-based classical neural networks for findin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 773ms

[1391/1822] LLM-based framework for automated scientific research using ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 692ms

[1392/1822] How to automate the scientific research process through iter...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 83ms

[1393/1822] CharToM benchmark for evaluating theory of mind in large lan...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 305ms

[1394/1822] Comparing human and AI theory of mind performance when reaso...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 87ms

[1395/1822] Theoretical analysis of diffusion models for nonparametric d...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 98ms

[1396/1822] How do sparse weight-sharing neural network architectures in...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 120ms

[1397/1822] Improving large language model reasoning performance through...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 88ms

[1398/1822] How can recursive sub-task breakdown and advanced scoring me...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 688ms

[1399/1822] Implementation of Retrieval-Augmented Generation using BGE-M...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 296ms

[1400/1822] Evaluating localized RAG systems for data privacy and perfor...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 117ms

[1401/1822] iterative pruning methods for diffusion models using gradien...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 98ms

[1402/1822] how to apply progressive soft pruning and gradient flow crit...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 219ms

[1403/1822] large scale dataset and benchmark for fake news detection in...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 98ms

[1404/1822] performance of large language models with quantized low rank...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 90ms

[1405/1822] evaluating large language models on aerospace manufacturing ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 363ms

[1406/1822] assessing the accuracy and hallucination risks of GPT-4 and ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 435ms

[1407/1822] Improving multimodal hierarchical classification accuracy by...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 519ms

[1408/1822] How to integrate hierarchical class relationships into multi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 158ms

[1409/1822] evaluating the performance of large language models for ment...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 155ms

[1410/1822] benchmarking multilingual and bilingual LLMs on Arabic menta...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 146ms

[1411/1822] How to use a proximal operator and local correlation regular...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 142ms

[1412/1822] Efficient methods for inducing structured 2:4 sparsity in ne...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 411ms

[1413/1822] How do task-in-prompt adversarial attacks use sequence-to-se...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 150ms

[1414/1822] Evaluating LLM jailbreak vulnerabilities using the PHRYGE be...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 664ms

[1415/1822] consensus-based optimization methods for derivative-free non...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 171ms

[1416/1822] how to apply mirror maps and bregman distances to particle-b...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.867 | Time: 222ms

[1417/1822] How to use open-source large language models for automatic d...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 133ms

[1418/1822] Improving zero-shot classification performance of open-sourc...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.913 | Time: 163ms

[1419/1822] How can privacy guardrails like OneShield be used to detect ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 160ms

[1420/1822] Effective frameworks for mitigating privacy risks in enterpr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 152ms

[1421/1822] How can specialized datasets like PlanGTG with reordering an...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 360ms

[1422/1822] Improving graph to text generation in large language models ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 156ms

[1423/1822] How to use Z-order curves and dimensionality reduction to en...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 185ms

[1424/1822] Efficient top-k attention mechanisms for causal transformers...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 160ms

[1425/1822] How can semantic analysis and natural language processing be...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 373ms

[1426/1822] A computational architecture for linking systematic analytic...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1231ms

[1427/1822] Scaling in-context reinforcement learning for cross-domain a...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 162ms

[1428/1822] How does algorithm distillation compare to expert distillati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 224ms

[1429/1822] generating realistic limit order book simulations using tran...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 286ms

[1430/1822] how to evaluate the realism of synthetic market data generat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 303ms

[1431/1822] How to use embedding-based data perturbation and Tsetlin Mac...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.833 | Time: 109ms

[1432/1822] Improving adversarial attacks on text detectors by combining...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 658ms

[1433/1822] convergence rate of probability flow ODE samplers in diffusi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 98ms

[1434/1822] how do probability flow ODEs in score-based generative model...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 103ms

[1435/1822] Automated black-box safety testing of large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 71ms

[1436/1822] How can LLMs be used as test oracles to identify harmful res...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 91ms

[1437/1822] How can hierarchical contrastive learning and concept memori...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.571 | Time: 88ms

[1438/1822] Improving the accuracy of generative language models in pred...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 555ms

[1439/1822] information theoretic framework for multi-bit watermarking i...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 257ms

[1440/1822] optimal schemes for distributional information embedding to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 137ms

[1441/1822] Can fine-tuned large language models effectively control spa...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 356ms

[1442/1822] Comparison of fine-tuned foundation models and traditional d...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 96ms

[1443/1822] hybrid machine learning and biophysical models for predictin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 427ms

[1444/1822] combining neural networks with physiological models to impro...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 836ms

[1445/1822] How to finetune large language models using a diffusion fram...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 170ms

[1446/1822] Integrating diffusion processes into autoregressive models f...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 146ms

[1447/1822] datasets and benchmarks for fine grained span level chinese ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 150ms

[1448/1822] evaluating the performance of large language models on ident...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 145ms

[1449/1822] how to apply negative feedback mechanisms from control theor...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 223ms

[1450/1822] efficient weight-only quantization for large language models...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 160ms

[1451/1822] comprehensive survey of deep learning architectures and thei...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 313ms

[1452/1822] how neural networks like CNNs and autoencoders are used for ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 161ms

[1453/1822] machine learning based qubit readout workflow using QICK and...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 168ms

[1454/1822] how to implement hardware efficient neural networks for sing...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 194ms

[1455/1822] Causal pre-processing methods for resolving the fairness-acc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 1220ms

[1456/1822] How can approximating a fictitious and normatively desired w...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 153ms

[1457/1822] Improving process reward modeling for mathematical reasoning...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 590ms

[1458/1822] How does hierarchical refinement and step merging affect the...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 150ms

[1459/1822] efficient methods for mitigating catastrophic forgetting in ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 274ms

[1460/1822] how to balance domain adaptation and general knowledge prese...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 340ms

[1461/1822] Does professional artistic expertise improve the quality of ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 172ms

[1462/1822] Experimental study on the transfer of traditional artistic s...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.924 | Time: 144ms

[1463/1822] How can generative large language models be used for ranking...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 298ms

[1464/1822] Unified framework for multi-modal question answering that co...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 164ms

[1465/1822] evaluating large language models mathematical reasoning capa...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 158ms

[1466/1822] how does randomizing variables in math word problems help de...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 148ms

[1467/1822] evaluation of large language models on their ability to tran...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 147ms

[1468/1822] do current automatic evaluation metrics like BLEU and COMET ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.876 | Time: 177ms

[1469/1822] Multi-agent deep reinforcement learning for target localizat...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 158ms

[1470/1822] Collaborative multi-agent system for radioactive source loca...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 158ms

[1471/1822] How do professional software developers perceive the readabi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 150ms

[1472/1822] Investigating the impact of LLM-based software development a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 147ms

[1473/1822] multimodal masked autoencoder framework for denoising modula...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 164ms

[1474/1822] self-supervised pretraining methods for automatic modulation...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 463ms

[1475/1822] How can reinforcement learning and dynamic early exit strate...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 160ms

[1476/1822] Frameworks for optimizing the trade-off between energy consu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 466ms

[1477/1822] How to perform domain adaptation of Llama 3.1 for e-commerce...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 197ms

[1478/1822] Techniques for adapting large language models to the e-comme...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 169ms

[1479/1822] analysis of the Gaussian distribution and statistical proper...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 197ms

[1480/1822] understanding why large foundation model weights follow Gaus...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 152ms

[1481/1822] How can prompt-based Monte Carlo Tree Search with dynamic ex...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 139ms

[1482/1822] Improving large model reasoning on SciEval datasets using ad...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 155ms

[1483/1822] supervised contrastive knowledge distillation for few-shot c...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 199ms

[1484/1822] how to alleviate catastrophic forgetting in class-incrementa...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 140ms

[1485/1822] How can text-driven adaptation and modality alignment be use...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 139ms

[1486/1822] Methodology for aligning image and text embeddings to perfor...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 217ms

[1487/1822] How to use weighted maximum likelihood estimation in RLHF to...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 150ms

[1488/1822] Improving reinforcement learning from human feedback by addr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 169ms

[1489/1822] How can multimodal AI agents in augmented reality proactivel...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 152ms

[1490/1822] Determining optimal intervention timing for proactive AI ass...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 153ms

[1491/1822] benchmarking multilingual gender neutral translation capabil...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 160ms

[1492/1822] systematic evaluation of inclusive translation and gender ne...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 161ms

[1493/1822] How do large language models perform on patient data extract...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 149ms

[1494/1822] Benchmarking Llama2 and Meditron for structured clinical dat...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.963 | Time: 677ms

[1495/1822] How to improve the ability of large language models to follo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 315ms

[1496/1822] Techniques for training large language models to satisfy mul...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 416ms

[1497/1822] How can weight recompute and computational graph rearrangeme...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 151ms

[1498/1822] Efficient fine-tuning methods for sparse large language mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 144ms

[1499/1822] How can multimodal large language models be used for zero-sh...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 143ms

[1500/1822] Improving the alignment of multimodal LLMs with human aesthe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 431ms

[1501/1822] How to improve text to speech for research papers with compl...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 146ms

[1502/1822] Text-to-speech systems for visually impaired researchers to ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1157ms

[1503/1822] How to detect machine-generated academic essays in English a...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 200ms

[1504/1822] Advanced techniques for identifying AI-written academic pape...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 450ms

[1505/1822] How to improve the efficiency of tree search in large langua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 253ms

[1506/1822] Reducing computational costs and redundancy in LLM multi-ste...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 73ms

[1507/1822] Applying two-fold curriculum learning and proximal policy op...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 78ms

[1508/1822] How can curriculum learning and variational autoencoders be ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 70ms

[1509/1822] How to use video-grounded entailment tree reasoning to impro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 81ms

[1510/1822] Frameworks for integrating entailment tree construction and ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 81ms

[1511/1822] How can stochastic distribution embeddings and Wasserstein s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 116ms

[1512/1822] Deep learning models for knowledge tracing that incorporate ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 89ms

[1513/1822] How to evaluate social bias in large language models specifi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 122ms

[1514/1822] Benchmarks and metrics for assessing fairness and social bia...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 77ms

[1515/1822] How can self-knowledge distillation and logit standardizatio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[1516/1822] State of the art generative dataset distillation methods foc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 74ms

[1517/1822] benchmarking large language models on linguistic reasoning t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 700ms

[1518/1822] evaluating the performance of LLMs on self-contained linguis...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 83ms

[1519/1822] How to improve large language model chain of thought reasoni...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 304ms

[1520/1822] Techniques for handling implicit or missing information in L...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 355ms

[1521/1822] How can hierarchical reinforcement learning and biometric fe...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 87ms

[1522/1822] Integrating neurobiological data and multi-agent systems for...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 94ms

[1523/1822] How to protect the copyright of hardware description languag...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 111ms

[1524/1822] Methods for embedding robust watermarks into Verilog RTL and...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 105ms

[1525/1822] using curiosity-driven reinforcement learning to automatical...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 81ms

[1526/1822] automated generation of adversarial prompts to identify bias...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 621ms

[1527/1822] how to improve x-ray prohibited item detection performance w...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1075ms

[1528/1822] data augmentation methods for robust object detection in x-r...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 77ms

[1529/1822] search-based software engineering framework for automated to...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 101ms

[1530/1822] how can evolutionary algorithms and iterative prompt generat...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 113ms

[1531/1822] How to use natural language processing on corporate 10-K fil...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 102ms

[1532/1822] A data-driven methodology for measuring firm-level AI engage...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 349ms

[1533/1822] benchmark dataset for evaluating the linguistic diversity an...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 107ms

[1534/1822] how do current open-vocabulary 3D visual grounding methods p...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 109ms

[1535/1822] dynamic context-aware positional encoding for improving long...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 392ms

[1536/1822] improving transformer performance using equivariant position...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 112ms

[1537/1822] zero-shot multi-hop question answering over hybrid sources o...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 94ms

[1538/1822] how to construct a unified hybrid graph from tabular and tex...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1094ms

[1539/1822] How to use hybrid static and dynamic fingerprinting techniqu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 320ms

[1540/1822] Identifying large language models in multi-agent systems and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 145ms

[1541/1822] Analyzing the risks of software supply chain attacks through...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1105ms

[1542/1822] Relationship between HumanEval coding benchmarks and the pro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[1543/1822] How to design a flexible LLM re-ranker with configurable dep...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 528ms

[1544/1822] Efficient large language model re-ranking techniques for pas...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 531ms

[1545/1822] How can projection-free algorithms be used to solve online c...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 99ms

[1546/1822] projection-free online learning policies that utilize linear...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 302ms

[1547/1822] How to detect backdoors in deep neural networks by analyzing...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 463ms

[1548/1822] Trojan scanning methods for deep learning models that work a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 650ms

[1549/1822] How do open-source contributions impact Large Language Model...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 245ms

[1550/1822] A comparative analysis of open-source versus proprietary Lar...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 94ms

[1551/1822] In-context reinforcement learning for few-shot budget alloca...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 109ms

[1552/1822] How to optimize budget allocation across stages in online ad...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 95ms

[1553/1822] How to use GraphRAG and retrieve-divide-solve agent pipeline...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 140ms

[1554/1822] Knowledge graph based RAG framework for exploring protein-pr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 99ms

[1555/1822] How to use unsupervised domain adaptation and graph-based kn...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 82ms

[1556/1822] Improving cross-modal feature representation in text-to-imag...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 96ms

[1557/1822] How to perform zero-shot verification of large language mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 80ms

[1558/1822] Methods for evaluating and guiding large language model reas...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 93ms

[1559/1822] How to use diffusion models for efficient neural video compr...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 75ms

[1560/1822] Applying foundational diffusion models to video compression ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 96ms

[1561/1822] How to use the Fisher information matrix for active learning...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 95ms

[1562/1822] Active learning strategies for sequential tasks that balance...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 98ms

[1563/1822] How can large language models be used to provide intelligent...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 319ms

[1564/1822] Improving IR-based bug localization through automated query ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 103ms

[1565/1822] how to improve multi-modal large language models for expert-...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 80ms

[1566/1822] integrating vision language models with physics simulators f...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 92ms

[1567/1822] How can large language models be used to automate the TinyML...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.963 | Time: 123ms

[1568/1822] Framework for leveraging large language models to streamline...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 582ms

[1569/1822] graph contrastive learning for short text classification wit...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 407ms

[1570/1822] how to use multi-view text embeddings from graph learning to...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 112ms

[1571/1822] How can artificial intelligence be used to identify social b...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 96ms

[1572/1822] Applications of AI for social inclusion including sign langu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 109ms

[1573/1822] evaluating the accuracy of large language models for transla...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 106ms

[1574/1822] benchmarking llama and gpt-4o for the automated translation ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 97ms

[1575/1822] How to perform zero-shot cyber threat intelligence informati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1049ms

[1576/1822] Scalable AI framework for extracting STIX compliant named en...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 167ms

[1577/1822] multi-parallel document-level translation corpus for African...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 171ms

[1578/1822] evaluating the performance of large language models versus N...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 161ms

[1579/1822] automated prompt engineering framework using Knowledge-Gradi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.871 | Time: 159ms

[1580/1822] How to apply sequential optimal learning and mixed-integer o...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 144ms

[1581/1822] How do different floating-point quantization parameters like...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 835ms

[1582/1822] What is the optimal bit-width and exponent-mantissa ratio fo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 276ms

[1583/1822] How can the concept of applied multiplexity be used to mitig...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 185ms

[1584/1822] Using multi-agent systems to improve cultural inclusivity an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 129ms

[1585/1822] how to improve graph retrieval augmented generation by recon...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 141ms

[1586/1822] methods for mitigating information loss in knowledge graphs ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 154ms

[1587/1822] automated evaluation metrics for retrieval augmented generat...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 149ms

[1588/1822] how to measure the conversational faithfulness and context r...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 152ms

[1589/1822] Large language model error patterns in mathematical word pro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 147ms

[1590/1822] Improving mathematical reasoning in LLMs through error-aware...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.962 | Time: 427ms

[1591/1822] impact of real-world spelling mistakes and wikipedia edit hi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 749ms

[1592/1822] comparing the robustness of mt5 and bloom models against hum...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 138ms

[1593/1822] automated network configuration translation between differen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 164ms

[1594/1822] how can large language models be used to migrate legacy netw...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 172ms

[1595/1822] how to model fine-grained time-dependent user interests from...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 149ms

[1596/1822] using time-gap-aware attention and retrieval mechanisms to c...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[1597/1822] How to use logit-based knowledge distillation to optimize de...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 146ms

[1598/1822] Research on distillation frameworks for deep spiking neural ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.952 | Time: 380ms

[1599/1822] framework for building domain-specific AI agents using natur...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 145ms

[1600/1822] improving long-horizon planning in LLM agents by integrating...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 148ms

[1601/1822] Applying knowledge graph completion and relational graph att...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.414 | Time: 156ms

[1602/1822] Knowledge graph based framework for modeling complex relatio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 165ms

[1603/1822] Automated prompt optimization techniques that use task-aware...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 724ms

[1604/1822] How to implement task-referenced adaptation and multi-metric...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 143ms

[1605/1822] benefits of introducing positive friction in conversational ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 325ms

[1606/1822] how slowing down AI interactions and adding deliberate frict...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 175ms

[1607/1822] how to use llm-based line-level filtering to improve the qua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 124ms

[1608/1822] improving training data quality through fine-grained line-le...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 137ms

[1609/1822] How to improve time series reasoning in multi-modal language...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 297ms

[1610/1822] Multi-modal language models for complex time series reasonin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 89ms

[1611/1822] How can heterogeneous graph neural networks be applied to re...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 70ms

[1612/1822] Deep learning approaches for multimodal emotion prediction i...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 115ms

[1613/1822] How to improve Quranic question answering using cross-langua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 217ms

[1614/1822] Cross-language strategies for addressing linguistic disparit...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 78ms

[1615/1822] How can sparse autoencoders be used to perform feature-level...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.828 | Time: 91ms

[1616/1822] Improving LLM response consistency for paraphrased inputs us...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.827 | Time: 78ms

[1617/1822] How to use Partial Information Decomposition principles to q...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 242ms

[1618/1822] Decomposing causal power using the Möbius function of the re...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 104ms

[1619/1822] evaluating the reliability of membership inference attacks o...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 615ms

[1620/1822] do membership inference attacks on LLMs actually detect trai...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 92ms

[1621/1822] How can retrieval-augmented generation and evidence-based me...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 89ms

[1622/1822] Advanced frameworks for medical LLMs that utilize evidence s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 248ms

[1623/1822] How can self-learning agents using curriculum learning princ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 108ms

[1624/1822] Using large language model agents and iterative exploration ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.943 | Time: 92ms

[1625/1822] How do multimodal large language models perform when users p...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 121ms

[1626/1822] Benchmarking the vulnerability of vision-language models to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 93ms

[1627/1822] How does implementing a retrieval-augmented generation frame...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 86ms

[1628/1822] Evaluating the performance of retriever and generative model...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 103ms

[1629/1822] multilingual end-to-end speech recognition using mixture of ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 93ms

[1630/1822] how to improve LID-based routers in MoE architectures for mu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[1631/1822] How to prevent explicit content generation in text-to-image ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 95ms

[1632/1822] Research on using embedding space distortion to defend again...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 102ms

[1633/1822] research on federated multimodal instruction tuning framewor...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 70ms

[1634/1822] how to use mixture of adapters and adaptive parameter aggreg...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 83ms

[1635/1822] deep learning approaches for idiom detection in Sorani Kurdi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 97ms

[1636/1822] evaluating the performance of transformer models for identif...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 92ms

[1637/1822] lightweight deep learning models for energy efficient weed d...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 94ms

[1638/1822] how to achieve high accuracy weed detection on low power har...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 86ms

[1639/1822] How can large multimodal language models be used to automati...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 75ms

[1640/1822] Large scale dataset and specialized multimodal vision encode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 99ms

[1641/1822] how to implement large language models for educational manag...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 73ms

[1642/1822] frameworks for applying fine-tuned large language models to ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.591 | Time: 85ms

[1643/1822] How can metric learning with proxy anchor methods and tri-tr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 108ms

[1644/1822] Using BLIP-2 pre-trained encoders and cross-modal transforme...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 110ms

[1645/1822] using low rank adaptation to finetune language models for be...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 502ms

[1646/1822] improving the efficiency of sparse autoencoder reconstructio...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 226ms

[1647/1822] how to design fair pricing mechanisms for LLM training data ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 161ms

[1648/1822] economic frameworks for large language model data markets fo...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 110ms

[1649/1822] Benchmarking AI agents on scientific discovery tasks through...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 581ms

[1650/1822] How can autonomous agents be evaluated on their ability to d...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 85ms

[1651/1822] How to use large language models to automatically synthesize...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 111ms

[1652/1822] Integrating LLM-generated heuristics into search algorithms ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1117ms

[1653/1822] How does the length of tokenized Java code affect the accura...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 171ms

[1654/1822] Evaluating the impact of input context window size on the pe...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 142ms

[1655/1822] how to use large language models and retrieval augmented gen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 155ms

[1656/1822] automated g-code generation for cnc machines using self-corr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 169ms

[1657/1822] How can Large Language Models be used for semantic consisten...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 215ms

[1658/1822] Semi-supervised sentiment classification using entity extrac...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 315ms

[1659/1822] Combining width and depth pruning strategies for efficient s...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 156ms

[1660/1822] How to perform two-stage structured pruning on LLMs by remov...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 144ms

[1661/1822] how to improve large language model logical reasoning using ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 126ms

[1662/1822] enhancing llm problem solving through town hall style debate...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 1045ms

[1663/1822] How can participatory design methods be used to create large...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 76ms

[1664/1822] Challenges and opportunities of developing a journalist-cont...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.571 | Time: 85ms

[1665/1822] How can large language models be used as reference-aware cri...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.951 | Time: 80ms

[1666/1822] Benchmarking execution-free evaluation methods for code agen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 76ms

[1667/1822] Performance comparison of YOLOv7 and Faster R-CNN for vision...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 69ms

[1668/1822] How can deep learning models like YOLOv7 be applied to autom...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 189ms

[1669/1822] How can few-shot optimization and iterative prompt engineeri...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 73ms

[1670/1822] Fine-tuning Mistral-7B-Instruct-v0.3 for robust hallucinatio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 94ms

[1671/1822] using in-context learning with transformer models to detect ...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 81ms

[1672/1822] how can large language model architectures and in-context le...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 338ms

[1673/1822] using large language models and retrieval augmented generati...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 67ms

[1674/1822] evaluating how interactive browser extensions and chat inter...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 646ms

[1675/1822] methods for 2-bit KV cache quantization in vision-language m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 91ms

[1676/1822] how to optimize vision-language model memory consumption usi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 90ms

[1677/1822] Evaluating random forest classifiers for the detection of Na...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 88ms

[1678/1822] How can researchers develop accurate language identification...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 79ms

[1679/1822] How can analytical decomposition of first-layer attention we...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 95ms

[1680/1822] Weight-based methods for analyzing how transformer models ma...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 972ms

[1681/1822] Adaptive reward function exploration for action-level backdo...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 88ms

[1682/1822] How to perform stealthy backdoor attacks in continuous reinf...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.949 | Time: 106ms

[1683/1822] How can fine-tuned large language models like LLaMA improve ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 114ms

[1684/1822] Comparative evaluation of neural machine translation and fin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 135ms

[1685/1822] Limitations and biases of automated factuality metrics in ev...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1082ms

[1686/1822] How reliable are automated factuality evaluators at measurin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 143ms

[1687/1822] how to optimize kv cache storage in large language models us...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 192ms

[1688/1822] adaptive kv cache pruning techniques that distinguish betwee...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 170ms

[1689/1822] methods for converting dense large language models into mixt...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 357ms

[1690/1822] how to apply differentiable dynamic pruning to transform MLP...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 153ms

[1691/1822] How can diversity-based adaptive random testing using string...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 154ms

[1692/1822] Effective test selection and prioritization strategies for L...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 632ms

[1693/1822] How to improve retrieval-augmented medical question answerin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.867 | Time: 146ms

[1694/1822] Methods for improving the accuracy of medical QA systems usi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 128ms

[1695/1822] A unified framework for general mobility trajectory modeling...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 137ms

[1696/1822] How can masked conditional diffusion models with contextual ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 127ms

[1697/1822] How can large language models be used for few-shot harmful c...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 161ms

[1698/1822] Evaluating the effectiveness of multimodal in-context learni...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 77ms

[1699/1822] How to improve perceptual consistency and visual sharpness i...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 564ms

[1700/1822] Perception-inspired loss functions for neural edge detection...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1622ms

[1701/1822] empirical study evaluating the performance of deep learning ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 105ms

[1702/1822] challenges and performance degradation of deep learning appr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 123ms

[1703/1822] How can large language models be used to develop autonomous ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 98ms

[1704/1822] Large language model based proactive dialogue systems for pr...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 94ms

[1705/1822] relationship between Ehrenfeucht-Haussler rank of Boolean fu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 93ms

[1706/1822] how many Chain of Thought steps are required for a single-la...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 108ms

[1707/1822] A comprehensive analysis of statistical methodology errors i...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 115ms

[1708/1822] Investigating the prevalence of inadequate data analysis tec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 201ms

[1709/1822] How can selective boosting of attention weights for local an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 78ms

[1710/1822] Mitigating hallucinations in LVLMs by intervening in self-at...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 955ms

[1711/1822] new optimization algorithms for large language model pre-tra...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 203ms

[1712/1822] improving Signum optimizer stability for GPT-2 training by i...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1003ms

[1713/1822] How to implement adaptive PII mitigation and policy-driven m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 158ms

[1714/1822] Advanced NLP techniques for context-aware analysis and anony...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 156ms

[1715/1822] How can node influence maximization and decoupled influence ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 316ms

[1716/1822] A scalable graph unlearning framework that uses influence fu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 986ms

[1717/1822] understanding the sequential learning of skills in neural ne...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 179ms

[1718/1822] research on the domino effect in deep learning training and ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 177ms

[1719/1822] How can large vision-language models be used to identify inc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 322ms

[1720/1822] Using large vision-language models and reference images to v...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 139ms

[1721/1822] How to use retrieval-augmented generation and large language...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 160ms

[1722/1822] Open source RAG-based systems for automatic extraction of pr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 130ms

[1723/1822] Benchmark datasets for Norwegian question answering evaluati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 152ms

[1724/1822] Performance evaluation of language models on new Norwegian q...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[1725/1822] How well does GPT-4o understand the ironic use of emojis com...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 154ms

[1726/1822] Comparative study of large language models and human percept...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 176ms

[1727/1822] improving direct preference optimization for large language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 167ms

[1728/1822] alternative DPO methods that down-weight misranked samples a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 148ms

[1729/1822] How does pyramid-descent visual position encoding enhance mu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 188ms

[1730/1822] Improving vision-language model performance by using periphe...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 159ms

[1731/1822] Empirical study analyzing gender and ethnicity bias in Stabl...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 163ms

[1732/1822] How do text-to-image generative models like Stable Diffusion...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 150ms

[1733/1822] How can 4-bit quantization reduce memory storage and speed u...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 161ms

[1734/1822] Using low-precision 4-bit integer quantization to optimize t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 964ms

[1735/1822] Effective multi-stage training strategies for bilingual neur...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 91ms

[1736/1822] Developing a lightweight bilingual Islamic LLM for document ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 90ms

[1737/1822] What are the results of a thematic analysis regarding the ad...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 80ms

[1738/1822] Qualitative research on the ethical integration of generativ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 77ms

[1739/1822] efficient structured pruning techniques for large language m...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 65ms

[1740/1822] how to prune large language models quickly using structured ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 76ms

[1741/1822] How can generative AI and quantum computing be integrated in...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 73ms

[1742/1822] Prototyping platform and dataset for 6G semantic communicati...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 78ms

[1743/1822] how to improve document-level machine translation consistenc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 78ms

[1744/1822] using doc-guided memory and agent-based approaches to ensure...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 88ms

[1745/1822] How does instruction tuning affect the fundamental task capa...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 217ms

[1746/1822] Correlation between instruction-tuned performance and base m...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 225ms

[1747/1822] multi-modal large language models for single-cell rna sequen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 74ms

[1748/1822] ai models for single-cell analysis capable of cell type anno...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 85ms

[1749/1822] natural language interface for optimization models using lar...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 75ms

[1750/1822] how can large language models be used to provide counterfact...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 76ms

[1751/1822] How to use Large Language Models as an action selection filt...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 71ms

[1752/1822] A hybrid approach combining large language model decision ma...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 88ms

[1753/1822] Integrating large language models into hierarchical planning...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 64ms

[1754/1822] Standardized benchmarks and datasets for evaluating the perf...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 80ms

[1755/1822] How to improve the reasoning capabilities of mixture of expe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 81ms

[1756/1822] Enhancing cognitive depth in language models by facilitating...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 78ms

[1757/1822] Norwegian abstractive summarization dataset for benchmarking...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 79ms

[1758/1822] How to evaluate the performance of Norwegian large language ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 58ms

[1759/1822] How to apply denoise diffusion models and stochastic differe...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 76ms

[1760/1822] Diffusion transformer based signal detection methods for red...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 171ms

[1761/1822] efficient elastic quantization framework for deploying large...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 547ms

[1762/1822] how to improve memory elasticity and transition granularity ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 247ms

[1763/1822] How does the magnitude of enriched categories of texts relat...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.943 | Time: 98ms

[1764/1822] Mathematical framework for computing magnitude homology and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[1765/1822] memory efficient on-FPGA training of transformer models usin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 933ms

[1766/1822] hardware accelerator design for end-to-end transformer train...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 113ms

[1767/1822] How can large language models be used to generate synthetic ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 110ms

[1768/1822] Effective methods for reducing gender bias in pre-trained mo...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 96ms

[1769/1822] Orchestration framework for large language model training us...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 88ms

[1770/1822] Implementing joint mining mechanisms and bilateral value sha...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 940ms

[1771/1822] Attention-based deep learning framework for interpretable en...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 622ms

[1772/1822] How can Transformer architectures be used to predict multipl...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 432ms

[1773/1822] How to combine BERT sentence embeddings and TF-IDF features ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 342ms

[1774/1822] Effective methods for Marathi plagiarism detection using a w...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 570ms

[1775/1822] L2 convergence rates and stability of linear Q-learning with...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 246ms

[1776/1822] stochastic approximation analysis of linear Q-learning diver...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 145ms

[1777/1822] How can large language models be used for reference-free and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 129ms

[1778/1822] Automated metrics for counterspeech generation evaluation us...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 165ms

[1779/1822] How to use large language models for active knowledge retrie...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 141ms

[1780/1822] LLM-enhanced knowledge augmentation methods that integrate e...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 148ms

[1781/1822] How do cognitive forcing functions and different explanation...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 140ms

[1782/1822] Comparing the impact of visual explanations and cognitive fo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 143ms

[1783/1822] neurosymbolic knowledge base for environmental social and go...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 202ms

[1784/1822] how to extract actionable sustainability information from co...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 150ms

[1785/1822] How can large language models be used as proxies to reduce t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 213ms

[1786/1822] Using LLM-based pipelines and DNF proper learning to acceler...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 159ms

[1787/1822] improving best-of-n sampling in large language models using ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 171ms

[1788/1822] how to use pairwise comparison and chain of thought reasonin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 189ms

[1789/1822] How can masking high perplexity tokens in training data help...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 1034ms

[1790/1822] The effect of LLM-generated synthetic data on reducing perfo...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 349ms

[1791/1822] How to identify both latent driving forces and direct causal...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 264ms

[1792/1822] Causal discovery and representation learning framework for u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 186ms

[1793/1822] How can debate and scalable oversight be used to improve wea...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 137ms

[1794/1822] Can a weak model extract trustworthy information from a stro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 145ms

[1795/1822] How can zero-knowledge proofs be used to verify the effectiv...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 263ms

[1796/1822] Privacy-preserving verification techniques for low-rank adap...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 364ms

[1797/1822] How do binary decision biases and sampling methods in large ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 145ms

[1798/1822] Investigating randomness and decision-making biases in GPT m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 155ms

[1799/1822] How to measure conditional feature importance using adversar...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 616ms

[1800/1822] Explainable AI methods for estimating on-manifold conditiona...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 181ms

[1801/1822] benchmarking vision language model safety using a dataset of...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 545ms

[1802/1822] multilingual evaluation of multimodal safety in vision langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 195ms

[1803/1822] How to optimize large scale machine learning training pipeli...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 305ms

[1804/1822] Techniques for improving embedding table lookups and handlin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 280ms

[1805/1822] How does language-adaptive fine-tuning with the AfriBERTa mo...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 378ms

[1806/1822] Evaluating the effectiveness of language-adaptive fine-tunin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 158ms

[1807/1822] Methods for adapting decoder-only large language models to p...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 148ms

[1808/1822] How can combining bidirectional and causal attention in a ge...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 245ms

[1809/1822] Evaluating how large language models like Llama and Claude m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 157ms

[1810/1822] Comparison of emotional intensity and semantic coherence bet...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 149ms

[1811/1822] How do large language models exhibit systematic provider bia...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 159ms

[1812/1822] Empirical study and dataset for evaluating service provider ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 151ms

[1813/1822] benchmarking vision language models for error detection and ...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.571 | Time: 321ms

[1814/1822] how well do current vision language models perform at evalua...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.876 | Time: 147ms

[1815/1822] How do large language models perform on recalling notable gl...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 261ms

[1816/1822] Evaluating geographic disparities and socioeconomic correlat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 146ms

[1817/1822] Automated pipeline for converting general Word documents int...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1076ms

[1818/1822] How to evaluate the quality of AI-generated presentations us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1007ms

[1819/1822] trends in the use of causal inference methods and their impa...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 191ms

[1820/1822] how does causal narrative complexity and novelty in economic...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 155ms

[1821/1822] optimizing Mixture-of-Experts model inference on serverless ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[1822/1822] distributed deployment of MoE models in serverless computing...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 155ms

DENSE Summary:
  Avg MRR:      0.433
  Avg NDCG@5:   0.410
  Avg NDCG@10:  0.434
  Avg P@5:      0.217
  Avg P@10:     0.140
  Avg Time:     440ms

============================================================
Evaluating mode: SPARSE
============================================================

[1/1822] enhancing large language model reasoning capabilities throug...
2026-02-19 15:24:34 | INFO     | arxiv-rag.bge_embedder | Loading BGE-M3 model on cuda...
Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 29174.38it/s]
2026-02-19 15:24:37 | INFO     | arxiv-rag.bge_embedder | BGE-M3 loaded in 2.7s
You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 7009ms

[2/1822] using reinforcement learning to develop emergent self reflec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3133ms

[3/1822] How to implement test-time scaling in large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3263ms

[4/1822] Improving language model reasoning performance on competitio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3078ms

[5/1822] Scaling reinforcement learning for large language models usi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3263ms

[6/1822] How to use long chain of thought training techniques to impr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3094ms

[7/1822] open-source world foundation models for physical AI developm...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3191ms

[8/1822] how to use general-purpose world models for training physica...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3309ms

[9/1822] How can agentic search workflows and reason-in-documents mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3294ms

[10/1822] Improving the trustworthiness of large reasoning models usin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3183ms

[11/1822] End-to-end native GUI agents that use vision-only screenshot...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3291ms

[12/1822] How does system-2 reasoning and iterative training with refl...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3306ms

[13/1822] challenging multi-modal benchmark for evaluating large langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2650ms

[14/1822] dataset for testing state of the art models on advanced acad...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3099ms

[15/1822] survey of collaboration mechanisms and coordination protocol...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 2719ms

[16/1822] what are the key dimensions and strategies for organizing la...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3389ms

[17/1822] how to effectively use llm-as-a-judge and consensus filterin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3099ms

[18/1822] identifying and mitigating biases in best-of-n evaluation st...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3341ms

[19/1822] improving small language model math reasoning using monte ca...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 3069ms

[20/1822] how to scale mathematical reasoning in small models through ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.845 | Time: 2975ms

[21/1822] autonomous LLM agent framework for end-to-end scientific dis...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.951 | Time: 2755ms

[22/1822] how to use large language model agents as research assistant...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 2396ms

[23/1822] how to reduce the inference overhead and reasoning length of...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2665ms

[24/1822] RL-style fine-tuning for pruning reasoning steps and optimiz...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2814ms

[25/1822] comprehensive survey on reinforcement learning methods for t...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 2588ms

[26/1822] the impact of test-time scaling and reinforced reasoning on ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 2456ms

[27/1822] survey of autonomous AI agents in retrieval augmented genera...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2281ms

[28/1822] how do agentic design patterns like reflection and planning ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.952 | Time: 2185ms

[29/1822] How to address the optimization trade-off between reconstruc...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2766ms

[30/1822] Training efficient Diffusion Transformers using VA-VAE and v...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3107ms

[31/1822] How to address underthinking and frequent thought switching ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2658ms

[32/1822] Improving the performance of reasoning models by using a dec...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2755ms

[33/1822] How can multimodal large language models use visual reasonin...
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.637 | Time: 2628ms

[34/1822] Improving spatial reasoning in multimodal models by generati...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2582ms

[35/1822] How to apply direct preference optimization to rectified flo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2330ms

[36/1822] Multi-dimensional video reward models for aligning flow-base...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2591ms

[37/1822] benchmark for evaluating large multimodal models on expert k...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 2167ms

[38/1822] measuring knowledge gain in multimodal learning through perc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 2534ms

[39/1822] How does the Qwen2.5-1M model achieve a one million token co...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2842ms

[40/1822] Inference optimization methods for one million token context...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2570ms

[41/1822] benchmarking different large language model steering methods...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2339ms

[42/1822] performance of sparse autoencoders versus rank-1 representat...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.695 | Time: 2433ms

[43/1822] inference-time steering of diffusion models using Feynman-Ka...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2584ms

[44/1822] how to control diffusion model generation with reward functi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 2174ms

[45/1822] How can large multimodal models maintain performance while c...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2483ms

[46/1822] Efficient large multimodal model architecture that uses moda...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.949 | Time: 3089ms

[47/1822] Applying chain of thought reasoning and step by step verific...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2363ms

[48/1822] Using the potential assessment reward model and direct prefe...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.953 | Time: 2228ms

[49/1822] How does the lightning attention mechanism combined with mix...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3403ms

[50/1822] Scaling large vision-language and text models using efficien...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3109ms

[51/1822] benchmarking expert-level medical reasoning and multimodal u...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2436ms

[52/1822] high difficulty medical question answering dataset with clin...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 2862ms

[53/1822] How to protect large language models from universal jailbrea...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2595ms

[54/1822] Evaluating the effectiveness of constitutional classifiers a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2464ms

[55/1822] evaluating large language models on their ability to navigat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2737ms

[56/1822] a multi-agent framework using an explore-critic paradigm to ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2582ms

[57/1822] How to use 3D tracking videos as control signals in diffusio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2524ms

[58/1822] A unified video diffusion framework that leverages 3D contro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2467ms

[59/1822] How to implement System 2 reasoning in large language models...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.945 | Time: 2360ms

[60/1822] Training language models to perform meta-reasoning about the...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3229ms

[61/1822] hierarchical multi-agent framework for mobile task automatio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3331ms

[62/1822] how can mobile agents learn from past experiences using tips...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3095ms

[63/1822] benchmark for evaluating multi-turn conversation capabilitie...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 3336ms

[64/1822] realistic multi-turn evaluation for large language models us...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2510ms

[65/1822] transformer-based diffusion models for closed-loop autonomou...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2257ms

[66/1822] achieving human-like driving behaviors and joint prediction-...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3246ms

[67/1822] benchmarking large language model performance on competitive...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2398ms

[68/1822] how to measure the reasoning and coding abilities of LLMs th...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2741ms

[69/1822] How does graph-based retrieval-augmented generation solve th...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 2167ms

[70/1822] Systematic review of GraphRAG technical foundations includin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 2600ms

[71/1822] benchmarks for evaluating expert level reasoning and domain ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.883 | Time: 2519ms

[72/1822] multimodal foundation model evaluation datasets with human e...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2592ms

[73/1822] how to use process reward models with speculative decoding t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2542ms

[74/1822] efficient large language model inference using a draft model...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 2442ms

[75/1822] benchmarking vision language models on physical world unders...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2401ms

[76/1822] how to improve physical reasoning in vision language models ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3059ms

[77/1822] Technical details of omni-modal models using multi-stage tra...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 2401ms

[78/1822] How does the Baichuan-Audio-Tokenizer capture semantic and a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.977 | Time: 2741ms

[79/1822] How to implement a temporally-aware knowledge graph system f...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.936 | Time: 2210ms

[80/1822] Benchmarking Zep against MemGPT using the Deep Memory Retrie...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 2439ms

[81/1822] human annotated datasets for large language model safety ali...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2603ms

[82/1822] how to train lightweight LLM safety guardrails using a hybri...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 2529ms

[83/1822] Multimodal large language models for real-time full-duplex v...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 2245ms

[84/1822] How can multimodal LLMs achieve seamless two-way voice commu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.940 | Time: 2891ms

[85/1822] How to synthesize agent interaction data using backward cons...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2513ms

[86/1822] Data-centric framework for adapting autonomous agents to dig...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2940ms

[87/1822] how can reinforcement learning and oversampling for increase...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2973ms

[88/1822] strategies for enabling inference scaling in large language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2553ms

[89/1822] How can multiagent systems and independent model specializat...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2676ms

[90/1822] Finetuning language models on synthetic data from multiagent...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3916ms

[91/1822] preference optimization algorithm for Thinking-LLM-as-a-Judg...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 2877ms

[92/1822] how does separating evaluation planning from execution in ch...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 2704ms

[93/1822] How can reasoning-based supervised fine-tuning and hard samp...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2559ms

[94/1822] A large-scale dataset with detailed reasoning steps for trai...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.898 | Time: 3034ms

[95/1822] Can large language models articulate their own learned behav...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2583ms

[96/1822] Research on behavioral self-awareness in LLMs and whether mo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2832ms

[97/1822] How are large language models being used across the differen...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 2473ms

[98/1822] A comprehensive review of the roles and methodologies of lar...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3112ms

[99/1822] How to use open-source large language models for efficient G...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2170ms

[100/1822] State of the art open-source LLM framework for SWE-bench Lit...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.900 | Time: 2633ms

[101/1822] reproducing o1 style slow-thinking in multimodal models by f...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2538ms

[102/1822] research on transferring long-form reasoning capabilities fr...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 2467ms

[103/1822] How does pre-training data influence the emergence of Chain ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2270ms

[104/1822] Survey of large language model architectures and scaling mec...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2874ms

[105/1822] How does increasing inference-time compute in reasoning mode...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2914ms

[106/1822] Investigating the impact of test-time compute scaling on the...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3079ms

[107/1822] How to improve multimodal GUI agents using a two-stage fine-...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3151ms

[108/1822] Developing generalist GUI agents with hierarchical reasoning...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2657ms

[109/1822] open source multi-modal reward models for aligning large vis...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2700ms

[110/1822] using reward models for reinforcement learning and test-time...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 2317ms

[111/1822] Recent advancements in large vision-language models for deta...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 2196ms

[112/1822] Large multi-modal models that outperform GPT-4o and Gemini 1...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2858ms

[113/1822] How can Monte Carlo Tree Search be integrated with large lan...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2370ms

[114/1822] Improving the exploration of LLM-based heuristic generation ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2328ms

[115/1822] statistical methods for justifying the replacement of human ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2411ms

[116/1822] how to evaluate if an LLM is a reliable substitute for human...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2949ms

[117/1822] How can large language model agents be designed to support l...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2612ms

[118/1822] A comprehensive survey of perception, memory, and action mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2562ms

[119/1822] Multimodal foundation models for computational pathology pre...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2471ms

[120/1822] How does incorporating transcriptomic data into the pre-trai...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2411ms

[121/1822] training-free test-time alignment of diffusion models using ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2378ms

[122/1822] how to optimize diffusion models for multiple reward objecti...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.482 | Time: 2433ms

[123/1822] Inference-time alignment techniques for diffusion models usi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.878 | Time: 2338ms

[124/1822] Guide to reward-guided generation and inference-time guidanc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.957 | Time: 2180ms

[125/1822] How to use multimodal large language models to convert chart...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2271ms

[126/1822] Multimodal LLM trained on Chart2Code-160k dataset for genera...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3161ms

[127/1822] how to use dynamic trend representation transformers and cro...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.957 | Time: 2138ms

[128/1822] fusing dynamic trends and static graph attributes for traffi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2365ms

[129/1822] How can learnable orthogonal and scaling transformations imp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3237ms

[130/1822] Post-training quantization methods using Quantization Space ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.940 | Time: 2786ms

[131/1822] Scalable parallel transformer architecture for video diffusi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2297ms

[132/1822] Memory-efficient training frameworks using hybrid parallelis...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2319ms

[133/1822] How can evolutionary search strategies be used to scale infe...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 2974ms

[134/1822] Using language models to generate and recombine candidate re...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.977 | Time: 2689ms

[135/1822] How to improve low-level spatial understanding in vision-lan...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2181ms

[136/1822] Recent advancements in training VLA models that integrate se...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2403ms

[137/1822] How can self-play between a conjecturer and a prover LLM imp...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2699ms

[138/1822] Training large language models for formal mathematical verif...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2658ms

[139/1822] self-supervised music representation learning models that ut...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2314ms

[140/1822] how to train a joint music-text embedding model using contra...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2103ms

[141/1822] How to improve large language model reasoning through test-t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2283ms

[142/1822] Efficient test-time computation methods for LLMs that levera...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2795ms

[143/1822] How does scaling long chain of thought data to one million s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2836ms

[144/1822] Investigating the impact of long-CoT dataset scaling and rei...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2809ms

[145/1822] How can chain-of-thought reasoning and spatial coordinate al...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2497ms

[146/1822] Advanced spatial reasoning in vision-language models using b...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2626ms

[147/1822] How to improve language model reasoning by training models t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2258ms

[148/1822] Comparison between critique fine-tuning and imitation learni...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2515ms

[149/1822] What are the primary challenges and open problems in using m...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2218ms

[150/1822] Investigating the limitations of machine unlearning for cybe...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2294ms

[151/1822] How can multimodal large language models use hidden latent s...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 2097ms

[152/1822] Compressing textual reasoning chains into compact thinking t...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 2420ms

[153/1822] best practices and data-centric strategies for post-training...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2237ms

[154/1822] detailed implementation of post-training data strategies and...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2525ms

[155/1822] benchmarks for evaluating temporal awareness and real-time r...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 2322ms

[156/1822] how to evaluate the ability of video LLMs to handle incremen...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 2395ms

[157/1822] How can large language model agents be trained with reinforc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2223ms

[158/1822] LLM-based autonomous agents for comprehensive literature ret...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2852ms

[159/1822] Are chain of thought explanations in reasoning models like D...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2131ms

[160/1822] Measuring if large language models can accurately describe h...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.867 | Time: 2382ms

[161/1822] benchmarking negation understanding in vision language model...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2287ms

[162/1822] improving CLIP model performance on negated text queries thr...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.560 | Time: 2827ms

[163/1822] multimodal dataset for building damage assessment combining ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2260ms

[164/1822] globally distributed dataset for training AI models in build...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2440ms

[165/1822] comprehensive survey of parameter-efficient fine-tuning tech...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2145ms

[166/1822] recent developments and systematic review of PEFT methods ac...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 1923ms

[167/1822] evaluation framework for assessing the functional correctnes...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2190ms

[168/1822] multilingual benchmarks for measuring whether AI-generated c...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.937 | Time: 2575ms

[169/1822] How do large language models improve cold-start recommendati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2698ms

[170/1822] Survey of recent advances and future research directions in ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3205ms

[171/1822] How to use Monte Carlo Tree Search and iterative self-traini...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3323ms

[172/1822] Training language model agents to perform self-reflection an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3441ms

[173/1822] How to maintain safety alignment and prevent jailbreaking ri...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3190ms

[174/1822] Parameter efficient fine-tuning methods that preserve safety...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3345ms

[175/1822] Investigating the stability of features extracted by sparse ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3509ms

[176/1822] Do TopK sparse autoencoders identify consistent features acr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3812ms

[177/1822] Scaling vision language model pretraining with massive high ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3527ms

[178/1822] How does progressively scaling SFT data quantity and complex...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3703ms

[179/1822] comprehensive survey of large language models used for autom...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3464ms

[180/1822] recent advancements and challenges in using large language m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3405ms

[181/1822] Interpretable machine unlearning in diffusion models using s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3600ms

[182/1822] Applying sparse autoencoders to diffusion model activations ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3181ms

[183/1822] interactive benchmark for evaluating multimodal large langua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3567ms

[184/1822] How do state of the art multimodal large language models per...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3482ms

[185/1822] comprehensive architectural framework and modular blueprint ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3509ms

[186/1822] how to implement reasoning language models using process-bas...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3540ms

[187/1822] training-free methods for consistent character identity in t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3602ms

[188/1822] how to achieve consistent identity in diffusion models using...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3373ms

[189/1822] Recent survey on the performance and training techniques of ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3488ms

[190/1822] Comparing the efficiency and scalability of task-specific sm...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3710ms

[191/1822] How to align large language model outputs with human prefere...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3425ms

[192/1822] Techniques for test-time preference optimization that use na...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4553ms

[193/1822] large scale multimodal benchmark for evaluating cultural bia...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3832ms

[194/1822] fine-tuning vision language models on the CultureVerse datas...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3358ms

[195/1822] speculative decoding techniques that use a judge model to ac...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3381ms

[196/1822] how to increase the inference speed of large language models...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3367ms

[197/1822] How can tensor product decomposition be used to reduce KV ca...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3314ms

[198/1822] Efficient attention mechanisms using contextual low-rank rep...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3144ms

[199/1822] Comparing the impact of conversational XAI interfaces versus...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2662ms

[200/1822] How does integrating large language model agents into conver...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2599ms

[201/1822] How are large language models being applied to automate and ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2423ms

[202/1822] A review of current research on using large language models ...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 2698ms

[203/1822] rehearsal-free class incremental learning using decoupled lo...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2314ms

[204/1822] how to achieve a stable stability-plasticity trade-off in co...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2328ms

[205/1822] How to train large language models using FP4 quantization wh...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2422ms

[206/1822] Techniques for ultra-low precision training of LLMs using di...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.982 | Time: 3083ms

[207/1822] How can instruction tuning with environment-based self-refin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.807 | Time: 2770ms

[208/1822] Training LLM agents to correct their own mistakes using envi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.828 | Time: 2609ms

[209/1822] How does inference-time scaling and extended reasoning chain...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.977 | Time: 2421ms

[210/1822] Applying journey learning and systematic clinical reasoning ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2775ms

[211/1822] comprehensive benchmark for evaluating large language model ...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2052ms

[212/1822] how to assess tool-augmented large language models using cat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2231ms

[213/1822] how can we align time series data with linguistic logic and ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.815 | Time: 2668ms

[214/1822] using dual scale context alignment graph neural networks to ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2592ms

[215/1822] benchmarking large language models for factual grounding and...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2871ms

[216/1822] automated evaluation methods and leaderboards for testing wh...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2324ms

[217/1822] how to implement retrieval augmented generation over a large...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.525 | Time: 2533ms

[218/1822] framework for dynamic video retrieval and informative frame ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2584ms

[219/1822] benchmarking multi-turn retrieval augmented generation syste...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 2729ms

[220/1822] how do state-of-the-art RAG systems handle multi-turn conver...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2626ms

[221/1822] How can process reward models be used to improve multimodal ...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.412 | Time: 2237ms

[222/1822] Training multimodal large language models for mathematical r...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2550ms

[223/1822] How do large language models use sparse autoencoders to repr...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.795 | Time: 2470ms

[224/1822] Investigation into whether multilingual LLMs encode universa...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 2354ms

[225/1822] methods for scaling serial and parallel test-time compute to...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 2560ms

[226/1822] using model-generated test voting and multi-turn selection t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 2709ms

[227/1822] Is the performance drop in continual learning of large langu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 2117ms

[228/1822] How does freezing bottom layers of large language models hel...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 2679ms

[229/1822] comparing the safety and alignment levels of DeepSeek-R1 and...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2390ms

[230/1822] systematic evaluation of DeepSeek-R1 and o3-mini reasoning m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2611ms

[231/1822] techniques for maintaining response diversity in large langu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.941 | Time: 2557ms

[232/1822] how to optimize language models to generate more diverse per...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2439ms

[233/1822] how to bridge the multilingual performance gap in mathematic...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2522ms

[234/1822] improving Korean language math reasoning capabilities in LLM...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2811ms

[235/1822] How does scaling the input vocabulary size using multi-gram ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2312ms

[236/1822] Investigating the impact of decoupling input and output voca...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2827ms

[237/1822] How can hierarchical memory systems be used to enable traini...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2428ms

[238/1822] Frameworks for real-time video reasoning that use parallel s...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.972 | Time: 2240ms

[239/1822] how does the inconsistency between comprehension and safety ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2545ms

[240/1822] using shuffle inconsistency and query-based black-box optimi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.901 | Time: 2523ms

[241/1822] How does the accuracy of frequent ChatGPT users compare to a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2663ms

[242/1822] Can experienced LLM users identify AI-written articles that ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2241ms

[243/1822] Theoretical analysis of gradient descent dynamics and loss t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 2245ms

[244/1822] How does the parametrization of key and query weights affect...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2475ms

[245/1822] How does the level of sparsity in mixture-of-experts models ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2279ms

[246/1822] Investigating the optimal balance between computational effi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.862 | Time: 2705ms

[247/1822] Evaluating large language models on multi-step and constrain...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2520ms

[248/1822] How to measure the performance of LLMs in complex tool use s...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.672 | Time: 2434ms

[249/1822] How can large language model-based generative agents be used...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2775ms

[250/1822] Using LLM-powered learner simulators with memory and reflect...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2612ms

[251/1822] How does global batch load balancing loss improve expert spe...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2355ms

[252/1822] Training Mixture-of-Experts models with global frequency syn...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.937 | Time: 2674ms

[253/1822] Best practices and design considerations for conducting exte...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2210ms

[254/1822] How can AI developers implement external red teaming framewo...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.959 | Time: 2441ms

[255/1822] Unified generative model for speech and singing voice enhanc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3029ms

[256/1822] How can masked generative models be used for zero-shot voice...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2505ms

[257/1822] Scalable deep graph neural networks for crystal property pre...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2095ms

[258/1822] Advanced graph neural network models for molecules and mater...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2468ms

[259/1822] How to map hidden knowledge of neural networks into the mult...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2251ms

[260/1822] Scalable method for validating AI models and identifying neu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2644ms

[261/1822] Methods for training retrieval augmented generation models t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 3146ms

[262/1822] Improving RAG performance through test-time compute scaling ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2592ms

[263/1822] comprehensive review of the methods and algorithms used to e...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.952 | Time: 2455ms

[264/1822] how are multi-turn interactions in large language models eva...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.889 | Time: 2644ms

[265/1822] How to use tree search algorithms like MCTS to mitigate hall...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2417ms

[266/1822] Applying dual process theory and slow thinking generation wi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2388ms

[267/1822] collaborative framework for large language models and small ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2193ms

[268/1822] how to integrate cloud-based LLMs with on-device small recom...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 2778ms

[269/1822] Best practices for optimizing Retrieval-Augmented Generation...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.422 | Time: 2186ms

[270/1822] How do factors like document chunk size, retrieval stride, a...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.905 | Time: 2411ms

[271/1822] How can self-updating libraries and task decomposition impro...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 2609ms

[272/1822] Frameworks for large language models that use dynamic memory...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2747ms

[273/1822] benchmarking text to image diffusion models for toxicity fai...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2664ms

[274/1822] comprehensive safety assessment framework for evaluating bia...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2305ms

[275/1822] Do different large language models produce similar creative ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2420ms

[276/1822] Study comparing the population-level diversity and homogenei...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2400ms

[277/1822] how effective are LLM-based software repair agents at fixing...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2953ms

[278/1822] comparison of agentic program repair performance between ope...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2531ms

[279/1822] benchmark for evaluating large language model hallucinations...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2803ms

[280/1822] new taxonomy for large language model hallucinations disting...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2416ms

[281/1822] how to improve automated feature interpretability in large l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2066ms

[282/1822] natural language descriptions for llm features that capture ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2274ms

[283/1822] How to improve multi-step reasoning in RAG systems using pro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2345ms

[284/1822] Addressing early-step bias in process reward models through ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2567ms

[285/1822] How do advanced second-order optimizers like Self-Scaled BFG...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3354ms

[286/1822] Performance of self-scaled quasi-Newton methods like SSBFGS ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3288ms

[287/1822] How to use adaptive projective gradient descent and shared s...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2683ms

[288/1822] Constrained optimization methods for multi-task model mergin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2665ms

[289/1822] How does displaying AI confidence levels influence human sel...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2619ms

[290/1822] Research on the alignment between artificial intelligence co...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2731ms

[291/1822] Performance comparison between large language models and tra...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.913 | Time: 2329ms

[292/1822] Analyzing the trade-off between F1-score and inference time ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 2428ms

[293/1822] comprehensive review of techniques for distinguishing betwee...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 2053ms

[294/1822] comparison of probabilistic methods and ensemble learning fo...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 2388ms

[295/1822] discrete diffusion models for multi-task drug discovery usin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2199ms

[296/1822] how to use non-autoregressive bidirectional parallel decodin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2047ms

[297/1822] semi-supervised split learning frameworks for addressing int...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2334ms

[298/1822] how to implement split learning for resource-constrained LEO...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2460ms

[299/1822] How to adapt ConvNeXt architectures for facial emotion recog...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2603ms

[300/1822] Deep learning frameworks for facial expression recognition t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 2848ms

[301/1822] Reducing hallucinations in legal question answering systems ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2148ms

[302/1822] Benchmarking and evaluating large language model factuality ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 2698ms

[303/1822] How can modified harmful datasets bypass moderation guardrai...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 2858ms

[304/1822] Evaluating the effectiveness of guardrail moderation in prev...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2248ms

[305/1822] How to integrate natural language, algorithmic, and symbolic...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 2289ms

[306/1822] Progressive paradigm training strategies for unifying multip...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.884 | Time: 3049ms

[307/1822] training large language models to adaptively allocate infere...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2373ms

[308/1822] Inference Budget-Constrained Policy Optimization for improvi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2619ms

[309/1822] What are the common limitations and reliability issues of us...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 2681ms

[310/1822] Frameworks and algorithms for improving the alignment and cr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2652ms

[311/1822] synthetic benchmarks for evaluating the long-context reasoni...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.963 | Time: 2701ms

[312/1822] how do large language models perform on long-context logical...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2589ms

[313/1822] How can sparse autoencoder features be optimized for precise...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.591 | Time: 2239ms

[314/1822] Comparison of Feature Guided Activation Additions with Contr...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 2622ms

[315/1822] how to automatically convert open ended visual question answ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 2644ms

[316/1822] agent based framework for generating challenging distractors...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2246ms

[317/1822] How to adapt a small auto-regressive model like Qwen2 for mu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2474ms

[318/1822] What are the most effective training data cleaning and sampl...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2380ms

[319/1822] How can Kolmogorov-Arnold Networks be combined with recurren...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2429ms

[320/1822] Applying learnable temporal spline functions and edge-based ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2811ms

[321/1822] how to use causal reward modeling and counterfactual invaria...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2650ms

[322/1822] improving the fairness and reliability of large language mod...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2408ms

[323/1822] comprehensive review of mitigation strategies for large lang...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2523ms

[324/1822] recent advancements in responsible AI for enhancing large la...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2214ms

[325/1822] How can matching user queries against pre-generated question...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.907 | Time: 2500ms

[326/1822] Techniques for reducing information dilution in RAG systems ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 2944ms

[327/1822] vision transformer framework using foundation models for ene...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 1943ms

[328/1822] automated generation of training data from immunofluorescenc...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.798 | Time: 2014ms

[329/1822] How to prevent attention distribution flattening in long con...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.907 | Time: 2328ms

[330/1822] Improving length generalization and key information retrieva...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2638ms

[331/1822] How can external document knowledge be integrated directly i...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.952 | Time: 2333ms

[332/1822] Techniques for parameterizing retrieved knowledge into model...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2866ms

[333/1822] What are the limitations of graph neural networks for solvin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2079ms

[334/1822] Investigating the computational power of message passing GNN...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2512ms

[335/1822] How to implement dynamic workflow adjustment and modular sub...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2508ms

[336/1822] Research on enhancing multi-agent framework performance thro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2561ms

[337/1822] How can developers build a structured safety case to prove t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2679ms

[338/1822] Evaluating AI control safety by using red teaming and conser...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.938 | Time: 2240ms

[339/1822] Comprehensive survey on explainable artificial intelligence ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2293ms

[340/1822] How can large language models and vision-language frameworks...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2215ms

[341/1822] benchmarking the performance of constrained decoding framewo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2229ms

[342/1822] how do different constrained decoding tools compare in terms...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 2331ms

[343/1822] how to merge multiple deep learning models sequentially with...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2184ms

[344/1822] training-free methods for scalable continual model merging t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2090ms

[345/1822] How to use audio large language models for natural language-...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2255ms

[346/1822] Alignment approach with LLM distillation for enhancing audio...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2922ms

[347/1822] benchmarking page-level and layout-level retrieval systems f...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2575ms

[348/1822] evaluation of visual retrievers versus text-based retrievers...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2409ms

[349/1822] How to improve multilingual reasoning in large language mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2616ms

[350/1822] Efficient methods for multilingual reasoning alignment that ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2718ms

[351/1822] How can large language models be trained to reason over user...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 2633ms

[352/1822] Self-training frameworks for personalized LLMs that utilize ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 3219ms

[353/1822] methods for reducing hallucinations in multimodal large lang...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2270ms

[354/1822] improving multimodal LLM alignment through cross-modal prefe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3267ms

[355/1822] How does Meta use Large Language Models for mutation-guided ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 2916ms

[356/1822] LLM-based test generation framework for hardening Android Ko...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2512ms

[357/1822] How to implement a neuro-fuzzy system on an FPGA for real-ti...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2223ms

[358/1822] FPGA-based intelligent sensor for personalizing time headway...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2854ms

[359/1822] impact of AWQ and GPTQ low-bit quantization on the mathemati...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2424ms

[360/1822] effective fine-tuning strategies to restore mathematical rea...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 2792ms

[361/1822] systematic review of large language models for natural disas...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2126ms

[362/1822] how are generative artificial intelligence and large languag...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 2816ms

[363/1822] Does OpenAI's o3 model represent true artificial general int...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2503ms

[364/1822] Critique of massive trialling of predefined operations as a ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 2233ms

[365/1822] Evaluating implicit sociodemographic bias in large language ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2181ms

[366/1822] Do advanced large language models exhibit greater implicit b...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2408ms

[367/1822] Systematic literature review of large language models in CHI...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2532ms

[368/1822] How are large language models being used as research tools a...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2284ms

[369/1822] Evaluating personalized long-form text generation using larg...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 2268ms

[370/1822] How to extract atomic aspects and evidence from LLM generate...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2296ms

[371/1822] large scale dataset of high quality science problem solution...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2462ms

[372/1822] automated extraction pipeline for building scientific reason...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 2683ms

[373/1822] comprehensive survey of gradient-based multi-objective optim...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2693ms

[374/1822] recent advancements in learning continuous Pareto sets and f...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2206ms

[375/1822] Comprehensive survey of deep reinforcement learning algorith...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2269ms

[376/1822] How does deep reinforcement learning compare to traditional ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 2143ms

[377/1822] Survey of model optimization and system architecture strateg...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2313ms

[378/1822] How can cognitive edge computing frameworks balance latency,...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2066ms

[379/1822] how to generate 360 panoramas using multi-view diffusion mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2429ms

[380/1822] diffusion based method for high resolution 360 degree panora...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2525ms

[381/1822] How to improve concept erasure in diffusion models by dynami...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.904 | Time: 2361ms

[382/1822] Adaptive Guided Erasure method for selectively removing harm...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 2641ms

[383/1822] How does a hybrid attention mechanism combining global and l...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.973 | Time: 2313ms

[384/1822] Comparison of RoPE, NoPE, and QK-Normalization patterns in t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2248ms

[385/1822] How to design a multi-agent framework using large language m...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2335ms

[386/1822] LLM-based approach for mapping learner goals to skills and o...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2974ms

[387/1822] how do large language models exhibit conformity bias and gro...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 2184ms

[388/1822] benchmarking social influence in AI agents using BenchForm t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2641ms

[389/1822] benchmarking framework for evaluating large language model b...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2315ms

[390/1822] holistic platform for testing ai agents using microservice f...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2098ms

[391/1822] How can multimodal prompts like images videos and humming be...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2493ms

[392/1822] A generalized framework for symbolic music generation that u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2532ms

[393/1822] how to implement training-free visual token pruning for mult...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 2435ms

[394/1822] reducing visual redundancy in MLLMs like LLaVA-NeXT through ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2912ms

[395/1822] distributed training of large language models using asynchro...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2307ms

[396/1822] how to improve DiLoCo for distributed training by overlappin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2241ms

[397/1822] How can count-based exploration and optimistic reward estima...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 2251ms

[398/1822] A practical algorithm for online RLHF that uses a coin-flip ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2882ms

[399/1822] How can large language models be used to identify adversaria...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.948 | Time: 2167ms

[400/1822] Improving the safety and robustness of autonomous vehicles t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2652ms

[401/1822] Observational study on how programming students use generati...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2529ms

[402/1822] How do computer science students interact with large languag...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2212ms

[403/1822] Video reasoning segmentation using multimodal large language...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2416ms

[404/1822] Improving video reasoning segmentation accuracy on ReVOS ben...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2343ms

[405/1822] How can concept activation vectors be used to steer large la...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2174ms

[406/1822] Lightweight framework for granular control of LLM outputs by...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2563ms

[407/1822] How does GPT-4o perform on multimodal physics concept invent...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2695ms

[408/1822] Evaluating the multimodal and multilingual capabilities of l...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 2235ms

[409/1822] Evaluation of large language models for translating natural ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2294ms

[410/1822] Fine-tuning small language models with distilled high-qualit...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.767 | Time: 2553ms

[411/1822] Fine-grained complexity analysis of visual autoregressive mo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2381ms

[412/1822] What are the computational limits and efficiency criteria fo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1901ms

[413/1822] best practices and lessons learned from red teaming generati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 2236ms

[414/1822] what are the key methodologies and threat model ontologies u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2269ms

[415/1822] How can large language models be fine-tuned to improve their...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2774ms

[416/1822] Techniques for training large language models to ignore coun...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2473ms

[417/1822] relationship between non-smooth convex optimization theory a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2375ms

[418/1822] how to use optimization theory bounds to transfer optimal le...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2300ms

[419/1822] How can multi-modal large language models be improved for fi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2595ms

[420/1822] Enhancing fine-grained recognition in MLLMs using contrastiv...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 2496ms

[421/1822] enhancing knowledge base question answering through agentic ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2307ms

[422/1822] improving low resource kbqa performance with mcts guided exp...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 2278ms

[423/1822] mathematical analysis of transformer layer dynamics using Vl...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2092ms

[424/1822] understanding the evolution of data anisotropy and clusterin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3389ms

[425/1822] adaptive retrieval methods for overcoming bounded recall in ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3240ms

[426/1822] improving retrieval recall by using listwise LLM rankers to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3359ms

[427/1822] How can large language model agents be used for goal-driven ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3220ms

[428/1822] Evaluation framework and novel dataset for assessing the qua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3291ms

[429/1822] The relationship between softmax numerical stability and the...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3191ms

[430/1822] Achieving grokking without regularization by mitigating soft...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3304ms

[431/1822] automated techniques for optimizing prompts to improve test ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3242ms

[432/1822] how to automatically generate model-specific prompts for sof...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3129ms

[433/1822] How can interdisciplinary collaboration between physicists a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3257ms

[434/1822] A roadmap for creating large physics models using foundation...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3184ms

[435/1822] How can small language models achieve high performance in re...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2448ms

[436/1822] Efficient retrieval augmented generation framework for resou...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 3016ms

[437/1822] How to reduce error accumulation in online test-time prompt ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2171ms

[438/1822] Techniques for adaptive prompt selection based on prediction...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2619ms

[439/1822] Multi-modal sequential recommendation models using hierarchi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2782ms

[440/1822] How can hierarchical mixture of experts and contrastive lear...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2300ms

[441/1822] A comprehensive review of Mixture of Experts architectures, ...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.719 | Time: 2659ms

[442/1822] How does the Mixture of Experts framework improve the perfor...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2821ms

[443/1822] comprehensive guide to pre-training generative models and al...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 2020ms

[444/1822] academic reference for understanding the fundamental pillars...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2290ms

[445/1822] training free inference time methods for mitigating hallucin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2509ms

[446/1822] How can contrastive decoding mechanisms and masking signific...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2671ms

[447/1822] How does the extended context window of large language model...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2511ms

[448/1822] Leveraging long context large language models for text to SQ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2330ms

[449/1822] enhancing the performance of LLM input guardrails through ch...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2690ms

[450/1822] how can fine-tuning large language models as judges with cha...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 2754ms

[451/1822] comprehensive review of text data augmentation methods using...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2299ms

[452/1822] current challenges and future opportunities in using generat...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2588ms

[453/1822] How can deep learning models combining LSTM and CNN with att...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2809ms

[454/1822] Using synthetic minority over-sampling technique and hybrid ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 3059ms

[455/1822] state of the art 8B small language model as a judge for gene...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2306ms

[456/1822] techniques for training small language models for automated ...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.501 | Time: 2151ms

[457/1822] How can self-reflection frameworks and simulated psychologic...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 2061ms

[458/1822] Investigating the inconsistency between explicit and implici...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 2264ms

[459/1822] How does GPU dynamic voltage and frequency scaling impact th...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2314ms

[460/1822] Analyzing the relationship between input sequence characteri...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2254ms

[461/1822] How can transformers perform full Bayesian inference using i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2400ms

[462/1822] Comparing transformer in-context learning with Markov Chain ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2513ms

[463/1822] evaluating the effectiveness and limitations of reinforcemen...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.934 | Time: 2655ms

[464/1822] hybrid training approaches combining RL and SFT to address r...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.860 | Time: 2349ms

[465/1822] How to apply speculative sampling techniques to accelerate i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2234ms

[466/1822] Speeding up diffusion model generation using speculative dec...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2601ms

[467/1822] How to use synthetic persona data from Persona Hub to improv...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2521ms

[468/1822] Large-scale synthetic data generation strategies for trainin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 2735ms

[469/1822] How can temperature sensitivity be used to detect if instruc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 2890ms

[470/1822] Assessing the vulnerability of vision-language models to mem...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2277ms

[471/1822] How has constructionism evolved as an educational framework ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2064ms

[472/1822] Applying constructionist principles to smart education model...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 2492ms

[473/1822] agentic workflows for program synthesis using LLM quality ch...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2635ms

[474/1822] how to implement a dynamic multi-agent system for program sy...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2325ms

[475/1822] How to use hierarchical feature trees and high-level abstrac...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2481ms

[476/1822] Iterative feature tree synthesis framework for generating hi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2541ms

[477/1822] Recent surveys on large vision-language model alignment and ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2256ms

[478/1822] What are the primary causes of multimodal misalignment in vi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 2217ms

[479/1822] performance comparison of GPT-4o and Claude 3.5 against trad...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2512ms

[480/1822] evaluating the accuracy of large language models for line-by...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 2263ms

[481/1822] benchmarking the security and vulnerability of retrieval-aug...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2405ms

[482/1822] how do external knowledge injections and unverified context ...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 2129ms

[483/1822] automated extraction of olympiad level math problems from on...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2223ms

[484/1822] benchmarking mathematical reasoning in large language models...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2401ms

[485/1822] benchmarking the performance of large language model agents ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2460ms

[486/1822] standardized evaluation framework for testing the planning a...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.871 | Time: 2116ms

[487/1822] benchmarking framework for evaluating the effectiveness of h...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2530ms

[488/1822] how well do modern hate speech detection systems perform aga...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2101ms

[489/1822] comprehensive survey of generative artificial intelligence t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 2322ms

[490/1822] how are diffusion models and multimodal AI being applied to ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.936 | Time: 2480ms

[491/1822] How can diffusion priors be used to balance perceptual quali...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2242ms

[492/1822] A unified image restoration model using diffusion priors wit...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2706ms

[493/1822] validated tools and metrics for evaluating the quality and a...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2637ms

[494/1822] how to assess the structural and substantive validity of aut...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3227ms

[495/1822] investigating the emergence and formation of localized task ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2614ms

[496/1822] using task vector prompting loss to enhance task representat...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2682ms

[497/1822] How can large language models achieve higher knowledge densi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2531ms

[498/1822] Machine writing frameworks that simulate human-like cognitiv...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2925ms

[499/1822] How can modified Algorithm-of-Thoughts techniques like AoT+ ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2519ms

[500/1822] Investigating the use of enhanced Algorithm-of-Thoughts fram...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3127ms

[501/1822] How can multi-agent systems and agentic AI frameworks like O...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3124ms

[502/1822] Evaluation of hallucination mitigation strategies using hier...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 2490ms

[503/1822] How can knowledge distillation and conditional variational a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2160ms

[504/1822] Adaptive diversity distillation techniques for math word pro...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.940 | Time: 2577ms

[505/1822] How can large language models be used to generate functional...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2433ms

[506/1822] Leveraging LLMs and design layout graphs to automate the cre...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 2589ms

[507/1822] How well do large language models perform on Allen's interva...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2299ms

[508/1822] Evaluating large language model capabilities in temporal und...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 2355ms

[509/1822] segment-level direct preference optimization for improving m...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 2218ms

[510/1822] how to optimize social agents using segment-based direct pre...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2090ms

[511/1822] How to improve RAG systems for industrial applications using...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2657ms

[512/1822] Techniques for knowledge atomizing and knowledge-aware task ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2758ms

[513/1822] How can large language models be integrated into an analytic...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2366ms

[514/1822] A framework for analyzing multi-user XR sessions using a pla...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2645ms

[515/1822] generating diverse and customizable synthetic Q&A pairs for ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.885 | Time: 2442ms

[516/1822] a two-stage framework for producing lexically and semantical...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.908 | Time: 2231ms

[517/1822] How can large language models like fine-tuned BART and BERT ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2633ms

[518/1822] Proactive intrusion prediction framework for IoT security us...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 2648ms

[519/1822] Evaluating the performance of machine-generated text detecto...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.884 | Time: 2118ms

[520/1822] Can machine learning models robustly detect AI-generated con...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 2522ms

[521/1822] How to improve safety visual reasoning in large vision-langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3281ms

[522/1822] Reducing attack success rate in vision-language models by ad...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3092ms

[523/1822] How to achieve temporally consistent video relighting using ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2158ms

[524/1822] Frameworks for video relighting that preserve illumination p...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3119ms

[525/1822] How to improve Sharpness-Aware Minimization performance by e...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2613ms

[526/1822] Analysis of SAM training dynamics using third-order stochast...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2727ms

[527/1822] evaluating uncertainty estimation versus self-knowledge for ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2414ms

[528/1822] comprehensive analysis of uncertainty estimation techniques ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2376ms

[529/1822] improving mathematical reasoning in large language models us...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3094ms

[530/1822] enhancing LLM math capabilities with a first-try strategy an...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.913 | Time: 2797ms

[531/1822] benchmarks for evaluating long-context language models on co...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.894 | Time: 2312ms

[532/1822] how do current large language models perform on tasks requir...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2639ms

[533/1822] How can domain prompts and semantic prototypes be used in a ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2435ms

[534/1822] Diffusion-based time series generation using prototype assig...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 3072ms

[535/1822] How can we effectively detect AI-generated text that has bee...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 2393ms

[536/1822] A data-centric augmentation approach for building robust mod...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.839 | Time: 2767ms

[537/1822] How does the presence of citations in large language model r...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3199ms

[538/1822] Experimental research examining if users trust LLM generated...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2534ms

[539/1822] How can retrieval-augmented dynamic prompt tuning improve th...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2306ms

[540/1822] A framework for incomplete multimodal learning using retriev...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2940ms

[541/1822] How can model editing and attention head analysis be used to...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 2351ms

[542/1822] Improving large language model robustness by identifying key...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2519ms

[543/1822] evaluating large language models on everyday moral dilemmas ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2534ms

[544/1822] how do large language models perform on complex social ethic...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2148ms

[545/1822] Effective post-training quantization methods for Mamba archi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1977ms

[546/1822] How to use variance aligned rotation and Karhunen-Loeve Tran...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2231ms

[547/1822] How can black-box adversarial attacks disrupt decision-makin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2424ms

[548/1822] Evaluating the robustness of vision-language models in auton...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.944 | Time: 2267ms

[549/1822] How to use optimal transport and Monge-Kantorovich vector ra...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3324ms

[550/1822] Improving conformal prediction for multi-output regression a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3167ms

[551/1822] How can direct preference optimization be used to personaliz...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3300ms

[552/1822] Aligning diffusion models with multiple individual user rewa...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3289ms

[553/1822] Survey of recent research on how embodiment, symbol groundin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2601ms

[554/1822] How are researchers addressing the limitations of large lang...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2419ms

[555/1822] academic benchmark for evaluating multi-hop tool use in larg...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2405ms

[556/1822] evaluation dataset for testing complex function calling and ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 2566ms

[557/1822] benchmarking cultural bias towards Western entities in Arabi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 2244ms

[558/1822] impact of pre-training data frequency and subword tokenizati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2591ms

[559/1822] How can internal activation steering improve the safety and ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2126ms

[560/1822] Methods for revising multimodal model activations during gen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3297ms

[561/1822] How can multi-agent large language model frameworks improve ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3136ms

[562/1822] Implementing multi-agent orchestration for geospatial tasks ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3348ms

[563/1822] How can we design just-in-time and human-verifiable security...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3303ms

[564/1822] A framework for generating contextual security policies for ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2657ms

[565/1822] External safety evaluation results and red teaming findings ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 2325ms

[566/1822] Systematic generation of unsafe test inputs to assess the pr...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.834 | Time: 2320ms

[567/1822] flexible and scalable training system for sparse mixture of ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2266ms

[568/1822] optimizing task scheduling and communication overhead in lar...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 1964ms

[569/1822] How can retrieval augmented generation and large language mo...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2496ms

[570/1822] Implementing RAG-based systems for real-time phone call frau...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2934ms

[571/1822] benchmarking the performance of multimodal large language mo...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 2280ms

[572/1822] evaluating the ability of MLLMs to determine chronological e...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 2633ms

[573/1822] Improving quantum machine learning performance by training t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 1862ms

[574/1822] End-to-end differentiable framework for learning parameteriz...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2063ms

[575/1822] Best practices and training recipes for domain-adaptive post...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.970 | Time: 2450ms

[576/1822] How does combining continual pre-training with instruction-f...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2416ms

[577/1822] how to model long-range dependencies in brain networks using...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1996ms

[578/1822] using biased random walks in brain graph transformers to mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2412ms

[579/1822] adaptive perturbation methods for mitigating harmful fine-tu...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 2298ms

[580/1822] how to recover large language model safety alignment after h...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2732ms

[581/1822] comprehensive benchmark for evaluating undergraduate level m...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2397ms

[582/1822] evaluating large reasoning models using metrics like effecti...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2307ms

[583/1822] How to improve table understanding in language models using ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2323ms

[584/1822] A framework for enhancing table reasoning in LLMs by extract...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2070ms

[585/1822] comprehensive review of foundation models in computational p...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2359ms

[586/1822] what are the current challenges and methodologies for buildi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2428ms

[587/1822] adaptive world model reinforcement learning for autonomous d...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2452ms

[588/1822] how to address distribution shift in world model based plann...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2453ms

[589/1822] A comprehensive review of recent deep learning architectures...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2387ms

[590/1822] How do modern deep learning foundation models categorize the...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2210ms

[591/1822] evaluation of explainability methods for encoder based langu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2212ms

[592/1822] comparative analysis of lime shap and lrp techniques for tra...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2256ms

[593/1822] How do attention modules in different transformer layers con...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2271ms

[594/1822] Analyzing the role of early versus late transformer layers i...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.798 | Time: 2389ms

[595/1822] evaluating the reliability of large language models as judge...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 2538ms

[596/1822] comparing human rater performance with LLM as judge models u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2358ms

[597/1822] How to generate task-specific LoRA weights using Conditional...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2475ms

[598/1822] Using meta-learning and CVAE generators to produce task-awar...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 2630ms

[599/1822] how to use a world knowledge tree and self-reflection refine...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.905 | Time: 2357ms

[600/1822] framework for scaling supervised fine-tuning data through kn...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.564 | Time: 2520ms

[601/1822] Challenges and future research directions for integrating mu...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2308ms

[602/1822] What are the essential requirements and desiderata for devel...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2192ms

[603/1822] how to use prioritized depth-first search and large language...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.945 | Time: 2390ms

[604/1822] combining embedding based retrieval with search heuristics t...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.442 | Time: 2159ms

[605/1822] taxonomy of interaction types between software developers an...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2127ms

[606/1822] how do developers interact with generative AI and large lang...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2261ms

[607/1822] A comprehensive analysis and taxonomy of common error types ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2542ms

[608/1822] New methods for efficient error detection and automated repa...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2424ms

[609/1822] comprehensive survey of large language models in bioinformat...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.899 | Time: 2012ms

[610/1822] what are the current challenges and future directions for us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3272ms

[611/1822] comprehensive review of test-time compute scaling methods fo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3340ms

[612/1822] survey on inference time computation techniques such as self...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3168ms

[613/1822] integrating large language model multi-agent frameworks with...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 2988ms

[614/1822] performance evaluation of Autogen based multi-agent systems ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2435ms

[615/1822] How do large language models respond to malicious jailbreaki...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2523ms

[616/1822] Investigating the impact of using fabricated scientific argu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2805ms

[617/1822] factors influencing high school students' acceptance of gene...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2155ms

[618/1822] research on the role of perceived enjoyment and compatibilit...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2249ms

[619/1822] dataset for evaluating vision language models on handwritten...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2129ms

[620/1822] how well do vision language models perform on reasoning task...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2370ms

[621/1822] adaptive interpolation methods for knowledge distillation to...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2547ms

[622/1822] how to prevent mode collapse and mode averaging in language ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2299ms

[623/1822] automated methods for optimizing large language model pretra...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2713ms

[624/1822] how to improve LLM training efficiency with UtiliMax and MED...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2598ms

[625/1822] Diffusion transformer models for joint image and video virtu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2372ms

[626/1822] How to maintain temporal consistency in long video virtual t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2203ms

[627/1822] how to mitigate systematic misalignment in reinforcement lea...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2428ms

[628/1822] the impact of providing evaluators with simulated future con...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.793 | Time: 2405ms

[629/1822] How to perform precise free-form grounding across multiple i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2863ms

[630/1822] Improving multi-image grounding capabilities in MLLMs throug...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3387ms

[631/1822] How can large language models be integrated with high-order ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3248ms

[632/1822] Framework for mixed-type data imputation using bidirectional...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3291ms

[633/1822] How can large language models and CTGANs be used to generate...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3158ms

[634/1822] Evaluating the performance of GPT-based models and CTGAN in ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2952ms

[635/1822] How can optimized soft prompts in the textual embedding spac...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2454ms

[636/1822] Effective safety alignment for diffusion models using catego...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2537ms

[637/1822] using reinforcement learning and representation space guidan...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.906 | Time: 2305ms

[638/1822] interpretable reinforcement learning methods for LLM jailbre...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.900 | Time: 2304ms

[639/1822] how to improve diversity in mixture of experts for low-rank ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3376ms

[640/1822] using the Gram-Schmidt process and Stiefel manifold to enhan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3205ms

[641/1822] How can hierarchical autoregressive transformers combine cha...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3339ms

[642/1822] Large language models using character-to-word hierarchical a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3240ms

[643/1822] How does the increase in energy loss in the final layer of l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3411ms

[644/1822] Effective methods for mitigating reward hacking in RLHF by p...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3274ms

[645/1822] certified robustness of large language models using randomiz...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3424ms

[646/1822] how to calculate tight lower bounds for the worst-case robus...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3348ms

[647/1822] How do scaling laws for large language models change when in...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3148ms

[648/1822] Empirical analysis of scaling laws for training differential...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3304ms

[649/1822] How consistent are large language models like GPT-4 and Clau...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3128ms

[650/1822] Measuring the instability of LLM outputs for legal decision ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3194ms

[651/1822] Assessing the ability of large language models to trace exec...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 2557ms

[652/1822] Measuring the gap between code generation performance and st...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2365ms

[653/1822] impact of using problem-solving data versus general mathemat...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2376ms

[654/1822] comparing mathematical reasoning performance of LLMs trained...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2537ms

[655/1822] How to modify Chinchilla scaling laws to include inference l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2545ms

[656/1822] Training language models for inference efficiency by co-opti...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2611ms

[657/1822] How to use a hierarchical mixture-of-experts framework to mo...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.945 | Time: 2405ms

[658/1822] Advanced multimodal fake news detection methods focusing on ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2457ms

[659/1822] Evaluating instruction-following large language models for z...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2068ms

[660/1822] How can large language models leverage natural language infe...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2360ms

[661/1822] accelerating diffusion model inference by exploiting tempora...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2683ms

[662/1822] how to design a hardware accelerator that leverages temporal...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.947 | Time: 2751ms

[663/1822] A comprehensive survey of five hundred seventy two code benc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2815ms

[664/1822] The HOW2BENCH framework and checklists for improving the qua...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3160ms

[665/1822] How does complexity control and neuron condensation influenc...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.934 | Time: 2700ms

[666/1822] Investigating the internal information circuits and complexi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2746ms

[667/1822] using fine-tuned small language models like Llama 3 to gener...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2736ms

[668/1822] evaluating the effectiveness of quantized small language mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2486ms

[669/1822] Evaluating end-to-end spoken language models on knowledge un...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2604ms

[670/1822] A benchmark for assessing the robustness and world knowledge...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.948 | Time: 2607ms

[671/1822] dynamic self-adaptation of large language models through sin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2372ms

[672/1822] efficient alternative to LoRA for real-time task specific ad...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2353ms

[673/1822] fine-tuning T5-small for scalable and topic-controlled quest...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2625ms

[674/1822] how to generate semantically aligned and topic-specific ques...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2529ms

[675/1822] Open source TypeScript framework for building autonomous AI ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2423ms

[676/1822] How to integrate large language models with web3 application...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2463ms

[677/1822] Theoretical analysis of prompt optimization as an alternativ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2412ms

[678/1822] How can prompt optimization be formulated as an optimization...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.919 | Time: 2640ms

[679/1822] adaptive sublayer skipping techniques to accelerate prefilli...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3333ms

[680/1822] how to improve long-context LLM inference efficiency by iden...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.895 | Time: 2647ms

[681/1822] Dynamic structured pruning for large language models that ad...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.772 | Time: 2657ms

[682/1822] Using a sparse mask predictor to dynamically select relevant...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.929 | Time: 2383ms

[683/1822] efficient large language model tool learning methods using p...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2351ms

[684/1822] how to improve LLM tool calling efficiency by dividing compl...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2612ms

[685/1822] How does in-execution self-debugging using intermediate stat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3313ms

[686/1822] Improving large language model programming performance throu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3250ms

[687/1822] how to implement curriculum learning for large language mode...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.935 | Time: 2556ms

[688/1822] dynamic pretraining data selection strategies based on chang...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.841 | Time: 2233ms

[689/1822] How well do large language models like GPT-4 and Claude perf...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2447ms

[690/1822] Benchmarking different large language models and prompt engi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.859 | Time: 2582ms

[691/1822] How can we identify and edit specific gender neurons in larg...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2463ms

[692/1822] A method for mitigating gender bias in LLMs through interpre...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2849ms

[693/1822] Strategies for accelerating deep learning inference on resou...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.810 | Time: 2647ms

[694/1822] A comprehensive review of techniques like pruning, quantizat...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 2522ms

[695/1822] How to improve text-to-CAD generation using large language m...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 2293ms

[696/1822] Training large language models for CAD model creation throug...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 2231ms

[697/1822] How can internal representations and hidden states of large ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 2351ms

[698/1822] Evaluating LLM code generation quality by analyzing latent s...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 2515ms

[699/1822] how to measure and detect conversational bias in multi-agent...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2211ms

[700/1822] evaluating why traditional questionnaire-based bias detectio...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2659ms

[701/1822] enhancing graph retrieval-augmented generation for medical r...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3071ms

[702/1822] improving large language model explainability in high-stakes...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3168ms

[703/1822] How can attackers manipulate voting-based LLM leaderboards l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3299ms

[704/1822] Security vulnerabilities and mitigation strategies for crowd...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 2055ms

[705/1822] improving mathematical reasoning in large language models by...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2329ms

[706/1822] methods for optimizing the intermediate steps of chain-of-th...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.942 | Time: 2230ms

[707/1822] critic-free reinforcement learning from human feedback using...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2354ms

[708/1822] comparison of local versus global advantage normalization in...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 3194ms

[709/1822] How to implement multimodal large language model multi-agent...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3311ms

[710/1822] Designing no-code frameworks for multimodal multi-agent syst...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.525 | Time: 2319ms

[711/1822] foundation model for automating systematic reviews through h...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1945ms

[712/1822] performance of large language models compared to clinicians ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1998ms

[713/1822] How to train neural networks with brain-like topographic org...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2414ms

[714/1822] Developing spatially organized artificial neural networks th...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.907 | Time: 2890ms

[715/1822] Expert annotated datasets for legal information retrieval an...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3166ms

[716/1822] What are the available benchmarks for evaluating retrieval s...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 1858ms

[717/1822] decentralized framework for specialized large language model...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 2360ms

[718/1822] how can blockchain technology be integrated with fine-tuned ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 2452ms

[719/1822] Multi-agent system for question answering using routing and ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2896ms

[720/1822] How can routing and planning in multi-agent RAG systems impr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2555ms

[721/1822] How can crowdsourced LLM leaderboards like Chatbot Arena be ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2395ms

[722/1822] Analysis of the vulnerability of Elo rating systems in large...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2001ms

[723/1822] How to improve the transparency and verification of intermed...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2518ms

[724/1822] Segmenting the chain of thought reasoning process into layer...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 3028ms

[725/1822] using crowdsourced metaphors to analyze public perception of...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3287ms

[726/1822] how do open-ended mental models and metaphors predict trust ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2324ms

[727/1822] how to automatically verify the factual accuracy of large la...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2583ms

[728/1822] benchmarking automated systems for fact-checking medical dis...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2555ms

[729/1822] enhancing clinical reasoning in small language models throug...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2144ms

[730/1822] self-evolving framework for medical reasoning using soft dua...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2159ms

[731/1822] how to improve the diversity of language model outputs in RL...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2650ms

[732/1822] techniques for balancing human preference alignment and resp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2447ms

[733/1822] Synthesizing long-context training data for large language m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2915ms

[734/1822] How can we effectively train long-context large language mod...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.795 | Time: 2644ms

[735/1822] How do next-generation intelligent tutoring systems like Soc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 2303ms

[736/1822] Using generative AI and JSON-based prompts to create adaptiv...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.913 | Time: 2973ms

[737/1822] How to perform hierarchical code summarization for entire so...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2425ms

[738/1822] Improving repository-level software documentation through sy...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2640ms

[739/1822] How can Simulation Theory and task decomposition be used to ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.923 | Time: 2509ms

[740/1822] Enhancing LLM performance on higher-order Theory of Mind tas...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 2479ms

[741/1822] How can a taxonomy of user information needs guide the integ...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 2479ms

[742/1822] Synergies between large language models and knowledge graphs...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 2886ms

[743/1822] how to use large language models and constraint logic progra...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2092ms

[744/1822] combining LLMs with logic programs to improve the accuracy a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2584ms

[745/1822] Controllable video generation using blob representations and...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.973 | Time: 2750ms

[746/1822] Methods for enhancing compositional text-to-video generation...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2796ms

[747/1822] Empirical study evaluating the proficiency of code large lan...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2800ms

[748/1822] How do the internal biases of code LLMs affect their ability...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 2625ms

[749/1822] How to achieve faster LLM inference on mobile devices by usi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2571ms

[750/1822] Optimization techniques for on-device large language model i...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2270ms

[751/1822] applying superstatistical methods and q-Gaussian distributio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2090ms

[752/1822] using the Informer transformer model and LightGBM for long-t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2346ms

[753/1822] Multi-agent systems using small language models and retrieva...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.792 | Time: 2395ms

[754/1822] Using fine-tuned language models and RAG to democratize bioi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 2380ms

[755/1822] semi-supervised learning methods for fine-grained action rec...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2395ms

[756/1822] how to improve fine-grained action recognition with limited ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3323ms

[757/1822] Scalable graph neural network framework for recommendation u...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2401ms

[758/1822] How to improve the efficiency and robustness of graph based ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2262ms

[759/1822] evaluating the reliability and cultural sensitivity of large...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2194ms

[760/1822] benchmarking value misalignment in open-source large languag...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2723ms

[761/1822] how to improve multimodal large language model performance o...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2334ms

[762/1822] benchmarking numerical reasoning and structure recognition i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2638ms

[763/1822] How can internal chain of thought reasoning steps in customi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2706ms

[764/1822] Stealthy backdoor attacks on large language models that use ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2751ms

[765/1822] how to improve contextual faithfulness in retrieval-augmente...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2454ms

[766/1822] enhancing faithfulness in long-form question answering by tr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2772ms

[767/1822] How to use language-guided cross-attention mechanisms to pru...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2670ms

[768/1822] A simple plug-and-play method for vision token pruning in ML...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2469ms

[769/1822] How to use Transformer models for controllable multitrack MI...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2119ms

[770/1822] Generative music models for computer-assisted composition th...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.947 | Time: 2424ms

[771/1822] How can structured prompt design and in-context learning tec...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 2683ms

[772/1822] Evaluating the effectiveness of general-purpose large langua...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2729ms

[773/1822] research on context-aware safety benchmarks for large langua...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2536ms

[774/1822] how to evaluate large language model safety by considering c...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2399ms

[775/1822] how to use knowledge graph retrieval augmented generation to...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.951 | Time: 2720ms

[776/1822] graph based retrieval methods for resolving semantic ambigui...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2549ms

[777/1822] How to measure and reduce redundancy in multi-modality large...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.687 | Time: 2356ms

[778/1822] Quantitative analysis of redundant test questions and overla...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.806 | Time: 2734ms

[779/1822] How does changing the reward function shape with an alpha pa...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2455ms

[780/1822] Using AlphaPO to mitigate likelihood displacement and over-o...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2758ms

[781/1822] Recent survey of foundation model based agents capable of co...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2315ms

[782/1822] What are the current research gaps and taxonomies for digita...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2659ms

[783/1822] machine learning models and explainable AI techniques for de...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 2266ms

[784/1822] comparison of XGBoost and Random Forest performance against ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 2341ms

[785/1822] How can a multi-agent LLM system provide decision interpreta...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2546ms

[786/1822] Improving the reliability of LLM-based RTL code generation t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2379ms

[787/1822] efficient document compression methods for retrieval augment...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 2330ms

[788/1822] how to achieve high compression rates for RAG context window...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.840 | Time: 2413ms

[789/1822] how does using chain of thought reasoning influence the conf...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.893 | Time: 2515ms

[790/1822] impact of providing reasoning steps on the overconfidence of...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2371ms

[791/1822] how can chain of thought prompting be used to generate empat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3202ms

[792/1822] two-stage training approach for speech-based large language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3303ms

[793/1822] evaluating instruction tuning data quality by measuring the ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3379ms

[794/1822] how to improve synthetic dataset integrity by filtering out ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3140ms

[795/1822] Evaluating the performance of causal sequence decoding model...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3234ms

[796/1822] How do language model decoders trained with cross-entropy lo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3370ms

[797/1822] automatic prompt engineering for multi-step LLM pipelines us...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2847ms

[798/1822] how to optimize complex LLM workflows with feedback-based pr...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 2496ms

[799/1822] training-free approach to long video understanding using con...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.944 | Time: 2211ms

[800/1822] how to process arbitrarily long videos in video question ans...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2360ms

[801/1822] How can large language model serving systems achieve both pr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2158ms

[802/1822] The impact of Deficit Longest Prefix Match and D2LPM on thro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2568ms

[803/1822] using entropy-based selective classifiers to estimate confid...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3418ms

[804/1822] comparing model calibration and error detection performance ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3436ms

[805/1822] how to improve retrieval-augmented generation performance us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3378ms

[806/1822] impact of multi-granular and self-contained document chunkin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3173ms

[807/1822] Performance comparison of monolingual versus multilingual BE...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3566ms

[808/1822] Introduction of a public UPOS-tagged dataset and evaluation ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3370ms

[809/1822] AI-powered learning platform using Retrieval-Augmented Gener...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3303ms

[810/1822] How can agentic Large Language Model assistants provide pers...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1902ms

[811/1822] How does the Graph-PReFLexOR framework use graph reasoning a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 952ms

[812/1822] Integrating category theory and knowledge graph growth strat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1361ms

[813/1822] high quality Chinese datasets for large language model pretr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1929ms

[814/1822] curated Chinese language model training corpora for improvin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1013ms

[815/1822] How to improve many-shot in-context learning performance in ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2027ms

[816/1822] Techniques for addressing performance degradation in many-sh...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2270ms

[817/1822] How to improve context selection in multimodal RAG using rel...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1517ms

[818/1822] Adaptive context selection and re-ranking methods for improv...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1303ms

[819/1822] benchmarks for evaluating adversarial robustness and composi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1857ms

[820/1822] how to improve the reliability of audio-visual LLMs using ca...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1721ms

[821/1822] How to improve GUI action grounding in novel environments us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1697ms

[822/1822] Using MLLM based agents and Q-value-Incentive In-Context Rei...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2291ms

[823/1822] impact of learning rates and training data size on the out-o...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1708ms

[824/1822] how to optimize instruction tuning for table tasks while mai...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1440ms

[825/1822] How to use rank-wise mixture of experts in LoRA to improve m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4104ms

[826/1822] Efficient parameter fine-tuning for multiple tasks by treati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3250ms

[827/1822] evaluation datasets for measuring the instruction following ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3134ms

[828/1822] how do multilingual retrieval models perform when given comp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3283ms

[829/1822] Applying social choice theory and maximal lotteries to impro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3153ms

[830/1822] How does Nash Learning from Human Feedback approximate maxim...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3237ms

[831/1822] LLM retrieval methods that align complex questions with data...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3324ms

[832/1822] How to use relationship exploration between data objects to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3305ms

[833/1822] How can decoder-only LLMs be used for extractive schema link...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3184ms

[834/1822] Improving schema linking accuracy and computational efficien...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3305ms

[835/1822] graph prompt tuning for heterophily graphs using distributio...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3298ms

[836/1822] how to use hop-specific prompts and generalized low-rank ada...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3265ms

[837/1822] How to implement hierarchical backpressure for autoscaling l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3302ms

[838/1822] Improving GPU utilization and SLO attainment in LLM inferenc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3109ms

[839/1822] zero-shot hallucination detection in large language models u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2946ms

[840/1822] how can attention weights and query categorization be used t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.887 | Time: 2583ms

[841/1822] How vulnerable is GraphRAG to data poisoning attacks compare...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2706ms

[842/1822] Evaluation of poisoning attacks on knowledge graph based RAG...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2080ms

[843/1822] How to evaluate the coverage of diverse factual information ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 2451ms

[844/1822] Automated framework for measuring information diversity and ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2702ms

[845/1822] theoretical framework for contrastive pre-training using app...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2096ms

[846/1822] sample complexity guarantees and joint generative hierarchic...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 2123ms

[847/1822] How can retrieval-augmented dialogue knowledge aggregation i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2462ms

[848/1822] A multi-granularity graph-based approach for aggregating sem...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3130ms

[849/1822] how can large language models be used for zero-shot and few-...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3238ms

[850/1822] large scale source code authorship identification using a to...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2298ms

[851/1822] Auto-regressive transformer models for graph generation usin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3239ms

[852/1822] How to use pre-trained transformers for graph property predi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2753ms

[853/1822] framework for evaluating multimodal retrieval augmented gene...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.793 | Time: 2757ms

[854/1822] measuring the reliability of multimodal RAG systems by asses...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.839 | Time: 2493ms

[855/1822] detailed training logs and implementation strategies for bui...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2450ms

[856/1822] open source resources and technical documentation for addres...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2420ms

[857/1822] hybrid framework for automated log analysis using uncertaint...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2059ms

[858/1822] how to improve log analysis performance using large language...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2489ms

[859/1822] self-supervised quantization methods for integrating knowled...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2548ms

[860/1822] how to use quantized entity representations to improve large...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2338ms

[861/1822] generalizing the logistic loss function for language modelin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2405ms

[862/1822] evaluating alpha-divergence based loss functions and paralle...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.973 | Time: 2667ms

[863/1822] How do current large language models perform on tasks evalua...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.973 | Time: 2462ms

[864/1822] A comprehensive evaluation framework and synthetic benchmark...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 3261ms

[865/1822] How do function encoders using least-squares optimization co...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3190ms

[866/1822] A geometric approach to transfer learning characterizing int...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3331ms

[867/1822] How does a block causal transformer architecture improve nex...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3114ms

[868/1822] Foundation models for fluid dynamics using block causal tran...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2551ms

[869/1822] how to obtain valid confidence intervals when using machine ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2412ms

[870/1822] prediction powered inference bootstrap methods for debiasing...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.946 | Time: 2977ms

[871/1822] How can mixture of experts architectures improve the perform...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3248ms

[872/1822] Using rectified flow pose diffusion and multi-modal LLMs for...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3294ms

[873/1822] using parameter trust regions to mitigate knowledge conflict...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3110ms

[874/1822] training-free techniques for multi-task model merging that p...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3280ms

[875/1822] How to use probabilistic federated search to improve retriev...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3116ms

[876/1822] Improving RAG performance for multi-product QA by aggregatin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3239ms

[877/1822] multilingual dataset for evaluating consistency of large lan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3359ms

[878/1822] methodology for comparing health-related inquiry consistency...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3312ms

[879/1822] How can LLM-based multi-agent systems automate the entire fi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3738ms

[880/1822] A collaborative framework using multiple language model agen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3389ms

[881/1822] How to precisely control camera extrinsic and intrinsic para...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3187ms

[882/1822] Methods for adjusting camera angles and lens distortions in ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 3282ms

[883/1822] How can attackers use training loss information from proprie...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2827ms

[884/1822] Vulnerability of Google Gemini models to adversarial prompt ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2788ms

[885/1822] Diffusion models for large scale neural network parameter ge...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2486ms

[886/1822] Techniques for generating full network parameters for vision...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.941 | Time: 2628ms

[887/1822] How can abductive reasoning be used to infer user personas f...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2931ms

[888/1822] Improving LLM personalization by training on preference data...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2795ms

[889/1822] How can hierarchical attention mechanisms be used to improve...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2498ms

[890/1822] Recent approaches to zero-shot video-to-music generation usi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2502ms

[891/1822] LLM text-to-SQL framework using statistical conformal predic...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2302ms

[892/1822] How can human-in-the-loop and branching point prediction imp...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2441ms

[893/1822] How can optimal transport-based alignment loss and attention...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3269ms

[894/1822] State-of-the-art methods for integrating visual cues into au...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2913ms

[895/1822] How are large language models being used to detect hate spee...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2100ms

[896/1822] Recent advancements in using cutting edge language models fo...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 2659ms

[897/1822] how to generate counterfactual and contrastive explanations ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2470ms

[898/1822] model intrusive methods for interpreting DCNN image classifi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2268ms

[899/1822] How do dimensionality reduction techniques like PCA and UMAP...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2088ms

[900/1822] Analyzing the multidimensional and layer-wise characteristic...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2370ms

[901/1822] how to optimize the scaling factor in low-rank adaptation to...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 2724ms

[902/1822] efficient methods for accuracy recovery when fine-tuning pru...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.849 | Time: 2287ms

[903/1822] Dataset for long-form video understanding instruction tuning...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2596ms

[904/1822] How to improve video large language model performance on lon...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2762ms

[905/1822] evaluation of ChatGPT's ability to generate FEniCS and MATLA...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2463ms

[906/1822] using prompt engineering to implement numerical models for u...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 2331ms

[907/1822] comprehensive survey of recent advances in deep learning for...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.767 | Time: 2905ms

[908/1822] review of foundation models and specialized transformer arch...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3457ms

[909/1822] How can image-to-text conversion and chain-of-thought improv...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 2544ms

[910/1822] Methods to mitigate modality imbalance in vision language mo...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 2875ms

[911/1822] How can deep neural decision trees and forests be used to im...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2806ms

[912/1822] A comparative study on using deep neural decision forests an...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2059ms

[913/1822] how to use program-driven verification and dual refinement t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2356ms

[914/1822] enhancing large language model self-correction using self-ge...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2851ms

[915/1822] How can large language models be integrated with symbolic ac...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 2589ms

[916/1822] Using action languages to bridge the gap between natural lan...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.945 | Time: 2525ms

[917/1822] how to improve the robustness of retrieval augmented generat...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.839 | Time: 2606ms

[918/1822] a training free plug and play framework for filtering malici...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2560ms

[919/1822] How can large language models use iterative self-questioning...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.860 | Time: 2685ms

[920/1822] State of the art methods for generating chronological news s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3041ms

[921/1822] How to perform efficient stylized question answering in larg...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2308ms

[922/1822] Lightweight and train-free methods for controlling LLM respo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2447ms

[923/1822] How can large language models resolve conflicts between edit...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2636ms

[924/1822] A retrieval-based framework for updating large language mode...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2931ms

[925/1822] How to generate efficient Shapley value explanations for tim...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.977 | Time: 2600ms

[926/1822] Time-series transformer architectures that use Shapley-based...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2306ms

[927/1822] speculative decoding for large language models using draft a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3197ms

[928/1822] how to implement lossless speculative decoding for accelerat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3380ms

[929/1822] enhancing neural theorem proving with large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3142ms

[930/1822] efficient recursive proving algorithms for automated theorem...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.972 | Time: 2618ms

[931/1822] autonomous red teaming of multi-host networks using large la...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2293ms

[932/1822] evaluating the effectiveness of LLM-based penetration testin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2471ms

[933/1822] dataset for panoptic segmentation-captioning with instance-s...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2643ms

[934/1822] improving region-level comprehension and language generation...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2897ms

[935/1822] how do large language models handle time-sensitive factual k...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3137ms

[936/1822] improving the accuracy and consistency of large language mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3127ms

[937/1822] How to evaluate large language models in personalized recomm...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2705ms

[938/1822] Assessing the capability of large language models to capture...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 2670ms

[939/1822] applying test-time training to large language models for imp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2370ms

[940/1822] how to improve medical reasoning in LLMs using high quality ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.946 | Time: 2841ms

[941/1822] training medical patient simulators using dialogue strategie...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2100ms

[942/1822] investigating the relationship between medical inquiry quali...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2379ms

[943/1822] Fine-grained medical vision-language pre-training using larg...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.905 | Time: 2527ms

[944/1822] How to improve medical image analysis through knowledge inje...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2315ms

[945/1822] Using LLaVA and multimodal-to-text prompt engineering for id...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2255ms

[946/1822] How can large language models and feature embeddings be appl...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2151ms

[947/1822] evaluating sparse autoencoders for llm interpretability usin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.816 | Time: 2147ms

[948/1822] how do sparse autoencoders distinguish different meanings of...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.941 | Time: 2091ms

[949/1822] how to improve self-adaptation in configurable systems using...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2479ms

[950/1822] techniques for continuous configuration optimization in inte...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2236ms

[951/1822] How to use large-scale synthetic data to improve spoken dial...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2695ms

[952/1822] Multi-turn spoken dialogue systems utilizing heterogeneous f...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3202ms

[953/1822] multi-agent conversational bandit framework for online large...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3360ms

[954/1822] how to improve online evaluation and filtering of LLM respon...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3280ms

[955/1822] How to evaluate and improve long-context language models for...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3099ms

[956/1822] Improving retrieval performance in long-context LLMs through...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3151ms

[957/1822] comprehensive benchmarks for evaluating multi-modal large la...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3262ms

[958/1822] research on the performance of mainstream multi-modal assist...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 2793ms

[959/1822] comprehensive benchmark for evaluating large language models...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2688ms

[960/1822] standardized evaluation framework for testing the performanc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 2783ms

[961/1822] How to use unlearning strategies and layer-level patching to...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.972 | Time: 2829ms

[962/1822] Defending against LLM jailbreak attacks by identifying vulne...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2732ms

[963/1822] Integrating domain knowledge from large language models with...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2475ms

[964/1822] How can large language models be used to guide Bayesian opti...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2442ms

[965/1822] synthetic data generation using multi-hop reasoning on conte...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.939 | Time: 2681ms

[966/1822] improving document-level fact checking and grounded factuali...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2464ms

[967/1822] How to implement ultra-low latency deep learning inference o...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 2316ms

[968/1822] Optimizing lookup table based neural networks on FPGAs using...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2337ms

[969/1822] Improving Tip-Adapter for vision-language models by incorpor...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2198ms

[970/1822] Theoretical understanding of training-free few-shot CLIP ada...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2120ms

[971/1822] framework for quantifying the extent of model distillation a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2227ms

[972/1822] how to evaluate the degree of knowledge distillation from te...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.842 | Time: 2716ms

[973/1822] Improving cross-lingual knowledge consistency in large langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2830ms

[974/1822] How to use self-consistent responses across different langua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2537ms

[975/1822] How can in-context learning and retrieval-augmented generati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.948 | Time: 2540ms

[976/1822] Comparing the performance of few-shot in-context learning an...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.889 | Time: 2650ms

[977/1822] How to combine slot attention with pre-trained diffusion mod...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2367ms

[978/1822] Improving compositional image generation and object discover...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2501ms

[979/1822] How effective is fine-tuning large language models for the a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2313ms

[980/1822] Investigating the relationship between reasoning complexity ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2961ms

[981/1822] How can multi-listwise preference optimization be used to im...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2068ms

[982/1822] Finetuning protein large language models for multi-attribute...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2293ms

[983/1822] evaluating the safety and vulnerability of large audio langu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.928 | Time: 2251ms

[984/1822] benchmarking large audio language models for safety alignmen...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.571 | Time: 2657ms

[985/1822] learning-based multi-turn jailbreak attack framework for lar...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2633ms

[986/1822] How can multi-turn red-teaming strategies using turn-level L...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2679ms

[987/1822] What are the primary technical challenges in developing agen...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 1957ms

[988/1822] How to integrate large language models with neural graph dat...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2268ms

[989/1822] structured prompt engineering frameworks for developing task...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.860 | Time: 2443ms

[990/1822] using natural language specifications to design conversation...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2057ms

[991/1822] How to use hybrid attention mechanisms and large language mo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2398ms

[992/1822] Academic papers on combining textual statistical features wi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2778ms

[993/1822] How to improve large language model performance by aggregati...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2454ms

[994/1822] Fine-tuning techniques for large language models to synthesi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2471ms

[995/1822] Can Looped Transformers perform neural algorithmic reasoning...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2728ms

[996/1822] Extending the neural algorithmic reasoning capabilities of L...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 2959ms

[997/1822] How to improve text-to-video generation models using text em...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2246ms

[998/1822] Enhancing text-to-video synthesis by identifying optimal tex...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2779ms

[999/1822] multilingual dataset for hate speech and abusive language de...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.953 | Time: 2576ms

[1000/1822] benchmarking machine learning models for hate speech classif...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.870 | Time: 2115ms

[1001/1822] Bridging the gap between instruction tuning and pre-training...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 3235ms

[1002/1822] Using adaptive data selection and controlled rewriting of pr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 2662ms

[1003/1822] How do padding tokens in text encoders affect the image gene...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 2579ms

[1004/1822] Causal analysis of padding token representations in text-to-...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2584ms

[1005/1822] How does adding random punctuation to mathematical prompts a...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2613ms

[1006/1822] Evaluating the vulnerability of math-specialized large langu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.863 | Time: 2563ms

[1007/1822] benchmarking multimodal large language models for complex ge...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 2356ms

[1008/1822] agent based framework for improving MLLM performance on geol...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2108ms

[1009/1822] How can adaptive projector fusion driven by user instruction...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.951 | Time: 2338ms

[1010/1822] Video large language models using instruction-based dynamic ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.871 | Time: 2591ms

[1011/1822] how does inter-model response agreement and focal loss impro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2287ms

[1012/1822] improving large language model calibration using auxiliary m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2453ms

[1013/1822] How to implement black-box watermarking for retrieval augmen...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2658ms

[1014/1822] Robust knowledge-based watermarking methods for LLM retrieva...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3066ms

[1015/1822] How effective is ChatGPT-4o at generating WCAG compliant web...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3257ms

[1016/1822] Evaluating the utility of large language models in identifyi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2521ms

[1017/1822] How can goal-conditioned reinforcement learning policies tra...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2045ms

[1018/1822] Achieving horizon generalization in RL through planning inva...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2495ms

[1019/1822] How can segment-level reward models improve reinforcement le...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.898 | Time: 2443ms

[1020/1822] Using dynamic text segmentation and location-aware normalize...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.934 | Time: 2488ms

[1021/1822] how to integrate AI-driven intrusion detection systems with ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 2211ms

[1022/1822] multilevel defense strategies combining artificial intellige...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2611ms

[1023/1822] how to add early exit branches to pre-trained deep neural ne...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.884 | Time: 2329ms

[1024/1822] optimizing the speed accuracy tradeoff in deep learning usin...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.941 | Time: 2312ms

[1025/1822] how to improve the robustness of large language models again...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2274ms

[1026/1822] techniques for maintaining faithful integrity in LLMs to pre...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2599ms

[1027/1822] How can we evaluate the ability of large language models to ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.962 | Time: 2154ms

[1028/1822] Benchmarks and datasets for assessing and improving the mark...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 3048ms

[1029/1822] how to improve retrieval augmented generation for scientific...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3102ms

[1030/1822] utilizing contextualized graph representations and dense-spa...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3330ms

[1031/1822] How can 2D Gaussian splatting be integrated with vector quan...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2993ms

[1032/1822] Using flexible 2D Gaussian features and splatting operations...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2544ms

[1033/1822] How can overlapping messages in text-based human-AI interact...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2835ms

[1034/1822] Designing conversational AI systems that support concurrent ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3156ms

[1035/1822] How can neural language models be used to prioritize configu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3160ms

[1036/1822] Techniques for accelerating configuration performance bug te...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3295ms

[1037/1822] synthetic benchmarks for evaluating deductive reasoning in l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3177ms

[1038/1822] how do state of the art reasoning models perform compared to...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 3098ms

[1039/1822] flexible modular framework for knowledge graph retrieval aug...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.802 | Time: 2705ms

[1040/1822] improving knowledge graph retrieval augmented generation by ...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 2672ms

[1041/1822] evaluating theory of mind in large language models using ver...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 2322ms

[1042/1822] a multi-choice question answering benchmark for assessing fi...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 2742ms

[1043/1822] interpretable multiple instance learning for whole slide ima...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.712 | Time: 2460ms

[1044/1822] how to use clinical concepts and vision language models for ...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 2097ms

[1045/1822] how to map multimodal llm hidden states to interpretable vis...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3310ms

[1046/1822] training free method for steering multimodal large language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3171ms

[1047/1822] How to predict the accuracy of black-box language models by ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3156ms

[1048/1822] Using follow-up query responses to identify misrepresented m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3384ms

[1049/1822] How to adaptively select semantically similar translation de...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3180ms

[1050/1822] Improving neural machine translation in large language model...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3445ms

[1051/1822] How to improve Transformer attention mechanisms by incorpora...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3352ms

[1052/1822] Efficient fine-tuning of foundation models using sparse GIN-...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3325ms

[1053/1822] How can transformer architectures be optimized for generaliz...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3204ms

[1054/1822] Improving the out-of-distribution generalization of transfor...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3286ms

[1055/1822] How can multimodal large language models be used to help end...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3318ms

[1056/1822] Interactive systems for authoring custom AI vision sensors u...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3133ms

[1057/1822] How can multi-agent large language model frameworks be used ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3355ms

[1058/1822] Framework for implementing collaborative LLM agents with spe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3316ms

[1059/1822] How do language and text-to-image models exhibit religious b...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3205ms

[1060/1822] Measuring religious stereotypes in generative AI using natur...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 3261ms

[1061/1822] benchmarking generative AI models versus traditional ion exc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2153ms

[1062/1822] how to improve generative materials discovery using post-gen...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2448ms

[1063/1822] large time series models with billion scale parameters incor...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2309ms

[1064/1822] how to use patch convolutional embedding and human feedback ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2318ms

[1065/1822] Hierarchical tree-structured recommendation system using ret...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2303ms

[1066/1822] How can RAG-enhanced hierarchical models improve the accurac...
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.653 | Time: 2557ms

[1067/1822] How to use self-questioning techniques to reduce hallucinati...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2575ms

[1068/1822] Multi-round training framework for MLLMs that uses heuristic...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3052ms

[1069/1822] Analyzing phase transitions and emergent capabilities in lar...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 2425ms

[1070/1822] How to estimate the internal dimension and parameter suffici...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2492ms

[1071/1822] How can digital twins and ray tracing be used together with ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2592ms

[1072/1822] Using a multi-step tuning process with AI to bridge the gap ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3137ms

[1073/1822] How can large language models be used to automatically gener...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.941 | Time: 2216ms

[1074/1822] Automating the transformation of REST APIs into AI compatibl...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2642ms

[1075/1822] robust nonlinear subspace clustering using data-driven kerne...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2743ms

[1076/1822] how to improve kernel-based subspace clustering by learning ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2262ms

[1077/1822] Evaluation of fine-tuned GPT-4o-mini for cost-effective and ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2827ms

[1078/1822] How can fine-tuned large language models improve de-identifi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2550ms

[1079/1822] How to improve document retrieval accuracy using zero-shot r...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2576ms

[1080/1822] Techniques for zero-shot document re-ranking that use pre-tr...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2712ms

[1081/1822] How can natural language inference models be improved to rec...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2446ms

[1082/1822] Evaluating the performance of large language models on impli...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2793ms

[1083/1822] Research comparing the emotional variance and sentiment posi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2350ms

[1084/1822] How does the EmoXpt framework analyze differences in sentime...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2212ms

[1085/1822] current research roadmap and challenges for advancing large ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2239ms

[1086/1822] six-layer vision framework for analyzing orchestration and v...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2248ms

[1087/1822] graph-based framework for generating stealthy jailbreak prom...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2398ms

[1088/1822] how can interconnected graph structures with pruning improve...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2484ms

[1089/1822] How can zero-shot large language models and prompt engineeri...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.725 | Time: 2454ms

[1090/1822] Effective frameworks for using LLMs to evaluate student comp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3118ms

[1091/1822] How does the AIN bilingual multimodal model achieve state-of...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 2504ms

[1092/1822] Development of a large multimodal model for Arabic and Engli...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3136ms

[1093/1822] mathematical framework that unifies preference optimization ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 2470ms

[1094/1822] impact of optimization objectives and explicit reward models...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2168ms

[1095/1822] how to reduce activation memory during transformer fine-tuni...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3471ms

[1096/1822] efficient fine-tuning methods for large language models that...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3210ms

[1097/1822] Bayes-optimal generalisation error for shallow two-layer neu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2519ms

[1098/1822] Phase transition from universal to specialisation learning i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2404ms

[1099/1822] How can RAG and chain-of-thought reasoning be used with larg...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.772 | Time: 2608ms

[1100/1822] Using RAG-based agentic LLMs and chain-of-thought prompting ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.953 | Time: 2774ms

[1101/1822] evaluating large language models for longitudinal clinical s...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.687 | Time: 2206ms

[1102/1822] performance of retrieval augmented generation and chain of t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2514ms

[1103/1822] How to improve large language model reasoning accuracy by co...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2494ms

[1104/1822] Research on formalizing natural language reasoning tasks for...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2768ms

[1105/1822] How to improve resource efficiency in compound AI systems us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3358ms

[1106/1822] Architectural designs for decoupling orchestration and resou...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3380ms

[1107/1822] Using pre-trained foundational vision transformers like DINO...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 3060ms

[1108/1822] Machine learning of material properties from microstructures...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 2583ms

[1109/1822] how to perform knowledge distillation from large transformer...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2734ms

[1110/1822] distilling large scale transformer language models into rwkv...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2413ms

[1111/1822] how to trigger hallucinations in multimodal large language m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2349ms

[1112/1822] transferable visual adversarial attacks against multimodal m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2240ms

[1113/1822] how to combine large language models with symbolic solvers u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2378ms

[1114/1822] neurosymbolic framework for improving large language model p...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2488ms

[1115/1822] survey of AI researchers' opinions on existential risk and t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1924ms

[1116/1822] empirical study on why AI experts disagree about catastrophi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2285ms

[1117/1822] preference datasets and benchmarks for evaluating reward mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2834ms

[1118/1822] how to develop reward models for trustworthy long-context ge...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2164ms

[1119/1822] How can legal concept generation and Determinantal Point Pro...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2439ms

[1120/1822] Improving legal document retrieval by augmenting query facts...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2840ms

[1121/1822] AI storytelling system using character symbol manipulation a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2440ms

[1122/1822] How can symbolic motions from toy-playing be used to guide l...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2518ms

[1123/1822] how to optimize large language model serving for multiple se...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2230ms

[1124/1822] efficient LLM inference systems that utilize hardware-aware ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2314ms

[1125/1822] how to use agentic frameworks and character knowledge graphs...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.964 | Time: 2541ms

[1126/1822] improving the factual accuracy of large language model summa...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2199ms

[1127/1822] How to optimize confidence thresholds in large language mode...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2327ms

[1128/1822] Probabilistic modeling of joint error distributions to tune ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2837ms

[1129/1822] Efficient LLM serving systems that co-locate latency-sensiti...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2413ms

[1130/1822] How can interference-aware scheduling and latency prediction...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 2212ms

[1131/1822] How to reduce communication overhead in distributed large la...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2581ms

[1132/1822] Techniques for overlapping communication with computation in...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2439ms

[1133/1822] instruction tuning for video facial expression captioning an...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2503ms

[1134/1822] how can multimodal large language models be trained to provi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2270ms

[1135/1822] How can multi-neuromodulatory systems like dopamine and nora...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2370ms

[1136/1822] Bio-inspired learning rules using multi-scale neuromodulatio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2925ms

[1137/1822] How to effectively integrate multiple vision encoders in mul...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.924 | Time: 2378ms

[1138/1822] Advanced fusion strategies for hybrid multimodal large langu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2335ms

[1139/1822] What are the current state of the art techniques for early e...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2494ms

[1140/1822] Comprehensive review of methodologies using intermediate lay...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2448ms

[1141/1822] how to implement remote and automatic cognitive impairment s...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2207ms

[1142/1822] performance of large language models like DistilBERT in clas...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2467ms

[1143/1822] How do large language models reflect and amplify stereotypes...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2808ms

[1144/1822] Measuring representational harms and social bias against non...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 2513ms

[1145/1822] how to exploit inter-iteration and intra-iteration output sp...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2550ms

[1146/1822] software-hardware co-design for diffusion models using ffn r...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2643ms

[1147/1822] how to achieve low-bit quantization of text-to-image diffusi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 2761ms

[1148/1822] addressing activation outliers and cross-attention distribut...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2303ms

[1149/1822] How to combine spatial layout information and semantic text ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2184ms

[1150/1822] A hybrid method for document segmentation using bounding box...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3400ms

[1151/1822] improving visual reasoning in large vision-language models u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2602ms

[1152/1822] DrivingVQA dataset for complex visual question answering in ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2628ms

[1153/1822] Using explainable AI and machine learning to distinguish bet...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.511 | Time: 2726ms

[1154/1822] Differentiating between multiple LLM sources using deep lear...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 2542ms

[1155/1822] How can large language models be used for document-level tex...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2308ms

[1156/1822] Improving document simplification performance in large langu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.951 | Time: 2864ms

[1157/1822] how can we evaluate the clinical appropriateness of conversa...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2126ms

[1158/1822] benchmarking large language model based psychiatric agents t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.949 | Time: 2150ms

[1159/1822] How can transformer-based models like mT5 and BanglaT5 be ap...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2501ms

[1160/1822] Fine-tuning pre-trained language models for solving Bengali ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2542ms

[1161/1822] How to improve text-image alignment in diffusion-based style...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.957 | Time: 2443ms

[1162/1822] Recent methods for balancing textual semantics and stylistic...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2289ms

[1163/1822] adversarial attacks on LLM routing systems to manipulate mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2987ms

[1164/1822] how can an adversary use confounder gadgets to compromise th...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3017ms

[1165/1822] A large-scale dataset of cybersecurity-specific prompts for ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 2833ms

[1166/1822] Evaluating large language model security using close-ended p...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2323ms

[1167/1822] How can Large Language Models be used to perform interpretab...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2378ms

[1168/1822] Using adaptive retrieval-augmented generation to bridge soci...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2281ms

[1169/1822] How can retrieval augmented generation and in-context learni...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 2632ms

[1170/1822] A framework for dynamic retrieval of regional cultural value...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 2846ms

[1171/1822] How does information bottleneck theory explain the internal ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 1991ms

[1172/1822] Improving large language model reasoning and inference effic...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2723ms

[1173/1822] How does data contamination in pre-training sets affect the ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 2649ms

[1174/1822] Controlled study measuring the impact of source and target d...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2541ms

[1175/1822] Why do adaptive optimizers like Adam outperform SGD in trans...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.964 | Time: 2401ms

[1176/1822] Analysis of how layer normalization placement affects gradie...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2401ms

[1177/1822] How can Generative Adversarial Networks be used to predict s...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2121ms

[1178/1822] Detecting smart grid instability and adversarial attacks usi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2534ms

[1179/1822] how to erase NSFW concepts from text to image diffusion mode...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2635ms

[1180/1822] efficient concept erasure for diffusion models using a seman...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2277ms

[1181/1822] benchmark for evaluating the effectiveness of large language...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2492ms

[1182/1822] comparing advanced reasoning models and classical LLMs on th...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.935 | Time: 2568ms

[1183/1822] generative multimodal large language models for explicit sem...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 2109ms

[1184/1822] large scale dataset and framework for training event-based v...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.457 | Time: 2659ms

[1185/1822] How can we automatically generate high-quality dialogue benc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2494ms

[1186/1822] Using query-based subgraph retrieval and multi-stage LLM pip...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 2833ms

[1187/1822] How effective are vision-language models like GPT and Gemini...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2419ms

[1188/1822] A study on using VLMs for the automated assessment of AR sce...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2752ms

[1189/1822] How to use ensemble models and committee voting strategies t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2640ms

[1190/1822] Improving dataset distillation performance by leveraging col...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2860ms

[1191/1822] How to use ChatGPT and tag-based data analysis to generate p...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2445ms

[1192/1822] A method for converting student learning behavior data into ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.910 | Time: 2107ms

[1193/1822] retrosynthesis prediction using dual graph representations f...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2107ms

[1194/1822] how to improve retrosynthesis prediction by combining dual g...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2181ms

[1195/1822] How can reconfigurable optical circuit switching improve com...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2584ms

[1196/1822] Improving training cost efficiency of MoE models using a reg...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2261ms

[1197/1822] how to use gpt models and hierarchical summarization for the...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.815 | Time: 2390ms

[1198/1822] leveraging large language models and prompt engineering for ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.905 | Time: 2367ms

[1199/1822] automated neural architecture search and compression techniq...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 2482ms

[1200/1822] how to design low latency neural networks for bragg peak fin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.820 | Time: 2506ms

[1201/1822] How can large language models be used as automated simulator...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.970 | Time: 2303ms

[1202/1822] Evaluating concept-based explanation methods through automat...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2261ms

[1203/1822] How to reduce hallucinations in multimodal large language mo...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.972 | Time: 2842ms

[1204/1822] A post-pretraining method for improving visual representatio...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 2126ms

[1205/1822] How to use multiple large language models to generate high q...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2475ms

[1206/1822] Improving scientific figure captioning by using multimodal L...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2804ms

[1207/1822] How can we identify and neutralize backdoor trigger tokens i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2299ms

[1208/1822] Defending against backdoor attacks in natural language proce...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2548ms

[1209/1822] How do large language models simplify or omit representation...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2449ms

[1210/1822] Evaluating cultural representational gaps and power inequiti...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2445ms

[1211/1822] How can semantic graphs and uncertainty propagation between ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2787ms

[1212/1822] Improving uncertainty-based hallucination detection by model...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2413ms

[1213/1822] how to generate counterfactual examples for natural language...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 2512ms

[1214/1822] framework for automatic counterfactual generation using labe...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2279ms

[1215/1822] autonomous driving framework using dual-process decision mak...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2120ms

[1216/1822] improving autonomous vehicle navigation through cognitive pe...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2052ms

[1217/1822] how to improve video large language models by integrating mu...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2464ms

[1218/1822] leveraging multiple frozen vision backbones to create unifie...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2009ms

[1219/1822] lightweight and explainable intrusion detection systems for ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2246ms

[1220/1822] how to improve transparency and computational efficiency in ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2309ms

[1221/1822] applying compositional diffusion models for 6 degree-of-free...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2033ms

[1222/1822] how can generative diffusion policies be used for few-shot a...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 2039ms

[1223/1822] How can block-wise mixed format quantization using FP4 diale...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2282ms

[1224/1822] Fine-grained block-level quantization techniques for LLMs th...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2720ms

[1225/1822] evaluating multi-step tool use reasoning in large language m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2320ms

[1226/1822] comparison of process supervised reward models and outcome s...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2361ms

[1227/1822] how to use hadamard rotations to mitigate activation and wei...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2493ms

[1228/1822] low-precision fine-tuning of transformers using hadamard-ass...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2457ms

[1229/1822] How to implement efficient context pruning using sequence la...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 2351ms

[1230/1822] A robust approach for combining context reranking and sequen...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.767 | Time: 2807ms

[1231/1822] How can large language models and knowledge graphs be used t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3386ms

[1232/1822] Multi-branched reaction pathway search algorithm for identif...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3342ms

[1233/1822] datasets for training foundation models on linked business t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.792 | Time: 2435ms

[1234/1822] what datasets are available for research in multi-table repr...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2452ms

[1235/1822] hybrid retrieval-augmented generation framework for universi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.923 | Time: 2120ms

[1236/1822] effective strategies for implementing unified RAG systems in...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.761 | Time: 2079ms

[1237/1822] How to achieve zero-shot vision to speech generalization in ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2501ms

[1238/1822] Open-source omnimodal models for real-time emotional speech ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2336ms

[1239/1822] How can large language models improve their own critique and...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.904 | Time: 2834ms

[1240/1822] Enhancing LLM self-critique capabilities through self-genera...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.672 | Time: 2962ms

[1241/1822] How can large language models improve long-term memory in vo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2587ms

[1242/1822] Enhancing in-car voice assistant memory by using LLMs to ext...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2948ms

[1243/1822] How to use Conditional Value at Risk in reinforcement learni...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2199ms

[1244/1822] Risk-averse fine-tuning methods for large language models to...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3141ms

[1245/1822] How to use large language models for lexicon-based text embe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3149ms

[1246/1822] Improving sparse or lexicon-based embedding performance on t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3350ms

[1247/1822] How can domain-adversarial fine-tuning improve the generaliz...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3123ms

[1248/1822] Enhancing small language model chain of thought performance ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 2658ms

[1249/1822] using large language models for automated regression test ge...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2433ms

[1250/1822] feedback directed zero shot LLM approach for generating repr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2468ms

[1251/1822] How to address influence score bias in data selection for in...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2771ms

[1252/1822] Balanced and influential data selection techniques for instr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2735ms

[1253/1822] how to use large language models for knowledge graph complet...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.828 | Time: 2918ms

[1254/1822] automated curriculum modeling and topic extraction from lect...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2839ms

[1255/1822] applying large language models to few-shot multivariate time...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.890 | Time: 2568ms

[1256/1822] how to leverage pre-trained large language models for few-sh...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2581ms

[1257/1822] Evaluating the effectiveness of open-source and commercial l...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2103ms

[1258/1822] How do large language models compare to human teaching assis...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2451ms

[1259/1822] evaluating the fidelity and constraint satisfaction of large...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.937 | Time: 2400ms

[1260/1822] systematic framework and error-correction mechanisms for ass...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3131ms

[1261/1822] how can large language models perform zero-shot image and au...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3002ms

[1262/1822] a training-free approach using gradient-free optimization an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2498ms

[1263/1822] methods for separating motion concepts from appearance in te...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2501ms

[1264/1822] using temporal attention purification and appearance highway...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2430ms

[1265/1822] How can chain-of-thought reasoning be applied to improve lar...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2521ms

[1266/1822] Using proactive intent analysis and reasoning steps to enhan...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2833ms

[1267/1822] fine-tuning large language models using ensembles of LoRA ex...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 2823ms

[1268/1822] how can clustering training data by gradient directions and ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.939 | Time: 2726ms

[1269/1822] How can trie-based prefix tree structures be used to optimiz...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2417ms

[1270/1822] Efficient beam search algorithms for LLMs that use shared pr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2472ms

[1271/1822] how effective are large language models at extracting specie...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2593ms

[1272/1822] using large language models for automated extraction of spec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2580ms

[1273/1822] limitations of current large language model cybersecurity ev...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 2289ms

[1274/1822] comprehensive risk assessment framework for LLM cyber capabi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2124ms

[1275/1822] How do one-layer attention-only transformers develop interna...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 2440ms

[1276/1822] Understanding how training data features shape the internal ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 2256ms

[1277/1822] How to use preference optimization to intrinsically reduce h...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.949 | Time: 2370ms

[1278/1822] Improving the reliability of LLM machine translation by fine...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 3216ms

[1279/1822] comparing the performance of LSTM deep learning models and A...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2153ms

[1280/1822] how do Long Short-Term Memory networks compare to traditiona...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.964 | Time: 2588ms

[1281/1822] How does the integration of generative AI tools and experien...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2065ms

[1282/1822] Research on human-AI collaboration in business education foc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.970 | Time: 2697ms

[1283/1822] How can multimodal large language models perform GUI groundi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2714ms

[1284/1822] State of the art methods for visual GUI agent grounding and ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2481ms

[1285/1822] How to improve the safety of large language model reward mod...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2315ms

[1286/1822] Dynamic selection of the most important safety rules to maxi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2568ms

[1287/1822] How to optimize the order of in-context learning examples fo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2342ms

[1288/1822] Dataset-free methods for finding the best sequence of few-sh...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2925ms

[1289/1822] systematic hyperparameter tuning for LLM-as-a-judge using mu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.949 | Time: 2430ms

[1290/1822] how to find cost-efficient open-weight LLM judges through mu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.822 | Time: 2432ms

[1291/1822] how to integrate domain specific large language models into ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 2394ms

[1292/1822] leveraging a suite of general and specialized LLMs within an...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2787ms

[1293/1822] Evaluating large language models and artificial intelligence...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 2058ms

[1294/1822] Implications of AI personhood and moral status for the ethic...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2240ms

[1295/1822] how to improve large language model reasoning and explainabi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2470ms

[1296/1822] using selective tree exploration and supervised fine tuning ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.901 | Time: 2686ms

[1297/1822] how to perform open-set test-time adaptation for multimodal ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2697ms

[1298/1822] robust multimodal test-time adaptation methods for identifyi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2505ms

[1299/1822] evaluating the correlation between cross-attention weights i...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.953 | Time: 2354ms

[1300/1822] investigating the plausibility of using cross-attention weig...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2380ms

[1301/1822] How can specific attention heads in LLMs be used for trainin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2610ms

[1302/1822] Techniques for improving large language model performance on...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2856ms

[1303/1822] How can Mixture-of-Experts models be improved by removing ro...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2415ms

[1304/1822] Research on expert self-selection mechanisms in language mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2645ms

[1305/1822] How to evaluate fact-conflicting hallucinations in small lan...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2430ms

[1306/1822] Benchmarking the factual analysis and context reasoning capa...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2324ms

[1307/1822] using direct preference optimization to generate multiple ch...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 2460ms

[1308/1822] training a model to predict student choices for generating m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2320ms

[1309/1822] How can large language models be jailbroken using positive s...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.820 | Time: 2537ms

[1310/1822] Effective jailbreak attack methods for large language models...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 2382ms

[1311/1822] benchmark for text-driven image editing evaluation with huma...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 2274ms

[1312/1822] multi-modality source-aware quality assessment metric for ev...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2220ms

[1313/1822] How can Pointwise V-Information based fine-tuning improve th...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2837ms

[1314/1822] A comprehensive dataset and evaluation framework for trainin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2709ms

[1315/1822] Scalable signature-based algorithm for path-dependent hedgin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2184ms

[1316/1822] Mathematical framework for deep hedging alternatives using u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1996ms

[1317/1822] alternative memory architectures for AI inference that optim...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2640ms

[1318/1822] how does managed-retention memory improve energy efficiency ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.942 | Time: 2344ms

[1319/1822] How to improve short text classification using graph learnin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2506ms

[1320/1822] Novel methods for addressing semantic sparsity in short text...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2840ms

[1321/1822] How to use class-specific visual prompts to improve the inte...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2272ms

[1322/1822] Improving saliency maps in Vision Transformers for fine-grai...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2534ms

[1323/1822] How can an autonomous multi-agent framework using dynamic ro...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 2138ms

[1324/1822] Large language model based multi-agent systems for psycholog...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2267ms

[1325/1822] iterative reinforced fine tuning using Monte Carlo Tree Sear...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2528ms

[1326/1822] addressing fragment deficiency and parameter errors in LLM t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.874 | Time: 2618ms

[1327/1822] how to use large language models and graph embedding techniq...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2043ms

[1328/1822] fine-tuning pre-trained large language models with graph and...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.396 | Time: 2820ms

[1329/1822] architectural methods for overlapping communication and comp...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2377ms

[1330/1822] improving distributed transformer inference speed by decoupl...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2185ms

[1331/1822] how to improve large language model performance on complex c...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2387ms

[1332/1822] effective agentic frameworks for overcoming reasoning and lo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3345ms

[1333/1822] How do large language models perform in detecting smart cont...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3403ms

[1334/1822] Reducing false positive rates in LLM-based smart contract se...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3250ms

[1335/1822] How to use model weight averaging during sequential fine-tun...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3314ms

[1336/1822] Mitigating forgetting in diverse domain continual learning b...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3174ms

[1337/1822] How can influence functions be used to identify labeler bias...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3222ms

[1338/1822] Efficient methods for measuring the impact of specific human...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3329ms

[1339/1822] comparing the performance of large language models and encod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3225ms

[1340/1822] how well do large language models perform at segment-level q...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3370ms

[1341/1822] comprehensive evaluation framework for assessing large langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3138ms

[1342/1822] how to benchmark financial large language models on professi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3305ms

[1343/1822] systematic mapping study on ethical risks and mitigation str...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3134ms

[1344/1822] current challenges and frameworks for mitigating ethical con...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3011ms

[1345/1822] Open source Python library for evaluating bias and fairness ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 2881ms

[1346/1822] How can I assess algorithmic bias in LLM responses using an ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2918ms

[1347/1822] How to use visual Hopfield networks and associative memory t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2098ms

[1348/1822] Improving LLM-based medical report generation by mining dise...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2253ms

[1349/1822] Using zero-shot prompting with open-source large language mo...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.932 | Time: 2348ms

[1350/1822] Evaluation of large language models for scoring transcribed ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 2378ms

[1351/1822] How to improve the inference efficiency of LLM-based recomme...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2256ms

[1352/1822] Optimizing accuracy and latency in large language model reco...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2588ms

[1353/1822] How can explicit visual prompts like markers and pointers be...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2299ms

[1354/1822] Framework for integrating medical entity extraction and visu...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.761 | Time: 2743ms

[1355/1822] How can model predictive control frameworks improve the plan...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2443ms

[1356/1822] Using large language models as implicit cost function minimi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 2450ms

[1357/1822] How does transfer learning work in variational quantum circu...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 2069ms

[1358/1822] Analytical fine-tuning methods for adapting pretrained varia...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.957 | Time: 2572ms

[1359/1822] benchmarks for evaluating multimodal large language models o...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2809ms

[1360/1822] large scale dataset for cross-event and within-event reasoni...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2490ms

[1361/1822] How can image-based multimodal large language models be used...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2488ms

[1362/1822] Techniques for improving the black-box transferability of ad...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2561ms

[1363/1822] comprehensive review of methodologies for applying large lan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2267ms

[1364/1822] what are the differences between graph2text and graph2token ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2385ms

[1365/1822] How can token-level uncertainty and attention mechanisms be ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2627ms

[1366/1822] Lightweight decoding strategies for improving faithfulness t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2365ms

[1367/1822] impact of explicit symmetry breaking and geometric reference...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 1901ms

[1368/1822] benchmarking trade-offs between group equivariance and symme...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.793 | Time: 2162ms

[1369/1822] How can the IDADP framework help large language models detec...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2375ms

[1370/1822] Effective prompting strategies for zero-shot irony comprehen...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.840 | Time: 2334ms

[1371/1822] How do large language models compare to humans in open-ended...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 2474ms

[1372/1822] Analyzing the limitations of LLM exploration through sparse ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2968ms

[1373/1822] Systematization of knowledge on security vulnerabilities and...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.910 | Time: 2220ms

[1374/1822] A comprehensive study on the risks of code hallucinations an...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 2394ms

[1375/1822] evaluating the fairness and robustness of commercial AI safe...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 2148ms

[1376/1822] how do safety moderation classifiers used as LLM guardrails ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 2716ms

[1377/1822] how to improve multimodal language models for mental health ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2196ms

[1378/1822] multimodal framework for fine grained anxiety symptom detect...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2584ms

[1379/1822] enhancing retrieval augmented generation for complex reasoni...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.934 | Time: 2395ms

[1380/1822] how to improve agentic RAG performance with self-consistency...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2272ms

[1381/1822] how can multimodal large language models use image represent...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.951 | Time: 2437ms

[1382/1822] using spatial intelligence of multimodal LLMs and visual gra...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2584ms

[1383/1822] how to use large language models and retrieval augmented gen...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.671 | Time: 2305ms

[1384/1822] enhancing automated short answer grading reliability with RA...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2412ms

[1385/1822] How to improve membership inference attack accuracy in large...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2221ms

[1386/1822] Detecting pretraining data leakage in large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2433ms

[1387/1822] How can lightweight models be used for evidence extraction t...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2380ms

[1388/1822] Comparison of quote-first-then-answer strategies versus full...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2623ms

[1389/1822] meta-learning for parameter initialization in variational qu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2424ms

[1390/1822] how to apply MAML-based classical neural networks for findin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2017ms

[1391/1822] LLM-based framework for automated scientific research using ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2393ms

[1392/1822] How to automate the scientific research process through iter...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2342ms

[1393/1822] CharToM benchmark for evaluating theory of mind in large lan...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 2065ms

[1394/1822] Comparing human and AI theory of mind performance when reaso...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 2145ms

[1395/1822] Theoretical analysis of diffusion models for nonparametric d...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2357ms

[1396/1822] How do sparse weight-sharing neural network architectures in...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3396ms

[1397/1822] Improving large language model reasoning performance through...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3312ms

[1398/1822] How can recursive sub-task breakdown and advanced scoring me...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3283ms

[1399/1822] Implementation of Retrieval-Augmented Generation using BGE-M...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.624 | Time: 3009ms

[1400/1822] Evaluating localized RAG systems for data privacy and perfor...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 2301ms

[1401/1822] iterative pruning methods for diffusion models using gradien...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3321ms

[1402/1822] how to apply progressive soft pruning and gradient flow crit...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2718ms

[1403/1822] large scale dataset and benchmark for fake news detection in...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2282ms

[1404/1822] performance of large language models with quantized low rank...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.842 | Time: 2210ms

[1405/1822] evaluating large language models on aerospace manufacturing ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 2297ms

[1406/1822] assessing the accuracy and hallucination risks of GPT-4 and ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.853 | Time: 2463ms

[1407/1822] Improving multimodal hierarchical classification accuracy by...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2880ms

[1408/1822] How to integrate hierarchical class relationships into multi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2744ms

[1409/1822] evaluating the performance of large language models for ment...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 2311ms

[1410/1822] benchmarking multilingual and bilingual LLMs on Arabic menta...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.550 | Time: 2688ms

[1411/1822] How to use a proximal operator and local correlation regular...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2519ms

[1412/1822] Efficient methods for inducing structured 2:4 sparsity in ne...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2408ms

[1413/1822] How do task-in-prompt adversarial attacks use sequence-to-se...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.945 | Time: 2431ms

[1414/1822] Evaluating LLM jailbreak vulnerabilities using the PHRYGE be...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 2448ms

[1415/1822] consensus-based optimization methods for derivative-free non...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3315ms

[1416/1822] how to apply mirror maps and bregman distances to particle-b...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2258ms

[1417/1822] How to use open-source large language models for automatic d...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 2503ms

[1418/1822] Improving zero-shot classification performance of open-sourc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2784ms

[1419/1822] How can privacy guardrails like OneShield be used to detect ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2535ms

[1420/1822] Effective frameworks for mitigating privacy risks in enterpr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2794ms

[1421/1822] How can specialized datasets like PlanGTG with reordering an...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2763ms

[1422/1822] Improving graph to text generation in large language models ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3074ms

[1423/1822] How to use Z-order curves and dimensionality reduction to en...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2159ms

[1424/1822] Efficient top-k attention mechanisms for causal transformers...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2515ms

[1425/1822] How can semantic analysis and natural language processing be...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2226ms

[1426/1822] A computational architecture for linking systematic analytic...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2621ms

[1427/1822] Scaling in-context reinforcement learning for cross-domain a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2935ms

[1428/1822] How does algorithm distillation compare to expert distillati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.952 | Time: 2073ms

[1429/1822] generating realistic limit order book simulations using tran...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2303ms

[1430/1822] how to evaluate the realism of synthetic market data generat...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2476ms

[1431/1822] How to use embedding-based data perturbation and Tsetlin Mac...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2757ms

[1432/1822] Improving adversarial attacks on text detectors by combining...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2923ms

[1433/1822] convergence rate of probability flow ODE samplers in diffusi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2251ms

[1434/1822] how do probability flow ODEs in score-based generative model...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2432ms

[1435/1822] Automated black-box safety testing of large language models ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2630ms

[1436/1822] How can LLMs be used as test oracles to identify harmful res...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2565ms

[1437/1822] How can hierarchical contrastive learning and concept memori...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.880 | Time: 2554ms

[1438/1822] Improving the accuracy of generative language models in pred...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 2555ms

[1439/1822] information theoretic framework for multi-bit watermarking i...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.789 | Time: 2499ms

[1440/1822] optimal schemes for distributional information embedding to ...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 2127ms

[1441/1822] Can fine-tuned large language models effectively control spa...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 2598ms

[1442/1822] Comparison of fine-tuned foundation models and traditional d...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.951 | Time: 2454ms

[1443/1822] hybrid machine learning and biophysical models for predictin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2185ms

[1444/1822] combining neural networks with physiological models to impro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2406ms

[1445/1822] How to finetune large language models using a diffusion fram...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2402ms

[1446/1822] Integrating diffusion processes into autoregressive models f...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2585ms

[1447/1822] datasets and benchmarks for fine grained span level chinese ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 2359ms

[1448/1822] evaluating the performance of large language models on ident...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2323ms

[1449/1822] how to apply negative feedback mechanisms from control theor...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2367ms

[1450/1822] efficient weight-only quantization for large language models...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2354ms

[1451/1822] comprehensive survey of deep learning architectures and thei...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1907ms

[1452/1822] how neural networks like CNNs and autoencoders are used for ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2134ms

[1453/1822] machine learning based qubit readout workflow using QICK and...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2749ms

[1454/1822] how to implement hardware efficient neural networks for sing...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2444ms

[1455/1822] Causal pre-processing methods for resolving the fairness-acc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2535ms

[1456/1822] How can approximating a fictitious and normatively desired w...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2661ms

[1457/1822] Improving process reward modeling for mathematical reasoning...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.911 | Time: 2230ms

[1458/1822] How does hierarchical refinement and step merging affect the...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2367ms

[1459/1822] efficient methods for mitigating catastrophic forgetting in ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2431ms

[1460/1822] how to balance domain adaptation and general knowledge prese...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2729ms

[1461/1822] Does professional artistic expertise improve the quality of ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2117ms

[1462/1822] Experimental study on the transfer of traditional artistic s...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.860 | Time: 2476ms

[1463/1822] How can generative large language models be used for ranking...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2349ms

[1464/1822] Unified framework for multi-modal question answering that co...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2686ms

[1465/1822] evaluating large language models mathematical reasoning capa...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2639ms

[1466/1822] how does randomizing variables in math word problems help de...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2602ms

[1467/1822] evaluation of large language models on their ability to tran...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2259ms

[1468/1822] do current automatic evaluation metrics like BLEU and COMET ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2532ms

[1469/1822] Multi-agent deep reinforcement learning for target localizat...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.924 | Time: 2651ms

[1470/1822] Collaborative multi-agent system for radioactive source loca...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2660ms

[1471/1822] How do professional software developers perceive the readabi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2357ms

[1472/1822] Investigating the impact of LLM-based software development a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2130ms

[1473/1822] multimodal masked autoencoder framework for denoising modula...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2214ms

[1474/1822] self-supervised pretraining methods for automatic modulation...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2195ms

[1475/1822] How can reinforcement learning and dynamic early exit strate...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2159ms

[1476/1822] Frameworks for optimizing the trade-off between energy consu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2189ms

[1477/1822] How to perform domain adaptation of Llama 3.1 for e-commerce...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2620ms

[1478/1822] Techniques for adapting large language models to the e-comme...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2588ms

[1479/1822] analysis of the Gaussian distribution and statistical proper...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2172ms

[1480/1822] understanding why large foundation model weights follow Gaus...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2338ms

[1481/1822] How can prompt-based Monte Carlo Tree Search with dynamic ex...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2509ms

[1482/1822] Improving large model reasoning on SciEval datasets using ad...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2800ms

[1483/1822] supervised contrastive knowledge distillation for few-shot c...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2457ms

[1484/1822] how to alleviate catastrophic forgetting in class-incrementa...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2433ms

[1485/1822] How can text-driven adaptation and modality alignment be use...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2733ms

[1486/1822] Methodology for aligning image and text embeddings to perfor...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2872ms

[1487/1822] How to use weighted maximum likelihood estimation in RLHF to...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2607ms

[1488/1822] Improving reinforcement learning from human feedback by addr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2082ms

[1489/1822] How can multimodal AI agents in augmented reality proactivel...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2491ms

[1490/1822] Determining optimal intervention timing for proactive AI ass...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2576ms

[1491/1822] benchmarking multilingual gender neutral translation capabil...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2173ms

[1492/1822] systematic evaluation of inclusive translation and gender ne...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2000ms

[1493/1822] How do large language models perform on patient data extract...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.860 | Time: 2373ms

[1494/1822] Benchmarking Llama2 and Meditron for structured clinical dat...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2791ms

[1495/1822] How to improve the ability of large language models to follo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2009ms

[1496/1822] Techniques for training large language models to satisfy mul...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2496ms

[1497/1822] How can weight recompute and computational graph rearrangeme...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1876ms

[1498/1822] Efficient fine-tuning methods for sparse large language mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3325ms

[1499/1822] How can multimodal large language models be used for zero-sh...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2898ms

[1500/1822] Improving the alignment of multimodal LLMs with human aesthe...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2871ms

[1501/1822] How to improve text to speech for research papers with compl...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2473ms

[1502/1822] Text-to-speech systems for visually impaired researchers to ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 2881ms

[1503/1822] How to detect machine-generated academic essays in English a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2599ms

[1504/1822] Advanced techniques for identifying AI-written academic pape...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2960ms

[1505/1822] How to improve the efficiency of tree search in large langua...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2264ms

[1506/1822] Reducing computational costs and redundancy in LLM multi-ste...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2476ms

[1507/1822] Applying two-fold curriculum learning and proximal policy op...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 2328ms

[1508/1822] How can curriculum learning and variational autoencoders be ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.824 | Time: 2456ms

[1509/1822] How to use video-grounded entailment tree reasoning to impro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2265ms

[1510/1822] Frameworks for integrating entailment tree construction and ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2707ms

[1511/1822] How can stochastic distribution embeddings and Wasserstein s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2296ms

[1512/1822] Deep learning models for knowledge tracing that incorporate ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2898ms

[1513/1822] How to evaluate social bias in large language models specifi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2548ms

[1514/1822] Benchmarks and metrics for assessing fairness and social bia...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 2427ms

[1515/1822] How can self-knowledge distillation and logit standardizatio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2749ms

[1516/1822] State of the art generative dataset distillation methods foc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 2768ms

[1517/1822] benchmarking large language models on linguistic reasoning t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2509ms

[1518/1822] evaluating the performance of LLMs on self-contained linguis...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3292ms

[1519/1822] How to improve large language model chain of thought reasoni...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.799 | Time: 2221ms

[1520/1822] Techniques for handling implicit or missing information in L...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2976ms

[1521/1822] How can hierarchical reinforcement learning and biometric fe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3196ms

[1522/1822] Integrating neurobiological data and multi-agent systems for...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3138ms

[1523/1822] How to protect the copyright of hardware description languag...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2382ms

[1524/1822] Methods for embedding robust watermarks into Verilog RTL and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3366ms

[1525/1822] using curiosity-driven reinforcement learning to automatical...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.878 | Time: 2088ms

[1526/1822] automated generation of adversarial prompts to identify bias...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2458ms

[1527/1822] how to improve x-ray prohibited item detection performance w...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2333ms

[1528/1822] data augmentation methods for robust object detection in x-r...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2128ms

[1529/1822] search-based software engineering framework for automated to...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2134ms

[1530/1822] how can evolutionary algorithms and iterative prompt generat...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.525 | Time: 2319ms

[1531/1822] How to use natural language processing on corporate 10-K fil...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2423ms

[1532/1822] A data-driven methodology for measuring firm-level AI engage...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2041ms

[1533/1822] benchmark dataset for evaluating the linguistic diversity an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3270ms

[1534/1822] how do current open-vocabulary 3D visual grounding methods p...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3348ms

[1535/1822] dynamic context-aware positional encoding for improving long...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3185ms

[1536/1822] improving transformer performance using equivariant position...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3235ms

[1537/1822] zero-shot multi-hop question answering over hybrid sources o...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3334ms

[1538/1822] how to construct a unified hybrid graph from tabular and tex...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3303ms

[1539/1822] How to use hybrid static and dynamic fingerprinting techniqu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3087ms

[1540/1822] Identifying large language models in multi-agent systems and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3232ms

[1541/1822] Analyzing the risks of software supply chain attacks through...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3104ms

[1542/1822] Relationship between HumanEval coding benchmarks and the pro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3267ms

[1543/1822] How to design a flexible LLM re-ranker with configurable dep...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3128ms

[1544/1822] Efficient large language model re-ranking techniques for pas...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2784ms

[1545/1822] How can projection-free algorithms be used to solve online c...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2347ms

[1546/1822] projection-free online learning policies that utilize linear...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2246ms

[1547/1822] How to detect backdoors in deep neural networks by analyzing...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3315ms

[1548/1822] Trojan scanning methods for deep learning models that work a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2553ms

[1549/1822] How do open-source contributions impact Large Language Model...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2395ms

[1550/1822] A comparative analysis of open-source versus proprietary Lar...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2365ms

[1551/1822] In-context reinforcement learning for few-shot budget alloca...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.936 | Time: 2272ms

[1552/1822] How to optimize budget allocation across stages in online ad...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2171ms

[1553/1822] How to use GraphRAG and retrieve-divide-solve agent pipeline...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2385ms

[1554/1822] Knowledge graph based RAG framework for exploring protein-pr...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 2974ms

[1555/1822] How to use unsupervised domain adaptation and graph-based kn...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2868ms

[1556/1822] Improving cross-modal feature representation in text-to-imag...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2708ms

[1557/1822] How to perform zero-shot verification of large language mode...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2288ms

[1558/1822] Methods for evaluating and guiding large language model reas...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 2444ms

[1559/1822] How to use diffusion models for efficient neural video compr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2223ms

[1560/1822] Applying foundational diffusion models to video compression ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2583ms

[1561/1822] How to use the Fisher information matrix for active learning...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2402ms

[1562/1822] Active learning strategies for sequential tasks that balance...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2768ms

[1563/1822] How can large language models be used to provide intelligent...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2325ms

[1564/1822] Improving IR-based bug localization through automated query ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2850ms

[1565/1822] how to improve multi-modal large language models for expert-...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3034ms

[1566/1822] integrating vision language models with physics simulators f...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2348ms

[1567/1822] How can large language models be used to automate the TinyML...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2328ms

[1568/1822] Framework for leveraging large language models to streamline...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 2863ms

[1569/1822] graph contrastive learning for short text classification wit...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2131ms

[1570/1822] how to use multi-view text embeddings from graph learning to...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2185ms

[1571/1822] How can artificial intelligence be used to identify social b...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2534ms

[1572/1822] Applications of AI for social inclusion including sign langu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2566ms

[1573/1822] evaluating the accuracy of large language models for transla...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.884 | Time: 2198ms

[1574/1822] benchmarking llama and gpt-4o for the automated translation ...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.456 | Time: 2479ms

[1575/1822] How to perform zero-shot cyber threat intelligence informati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 2195ms

[1576/1822] Scalable AI framework for extracting STIX compliant named en...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2424ms

[1577/1822] multi-parallel document-level translation corpus for African...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2688ms

[1578/1822] evaluating the performance of large language models versus N...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 1977ms

[1579/1822] automated prompt engineering framework using Knowledge-Gradi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2415ms

[1580/1822] How to apply sequential optimal learning and mixed-integer o...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2685ms

[1581/1822] How do different floating-point quantization parameters like...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2471ms

[1582/1822] What is the optimal bit-width and exponent-mantissa ratio fo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2084ms

[1583/1822] How can the concept of applied multiplexity be used to mitig...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2443ms

[1584/1822] Using multi-agent systems to improve cultural inclusivity an...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 2183ms

[1585/1822] how to improve graph retrieval augmented generation by recon...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 2457ms

[1586/1822] methods for mitigating information loss in knowledge graphs ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.912 | Time: 2614ms

[1587/1822] automated evaluation metrics for retrieval augmented generat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3737ms

[1588/1822] how to measure the conversational faithfulness and context r...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.855 | Time: 2593ms

[1589/1822] Large language model error patterns in mathematical word pro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2333ms

[1590/1822] Improving mathematical reasoning in LLMs through error-aware...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2569ms

[1591/1822] impact of real-world spelling mistakes and wikipedia edit hi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2432ms

[1592/1822] comparing the robustness of mt5 and bloom models against hum...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 2121ms

[1593/1822] automated network configuration translation between differen...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2235ms

[1594/1822] how can large language models be used to migrate legacy netw...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2347ms

[1595/1822] how to model fine-grained time-dependent user interests from...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2468ms

[1596/1822] using time-gap-aware attention and retrieval mechanisms to c...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2371ms

[1597/1822] How to use logit-based knowledge distillation to optimize de...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2484ms

[1598/1822] Research on distillation frameworks for deep spiking neural ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2236ms

[1599/1822] framework for building domain-specific AI agents using natur...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2343ms

[1600/1822] improving long-horizon planning in LLM agents by integrating...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.935 | Time: 2265ms

[1601/1822] Applying knowledge graph completion and relational graph att...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2810ms

[1602/1822] Knowledge graph based framework for modeling complex relatio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2647ms

[1603/1822] Automated prompt optimization techniques that use task-aware...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2897ms

[1604/1822] How to implement task-referenced adaptation and multi-metric...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2425ms

[1605/1822] benefits of introducing positive friction in conversational ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2569ms

[1606/1822] how slowing down AI interactions and adding deliberate frict...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2430ms

[1607/1822] how to use llm-based line-level filtering to improve the qua...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.873 | Time: 2738ms

[1608/1822] improving training data quality through fine-grained line-le...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2544ms

[1609/1822] How to improve time series reasoning in multi-modal language...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2558ms

[1610/1822] Multi-modal language models for complex time series reasonin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2619ms

[1611/1822] How can heterogeneous graph neural networks be applied to re...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2240ms

[1612/1822] Deep learning approaches for multimodal emotion prediction i...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2185ms

[1613/1822] How to improve Quranic question answering using cross-langua...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2838ms

[1614/1822] Cross-language strategies for addressing linguistic disparit...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3164ms

[1615/1822] How can sparse autoencoders be used to perform feature-level...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 2056ms

[1616/1822] Improving LLM response consistency for paraphrased inputs us...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2346ms

[1617/1822] How to use Partial Information Decomposition principles to q...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2173ms

[1618/1822] Decomposing causal power using the Möbius function of the re...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2404ms

[1619/1822] evaluating the reliability of membership inference attacks o...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2578ms

[1620/1822] do membership inference attacks on LLMs actually detect trai...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2345ms

[1621/1822] How can retrieval-augmented generation and evidence-based me...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2526ms

[1622/1822] Advanced frameworks for medical LLMs that utilize evidence s...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3269ms

[1623/1822] How can self-learning agents using curriculum learning princ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2111ms

[1624/1822] Using large language model agents and iterative exploration ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2570ms

[1625/1822] How do multimodal large language models perform when users p...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 2278ms

[1626/1822] Benchmarking the vulnerability of vision-language models to ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 2154ms

[1627/1822] How does implementing a retrieval-augmented generation frame...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.839 | Time: 2690ms

[1628/1822] Evaluating the performance of retriever and generative model...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 2290ms

[1629/1822] multilingual end-to-end speech recognition using mixture of ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 2504ms

[1630/1822] how to improve LID-based routers in MoE architectures for mu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2579ms

[1631/1822] How to prevent explicit content generation in text-to-image ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 2824ms

[1632/1822] Research on using embedding space distortion to defend again...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.973 | Time: 2752ms

[1633/1822] research on federated multimodal instruction tuning framewor...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2389ms

[1634/1822] how to use mixture of adapters and adaptive parameter aggreg...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2735ms

[1635/1822] deep learning approaches for idiom detection in Sorani Kurdi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2008ms

[1636/1822] evaluating the performance of transformer models for identif...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.908 | Time: 2347ms

[1637/1822] lightweight deep learning models for energy efficient weed d...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2610ms

[1638/1822] how to achieve high accuracy weed detection on low power har...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 2193ms

[1639/1822] How can large multimodal language models be used to automati...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 2428ms

[1640/1822] Large scale dataset and specialized multimodal vision encode...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3112ms

[1641/1822] how to implement large language models for educational manag...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 2168ms

[1642/1822] frameworks for applying fine-tuned large language models to ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2042ms

[1643/1822] How can metric learning with proxy anchor methods and tri-tr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2539ms

[1644/1822] Using BLIP-2 pre-trained encoders and cross-modal transforme...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2328ms

[1645/1822] using low rank adaptation to finetune language models for be...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 2314ms

[1646/1822] improving the efficiency of sparse autoencoder reconstructio...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2512ms

[1647/1822] how to design fair pricing mechanisms for LLM training data ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2317ms

[1648/1822] economic frameworks for large language model data markets fo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2809ms

[1649/1822] Benchmarking AI agents on scientific discovery tasks through...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 2416ms

[1650/1822] How can autonomous agents be evaluated on their ability to d...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.853 | Time: 2173ms

[1651/1822] How to use large language models to automatically synthesize...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2200ms

[1652/1822] Integrating LLM-generated heuristics into search algorithms ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2913ms

[1653/1822] How does the length of tokenized Java code affect the accura...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 2697ms

[1654/1822] Evaluating the impact of input context window size on the pe...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2129ms

[1655/1822] how to use large language models and retrieval augmented gen...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2324ms

[1656/1822] automated g-code generation for cnc machines using self-corr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2413ms

[1657/1822] How can Large Language Models be used for semantic consisten...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 2168ms

[1658/1822] Semi-supervised sentiment classification using entity extrac...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2756ms

[1659/1822] Combining width and depth pruning strategies for efficient s...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.571 | Time: 2578ms

[1660/1822] How to perform two-stage structured pruning on LLMs by remov...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.934 | Time: 2627ms

[1661/1822] how to improve large language model logical reasoning using ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2476ms

[1662/1822] enhancing llm problem solving through town hall style debate...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2394ms

[1663/1822] How can participatory design methods be used to create large...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2393ms

[1664/1822] Challenges and opportunities of developing a journalist-cont...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2515ms

[1665/1822] How can large language models be used as reference-aware cri...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2357ms

[1666/1822] Benchmarking execution-free evaluation methods for code agen...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2551ms

[1667/1822] Performance comparison of YOLOv7 and Faster R-CNN for vision...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2612ms

[1668/1822] How can deep learning models like YOLOv7 be applied to autom...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2007ms

[1669/1822] How can few-shot optimization and iterative prompt engineeri...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2351ms

[1670/1822] Fine-tuning Mistral-7B-Instruct-v0.3 for robust hallucinatio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3033ms

[1671/1822] using in-context learning with transformer models to detect ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2435ms

[1672/1822] how can large language model architectures and in-context le...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 2625ms

[1673/1822] using large language models and retrieval augmented generati...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 2547ms

[1674/1822] evaluating how interactive browser extensions and chat inter...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2376ms

[1675/1822] methods for 2-bit KV cache quantization in vision-language m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2362ms

[1676/1822] how to optimize vision-language model memory consumption usi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 2202ms

[1677/1822] Evaluating random forest classifiers for the detection of Na...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2654ms

[1678/1822] How can researchers develop accurate language identification...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.928 | Time: 2531ms

[1679/1822] How can analytical decomposition of first-layer attention we...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.963 | Time: 2220ms

[1680/1822] Weight-based methods for analyzing how transformer models ma...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2592ms

[1681/1822] Adaptive reward function exploration for action-level backdo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2206ms

[1682/1822] How to perform stealthy backdoor attacks in continuous reinf...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2542ms

[1683/1822] How can fine-tuned large language models like LLaMA improve ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2833ms

[1684/1822] Comparative evaluation of neural machine translation and fin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2559ms

[1685/1822] Limitations and biases of automated factuality metrics in ev...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2394ms

[1686/1822] How reliable are automated factuality evaluators at measurin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2335ms

[1687/1822] how to optimize kv cache storage in large language models us...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2019ms

[1688/1822] adaptive kv cache pruning techniques that distinguish betwee...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2198ms

[1689/1822] methods for converting dense large language models into mixt...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2662ms

[1690/1822] how to apply differentiable dynamic pruning to transform MLP...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2867ms

[1691/1822] How can diversity-based adaptive random testing using string...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2197ms

[1692/1822] Effective test selection and prioritization strategies for L...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2393ms

[1693/1822] How to improve retrieval-augmented medical question answerin...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 2861ms

[1694/1822] Methods for improving the accuracy of medical QA systems usi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2556ms

[1695/1822] A unified framework for general mobility trajectory modeling...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2746ms

[1696/1822] How can masked conditional diffusion models with contextual ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2507ms

[1697/1822] How can large language models be used for few-shot harmful c...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.773 | Time: 2473ms

[1698/1822] Evaluating the effectiveness of multimodal in-context learni...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2686ms

[1699/1822] How to improve perceptual consistency and visual sharpness i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2321ms

[1700/1822] Perception-inspired loss functions for neural edge detection...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2375ms

[1701/1822] empirical study evaluating the performance of deep learning ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2801ms

[1702/1822] challenges and performance degradation of deep learning appr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2868ms

[1703/1822] How can large language models be used to develop autonomous ...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.557 | Time: 2281ms

[1704/1822] Large language model based proactive dialogue systems for pr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2588ms

[1705/1822] relationship between Ehrenfeucht-Haussler rank of Boolean fu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2353ms

[1706/1822] how many Chain of Thought steps are required for a single-la...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2303ms

[1707/1822] A comprehensive analysis of statistical methodology errors i...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2183ms

[1708/1822] Investigating the prevalence of inadequate data analysis tec...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2193ms

[1709/1822] How can selective boosting of attention weights for local an...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2323ms

[1710/1822] Mitigating hallucinations in LVLMs by intervening in self-at...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2872ms

[1711/1822] new optimization algorithms for large language model pre-tra...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3397ms

[1712/1822] improving Signum optimizer stability for GPT-2 training by i...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 3167ms

[1713/1822] How to implement adaptive PII mitigation and policy-driven m...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2724ms

[1714/1822] Advanced NLP techniques for context-aware analysis and anony...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2240ms

[1715/1822] How can node influence maximization and decoupled influence ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2397ms

[1716/1822] A scalable graph unlearning framework that uses influence fu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3000ms

[1717/1822] understanding the sequential learning of skills in neural ne...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 2310ms

[1718/1822] research on the domino effect in deep learning training and ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.943 | Time: 2395ms

[1719/1822] How can large vision-language models be used to identify inc...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2536ms

[1720/1822] Using large vision-language models and reference images to v...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2494ms

[1721/1822] How to use retrieval-augmented generation and large language...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2231ms

[1722/1822] Open source RAG-based systems for automatic extraction of pr...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2842ms

[1723/1822] Benchmark datasets for Norwegian question answering evaluati...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2849ms

[1724/1822] Performance evaluation of language models on new Norwegian q...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2195ms

[1725/1822] How well does GPT-4o understand the ironic use of emojis com...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2361ms

[1726/1822] Comparative study of large language models and human percept...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2318ms

[1727/1822] improving direct preference optimization for large language ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 2063ms

[1728/1822] alternative DPO methods that down-weight misranked samples a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3356ms

[1729/1822] How does pyramid-descent visual position encoding enhance mu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2890ms

[1730/1822] Improving vision-language model performance by using periphe...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 2776ms

[1731/1822] Empirical study analyzing gender and ethnicity bias in Stabl...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2422ms

[1732/1822] How do text-to-image generative models like Stable Diffusion...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2684ms

[1733/1822] How can 4-bit quantization reduce memory storage and speed u...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2361ms

[1734/1822] Using low-precision 4-bit integer quantization to optimize t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2844ms

[1735/1822] Effective multi-stage training strategies for bilingual neur...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2421ms

[1736/1822] Developing a lightweight bilingual Islamic LLM for document ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 2394ms

[1737/1822] What are the results of a thematic analysis regarding the ad...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 2288ms

[1738/1822] Qualitative research on the ethical integration of generativ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 2431ms

[1739/1822] efficient structured pruning techniques for large language m...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.845 | Time: 2791ms

[1740/1822] how to prune large language models quickly using structured ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 2160ms

[1741/1822] How can generative AI and quantum computing be integrated in...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2436ms

[1742/1822] Prototyping platform and dataset for 6G semantic communicati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2998ms

[1743/1822] how to improve document-level machine translation consistenc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2499ms

[1744/1822] using doc-guided memory and agent-based approaches to ensure...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 2173ms

[1745/1822] How does instruction tuning affect the fundamental task capa...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.784 | Time: 2440ms

[1746/1822] Correlation between instruction-tuned performance and base m...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.900 | Time: 2518ms

[1747/1822] multi-modal large language models for single-cell rna sequen...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2224ms

[1748/1822] ai models for single-cell analysis capable of cell type anno...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2235ms

[1749/1822] natural language interface for optimization models using lar...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2371ms

[1750/1822] how can large language models be used to provide counterfact...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.839 | Time: 2685ms

[1751/1822] How to use Large Language Models as an action selection filt...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 2345ms

[1752/1822] A hybrid approach combining large language model decision ma...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2575ms

[1753/1822] Integrating large language models into hierarchical planning...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.905 | Time: 2323ms

[1754/1822] Standardized benchmarks and datasets for evaluating the perf...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2534ms

[1755/1822] How to improve the reasoning capabilities of mixture of expe...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 2338ms

[1756/1822] Enhancing cognitive depth in language models by facilitating...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3423ms

[1757/1822] Norwegian abstractive summarization dataset for benchmarking...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3207ms

[1758/1822] How to evaluate the performance of Norwegian large language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3244ms

[1759/1822] How to apply denoise diffusion models and stochastic differe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3274ms

[1760/1822] Diffusion transformer based signal detection methods for red...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3155ms

[1761/1822] efficient elastic quantization framework for deploying large...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3635ms

[1762/1822] how to improve memory elasticity and transition granularity ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3513ms

[1763/1822] How does the magnitude of enriched categories of texts relat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3793ms

[1764/1822] Mathematical framework for computing magnitude homology and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3606ms

[1765/1822] memory efficient on-FPGA training of transformer models usin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3594ms

[1766/1822] hardware accelerator design for end-to-end transformer train...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3311ms

[1767/1822] How can large language models be used to generate synthetic ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3510ms

[1768/1822] Effective methods for reducing gender bias in pre-trained mo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3519ms

[1769/1822] Orchestration framework for large language model training us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3284ms

[1770/1822] Implementing joint mining mechanisms and bilateral value sha...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3488ms

[1771/1822] Attention-based deep learning framework for interpretable en...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3485ms

[1772/1822] How can Transformer architectures be used to predict multipl...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3351ms

[1773/1822] How to combine BERT sentence embeddings and TF-IDF features ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3410ms

[1774/1822] Effective methods for Marathi plagiarism detection using a w...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3320ms

[1775/1822] L2 convergence rates and stability of linear Q-learning with...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3679ms

[1776/1822] stochastic approximation analysis of linear Q-learning diver...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3910ms

[1777/1822] How can large language models be used for reference-free and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3252ms

[1778/1822] Automated metrics for counterspeech generation evaluation us...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3549ms

[1779/1822] How to use large language models for active knowledge retrie...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3222ms

[1780/1822] LLM-enhanced knowledge augmentation methods that integrate e...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3537ms

[1781/1822] How do cognitive forcing functions and different explanation...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3358ms

[1782/1822] Comparing the impact of visual explanations and cognitive fo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3604ms

[1783/1822] neurosymbolic knowledge base for environmental social and go...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3703ms

[1784/1822] how to extract actionable sustainability information from co...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3397ms

[1785/1822] How can large language models be used as proxies to reduce t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3271ms

[1786/1822] Using LLM-based pipelines and DNF proper learning to acceler...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2675ms

[1787/1822] improving best-of-n sampling in large language models using ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 2092ms

[1788/1822] how to use pairwise comparison and chain of thought reasonin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.822 | Time: 2156ms

[1789/1822] How can masking high perplexity tokens in training data help...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3001ms

[1790/1822] The effect of LLM-generated synthetic data on reducing perfo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2562ms

[1791/1822] How to identify both latent driving forces and direct causal...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2064ms

[1792/1822] Causal discovery and representation learning framework for u...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.970 | Time: 2355ms

[1793/1822] How can debate and scalable oversight be used to improve wea...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 2439ms

[1794/1822] Can a weak model extract trustworthy information from a stro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2136ms

[1795/1822] How can zero-knowledge proofs be used to verify the effectiv...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2785ms

[1796/1822] Privacy-preserving verification techniques for low-rank adap...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 2359ms

[1797/1822] How do binary decision biases and sampling methods in large ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.864 | Time: 2402ms

[1798/1822] Investigating randomness and decision-making biases in GPT m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2347ms

[1799/1822] How to measure conditional feature importance using adversar...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3157ms

[1800/1822] Explainable AI methods for estimating on-manifold conditiona...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2152ms

[1801/1822] benchmarking vision language model safety using a dataset of...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.577 | Time: 2715ms

[1802/1822] multilingual evaluation of multimodal safety in vision langu...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.871 | Time: 2453ms

[1803/1822] How to optimize large scale machine learning training pipeli...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2403ms

[1804/1822] Techniques for improving embedding table lookups and handlin...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2142ms

[1805/1822] How does language-adaptive fine-tuning with the AfriBERTa mo...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2284ms

[1806/1822] Evaluating the effectiveness of language-adaptive fine-tunin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2324ms

[1807/1822] Methods for adapting decoder-only large language models to p...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2897ms

[1808/1822] How can combining bidirectional and causal attention in a ge...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2557ms

[1809/1822] Evaluating how large language models like Llama and Claude m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2296ms

[1810/1822] Comparison of emotional intensity and semantic coherence bet...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2527ms

[1811/1822] How do large language models exhibit systematic provider bia...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2137ms

[1812/1822] Empirical study and dataset for evaluating service provider ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2604ms

[1813/1822] benchmarking vision language models for error detection and ...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.812 | Time: 2346ms

[1814/1822] how well do current vision language models perform at evalua...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.776 | Time: 2574ms

[1815/1822] How do large language models perform on recalling notable gl...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2428ms

[1816/1822] Evaluating geographic disparities and socioeconomic correlat...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 2267ms

[1817/1822] Automated pipeline for converting general Word documents int...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2161ms

[1818/1822] How to evaluate the quality of AI-generated presentations us...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2633ms

[1819/1822] trends in the use of causal inference methods and their impa...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1810ms

[1820/1822] how does causal narrative complexity and novelty in economic...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 1951ms

[1821/1822] optimizing Mixture-of-Experts model inference on serverless ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2572ms

[1822/1822] distributed deployment of MoE models in serverless computing...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2175ms

SPARSE Summary:
  Avg MRR:      0.772
  Avg NDCG@5:   0.721
  Avg NDCG@10:  0.762
  Avg P@5:      0.548
  Avg P@10:     0.436
  Avg Time:     2599ms

============================================================
Evaluating mode: HYBRID
============================================================

[1/1822] enhancing large language model reasoning capabilities throug...
2026-02-19 16:43:34 | INFO     | arxiv-rag.bge_embedder | Loading BGE-M3 model on cuda...
Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 24193.26it/s]
2026-02-19 16:43:40 | INFO     | arxiv-rag.bge_embedder | BGE-M3 loaded in 5.5s
You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 14996ms

[2/1822] using reinforcement learning to develop emergent self reflec...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.442 | Time: 3867ms

[3/1822] How to implement test-time scaling in large language models ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3228ms

[4/1822] Improving language model reasoning performance on competitio...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3670ms

[5/1822] Scaling reinforcement learning for large language models usi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5674ms

[6/1822] How to use long chain of thought training techniques to impr...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 3539ms

[7/1822] open-source world foundation models for physical AI developm...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3379ms

[8/1822] how to use general-purpose world models for training physica...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2502ms

[9/1822] How can agentic search workflows and reason-in-documents mod...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 4930ms

[10/1822] Improving the trustworthiness of large reasoning models usin...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.459 | Time: 3547ms

[11/1822] End-to-end native GUI agents that use vision-only screenshot...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 5379ms

[12/1822] How does system-2 reasoning and iterative training with refl...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3019ms

[13/1822] challenging multi-modal benchmark for evaluating large langu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3966ms

[14/1822] dataset for testing state of the art models on advanced acad...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4034ms

[15/1822] survey of collaboration mechanisms and coordination protocol...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.959 | Time: 2762ms

[16/1822] what are the key dimensions and strategies for organizing la...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5055ms

[17/1822] how to effectively use llm-as-a-judge and consensus filterin...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 4219ms

[18/1822] identifying and mitigating biases in best-of-n evaluation st...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3702ms

[19/1822] improving small language model math reasoning using monte ca...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3847ms

[20/1822] how to scale mathematical reasoning in small models through ...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.482 | Time: 4271ms

[21/1822] autonomous LLM agent framework for end-to-end scientific dis...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.735 | Time: 2931ms

[22/1822] how to use large language model agents as research assistant...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 3968ms

[23/1822] how to reduce the inference overhead and reasoning length of...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2885ms

[24/1822] RL-style fine-tuning for pruning reasoning steps and optimiz...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4190ms

[25/1822] comprehensive survey on reinforcement learning methods for t...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3348ms

[26/1822] the impact of test-time scaling and reinforced reasoning on ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 3301ms

[27/1822] survey of autonomous AI agents in retrieval augmented genera...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.973 | Time: 4532ms

[28/1822] how do agentic design patterns like reflection and planning ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2368ms

[29/1822] How to address the optimization trade-off between reconstruc...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5739ms

[30/1822] Training efficient Diffusion Transformers using VA-VAE and v...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5711ms

[31/1822] How to address underthinking and frequent thought switching ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 4454ms

[32/1822] Improving the performance of reasoning models by using a dec...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3562ms

[33/1822] How can multimodal large language models use visual reasonin...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.451 | Time: 2642ms

[34/1822] Improving spatial reasoning in multimodal models by generati...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2774ms

[35/1822] How to apply direct preference optimization to rectified flo...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.890 | Time: 3375ms

[36/1822] Multi-dimensional video reward models for aligning flow-base...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2633ms

[37/1822] benchmark for evaluating large multimodal models on expert k...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.942 | Time: 4348ms

[38/1822] measuring knowledge gain in multimodal learning through perc...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 2535ms

[39/1822] How does the Qwen2.5-1M model achieve a one million token co...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 3837ms

[40/1822] Inference optimization methods for one million token context...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3932ms

[41/1822] benchmarking different large language model steering methods...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2947ms

[42/1822] performance of sparse autoencoders versus rank-1 representat...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 3763ms

[43/1822] inference-time steering of diffusion models using Feynman-Ka...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.641 | Time: 2518ms

[44/1822] how to control diffusion model generation with reward functi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.881 | Time: 3840ms

[45/1822] How can large multimodal models maintain performance while c...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.671 | Time: 5240ms

[46/1822] Efficient large multimodal model architecture that uses moda...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.933 | Time: 4608ms

[47/1822] Applying chain of thought reasoning and step by step verific...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 3921ms

[48/1822] Using the potential assessment reward model and direct prefe...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 3888ms

[49/1822] How does the lightning attention mechanism combined with mix...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3642ms

[50/1822] Scaling large vision-language and text models using efficien...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3079ms

[51/1822] benchmarking expert-level medical reasoning and multimodal u...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.474 | Time: 4812ms

[52/1822] high difficulty medical question answering dataset with clin...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 2876ms

[53/1822] How to protect large language models from universal jailbrea...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.964 | Time: 2775ms

[54/1822] Evaluating the effectiveness of constitutional classifiers a...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2879ms

[55/1822] evaluating large language models on their ability to navigat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3379ms

[56/1822] a multi-agent framework using an explore-critic paradigm to ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2842ms

[57/1822] How to use 3D tracking videos as control signals in diffusio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 5684ms

[58/1822] A unified video diffusion framework that leverages 3D contro...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2506ms

[59/1822] How to implement System 2 reasoning in large language models...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 3371ms

[60/1822] Training language models to perform meta-reasoning about the...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3896ms

[61/1822] hierarchical multi-agent framework for mobile task automatio...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3570ms

[62/1822] how can mobile agents learn from past experiences using tips...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2550ms

[63/1822] benchmark for evaluating multi-turn conversation capabilitie...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 2939ms

[64/1822] realistic multi-turn evaluation for large language models us...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2624ms

[65/1822] transformer-based diffusion models for closed-loop autonomou...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3520ms

[66/1822] achieving human-like driving behaviors and joint prediction-...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3767ms

[67/1822] benchmarking large language model performance on competitive...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3895ms

[68/1822] how to measure the reasoning and coding abilities of LLMs th...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3117ms

[69/1822] How does graph-based retrieval-augmented generation solve th...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 2883ms

[70/1822] Systematic review of GraphRAG technical foundations includin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3771ms

[71/1822] benchmarks for evaluating expert level reasoning and domain ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.611 | Time: 2776ms

[72/1822] multimodal foundation model evaluation datasets with human e...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.935 | Time: 4778ms

[73/1822] how to use process reward models with speculative decoding t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 3947ms

[74/1822] efficient large language model inference using a draft model...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3496ms

[75/1822] benchmarking vision language models on physical world unders...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3867ms

[76/1822] how to improve physical reasoning in vision language models ...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 4764ms

[77/1822] Technical details of omni-modal models using multi-stage tra...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.767 | Time: 4593ms

[78/1822] How does the Baichuan-Audio-Tokenizer capture semantic and a...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 3904ms

[79/1822] How to implement a temporally-aware knowledge graph system f...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.854 | Time: 2688ms

[80/1822] Benchmarking Zep against MemGPT using the Deep Memory Retrie...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3756ms

[81/1822] human annotated datasets for large language model safety ali...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 4695ms

[82/1822] how to train lightweight LLM safety guardrails using a hybri...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 3737ms

[83/1822] Multimodal large language models for real-time full-duplex v...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.897 | Time: 3512ms

[84/1822] How can multimodal LLMs achieve seamless two-way voice commu...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.689 | Time: 4088ms

[85/1822] How to synthesize agent interaction data using backward cons...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2745ms

[86/1822] Data-centric framework for adapting autonomous agents to dig...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4046ms

[87/1822] how can reinforcement learning and oversampling for increase...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4147ms

[88/1822] strategies for enabling inference scaling in large language ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3375ms

[89/1822] How can multiagent systems and independent model specializat...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2809ms

[90/1822] Finetuning language models on synthetic data from multiagent...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3401ms

[91/1822] preference optimization algorithm for Thinking-LLM-as-a-Judg...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 3576ms

[92/1822] how does separating evaluation planning from execution in ch...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.525 | Time: 4034ms

[93/1822] How can reasoning-based supervised fine-tuning and hard samp...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2657ms

[94/1822] A large-scale dataset with detailed reasoning steps for trai...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.615 | Time: 5403ms

[95/1822] Can large language models articulate their own learned behav...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3213ms

[96/1822] Research on behavioral self-awareness in LLMs and whether mo...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 4153ms

[97/1822] How are large language models being used across the differen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3701ms

[98/1822] A comprehensive review of the roles and methodologies of lar...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4188ms

[99/1822] How to use open-source large language models for efficient G...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 3271ms

[100/1822] State of the art open-source LLM framework for SWE-bench Lit...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.830 | Time: 3908ms

[101/1822] reproducing o1 style slow-thinking in multimodal models by f...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 4051ms

[102/1822] research on transferring long-form reasoning capabilities fr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2648ms

[103/1822] How does pre-training data influence the emergence of Chain ...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2965ms

[104/1822] Survey of large language model architectures and scaling mec...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 4999ms

[105/1822] How does increasing inference-time compute in reasoning mode...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3899ms

[106/1822] Investigating the impact of test-time compute scaling on the...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2211ms

[107/1822] How to improve multimodal GUI agents using a two-stage fine-...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3458ms

[108/1822] Developing generalist GUI agents with hierarchical reasoning...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3764ms

[109/1822] open source multi-modal reward models for aligning large vis...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.871 | Time: 3357ms

[110/1822] using reward models for reinforcement learning and test-time...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.396 | Time: 3454ms

[111/1822] Recent advancements in large vision-language models for deta...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.793 | Time: 4765ms

[112/1822] Large multi-modal models that outperform GPT-4o and Gemini 1...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 4492ms

[113/1822] How can Monte Carlo Tree Search be integrated with large lan...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2524ms

[114/1822] Improving the exploration of LLM-based heuristic generation ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2207ms

[115/1822] statistical methods for justifying the replacement of human ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2734ms

[116/1822] how to evaluate if an LLM is a reliable substitute for human...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3186ms

[117/1822] How can large language model agents be designed to support l...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 3517ms

[118/1822] A comprehensive survey of perception, memory, and action mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2904ms

[119/1822] Multimodal foundation models for computational pathology pre...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4698ms

[120/1822] How does incorporating transcriptomic data into the pre-trai...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2451ms

[121/1822] training-free test-time alignment of diffusion models using ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3451ms

[122/1822] how to optimize diffusion models for multiple reward objecti...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 2673ms

[123/1822] Inference-time alignment techniques for diffusion models usi...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.517 | Time: 2616ms

[124/1822] Guide to reward-guided generation and inference-time guidanc...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2162ms

[125/1822] How to use multimodal large language models to convert chart...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 4828ms

[126/1822] Multimodal LLM trained on Chart2Code-160k dataset for genera...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3977ms

[127/1822] how to use dynamic trend representation transformers and cro...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 2442ms

[128/1822] fusing dynamic trends and static graph attributes for traffi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2121ms

[129/1822] How can learnable orthogonal and scaling transformations imp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3591ms

[130/1822] Post-training quantization methods using Quantization Space ...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.559 | Time: 2868ms

[131/1822] Scalable parallel transformer architecture for video diffusi...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3468ms

[132/1822] Memory-efficient training frameworks using hybrid parallelis...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2712ms

[133/1822] How can evolutionary search strategies be used to scale infe...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.905 | Time: 3597ms

[134/1822] Using language models to generate and recombine candidate re...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3274ms

[135/1822] How to improve low-level spatial understanding in vision-lan...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 4784ms

[136/1822] Recent advancements in training VLA models that integrate se...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2560ms

[137/1822] How can self-play between a conjecturer and a prover LLM imp...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4243ms

[138/1822] Training large language models for formal mathematical verif...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4120ms

[139/1822] self-supervised music representation learning models that ut...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 3584ms

[140/1822] how to train a joint music-text embedding model using contra...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3601ms

[141/1822] How to improve large language model reasoning through test-t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.964 | Time: 3765ms

[142/1822] Efficient test-time computation methods for LLMs that levera...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3111ms

[143/1822] How does scaling long chain of thought data to one million s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3135ms

[144/1822] Investigating the impact of long-CoT dataset scaling and rei...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4387ms

[145/1822] How can chain-of-thought reasoning and spatial coordinate al...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3997ms

[146/1822] Advanced spatial reasoning in vision-language models using b...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 4885ms

[147/1822] How to improve language model reasoning by training models t...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.482 | Time: 4024ms

[148/1822] Comparison between critique fine-tuning and imitation learni...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.501 | Time: 4154ms

[149/1822] What are the primary challenges and open problems in using m...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3447ms

[150/1822] Investigating the limitations of machine unlearning for cybe...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2549ms

[151/1822] How can multimodal large language models use hidden latent s...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.624 | Time: 3652ms

[152/1822] Compressing textual reasoning chains into compact thinking t...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.640 | Time: 3908ms

[153/1822] best practices and data-centric strategies for post-training...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3479ms

[154/1822] detailed implementation of post-training data strategies and...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2623ms

[155/1822] benchmarks for evaluating temporal awareness and real-time r...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.572 | Time: 3780ms

[156/1822] how to evaluate the ability of video LLMs to handle incremen...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2763ms

[157/1822] How can large language model agents be trained with reinforc...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 4765ms

[158/1822] LLM-based autonomous agents for comprehensive literature ret...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.840 | Time: 3160ms

[159/1822] Are chain of thought explanations in reasoning models like D...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3497ms

[160/1822] Measuring if large language models can accurately describe h...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3566ms

[161/1822] benchmarking negation understanding in vision language model...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.977 | Time: 4890ms

[162/1822] improving CLIP model performance on negated text queries thr...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3790ms

[163/1822] multimodal dataset for building damage assessment combining ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.786 | Time: 4267ms

[164/1822] globally distributed dataset for training AI models in build...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3340ms

[165/1822] comprehensive survey of parameter-efficient fine-tuning tech...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5026ms

[166/1822] recent developments and systematic review of PEFT methods ac...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4620ms

[167/1822] evaluation framework for assessing the functional correctnes...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 4812ms

[168/1822] multilingual benchmarks for measuring whether AI-generated c...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 4381ms

[169/1822] How do large language models improve cold-start recommendati...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2749ms

[170/1822] Survey of recent advances and future research directions in ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2892ms

[171/1822] How to use Monte Carlo Tree Search and iterative self-traini...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2644ms

[172/1822] Training language model agents to perform self-reflection an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3838ms

[173/1822] How to maintain safety alignment and prevent jailbreaking ri...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.613 | Time: 4166ms

[174/1822] Parameter efficient fine-tuning methods that preserve safety...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3303ms

[175/1822] Investigating the stability of features extracted by sparse ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2670ms

[176/1822] Do TopK sparse autoencoders identify consistent features acr...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2499ms

[177/1822] Scaling vision language model pretraining with massive high ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2526ms

[178/1822] How does progressively scaling SFT data quantity and complex...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 2908ms

[179/1822] comprehensive survey of large language models used for autom...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 3978ms

[180/1822] recent advancements and challenges in using large language m...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4709ms

[181/1822] Interpretable machine unlearning in diffusion models using s...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2303ms

[182/1822] Applying sparse autoencoders to diffusion model activations ...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.763 | Time: 2766ms

[183/1822] interactive benchmark for evaluating multimodal large langua...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2780ms

[184/1822] How do state of the art multimodal large language models per...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4987ms

[185/1822] comprehensive architectural framework and modular blueprint ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2635ms

[186/1822] how to implement reasoning language models using process-bas...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3967ms

[187/1822] training-free methods for consistent character identity in t...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2339ms

[188/1822] how to achieve consistent identity in diffusion models using...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3014ms

[189/1822] Recent survey on the performance and training techniques of ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3387ms

[190/1822] Comparing the efficiency and scalability of task-specific sm...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 3522ms

[191/1822] How to align large language model outputs with human prefere...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2962ms

[192/1822] Techniques for test-time preference optimization that use na...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3039ms

[193/1822] large scale multimodal benchmark for evaluating cultural bia...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2461ms

[194/1822] fine-tuning vision language models on the CultureVerse datas...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.611 | Time: 2450ms

[195/1822] speculative decoding techniques that use a judge model to ac...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 2799ms

[196/1822] how to increase the inference speed of large language models...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2922ms

[197/1822] How can tensor product decomposition be used to reduce KV ca...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.639 | Time: 2683ms

[198/1822] Efficient attention mechanisms using contextual low-rank rep...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 4650ms

[199/1822] Comparing the impact of conversational XAI interfaces versus...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4783ms

[200/1822] How does integrating large language model agents into conver...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2926ms

[201/1822] How are large language models being applied to automate and ...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 4332ms

[202/1822] A review of current research on using large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3934ms

[203/1822] rehearsal-free class incremental learning using decoupled lo...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2585ms

[204/1822] how to achieve a stable stability-plasticity trade-off in co...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2451ms

[205/1822] How to train large language models using FP4 quantization wh...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3813ms

[206/1822] Techniques for ultra-low precision training of LLMs using di...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.789 | Time: 4740ms

[207/1822] How can instruction tuning with environment-based self-refin...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 2743ms

[208/1822] Training LLM agents to correct their own mistakes using envi...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 2744ms

[209/1822] How does inference-time scaling and extended reasoning chain...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 4394ms

[210/1822] Applying journey learning and systematic clinical reasoning ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4342ms

[211/1822] comprehensive benchmark for evaluating large language model ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2822ms

[212/1822] how to assess tool-augmented large language models using cat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3021ms

[213/1822] how can we align time series data with linguistic logic and ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 3310ms

[214/1822] using dual scale context alignment graph neural networks to ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3151ms

[215/1822] benchmarking large language models for factual grounding and...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3368ms

[216/1822] automated evaluation methods and leaderboards for testing wh...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 4134ms

[217/1822] how to implement retrieval augmented generation over a large...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2758ms

[218/1822] framework for dynamic video retrieval and informative frame ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2719ms

[219/1822] benchmarking multi-turn retrieval augmented generation syste...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.615 | Time: 4610ms

[220/1822] how do state-of-the-art RAG systems handle multi-turn conver...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.703 | Time: 2948ms

[221/1822] How can process reward models be used to improve multimodal ...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 3867ms

[222/1822] Training multimodal large language models for mathematical r...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 3959ms

[223/1822] How do large language models use sparse autoencoders to repr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 4874ms

[224/1822] Investigation into whether multilingual LLMs encode universa...
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.658 | Time: 2946ms

[225/1822] methods for scaling serial and parallel test-time compute to...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 3877ms

[226/1822] using model-generated test voting and multi-turn selection t...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 4288ms

[227/1822] Is the performance drop in continual learning of large langu...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2258ms

[228/1822] How does freezing bottom layers of large language models hel...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 4601ms

[229/1822] comparing the safety and alignment levels of DeepSeek-R1 and...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3547ms

[230/1822] systematic evaluation of DeepSeek-R1 and o3-mini reasoning m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 3896ms

[231/1822] techniques for maintaining response diversity in large langu...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.839 | Time: 4727ms

[232/1822] how to optimize language models to generate more diverse per...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 4742ms

[233/1822] how to bridge the multilingual performance gap in mathematic...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4509ms

[234/1822] improving Korean language math reasoning capabilities in LLM...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.750 | Time: 4170ms

[235/1822] How does scaling the input vocabulary size using multi-gram ...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 4347ms

[236/1822] Investigating the impact of decoupling input and output voca...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 4146ms

[237/1822] How can hierarchical memory systems be used to enable traini...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 3541ms

[238/1822] Frameworks for real-time video reasoning that use parallel s...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2617ms

[239/1822] how does the inconsistency between comprehension and safety ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 5035ms

[240/1822] using shuffle inconsistency and query-based black-box optimi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 3963ms

[241/1822] How does the accuracy of frequent ChatGPT users compare to a...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3901ms

[242/1822] Can experienced LLM users identify AI-written articles that ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2674ms

[243/1822] Theoretical analysis of gradient descent dynamics and loss t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2683ms

[244/1822] How does the parametrization of key and query weights affect...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 2909ms

[245/1822] How does the level of sparsity in mixture-of-experts models ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.936 | Time: 2473ms

[246/1822] Investigating the optimal balance between computational effi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.942 | Time: 2805ms

[247/1822] Evaluating large language models on multi-step and constrain...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 3946ms

[248/1822] How to measure the performance of LLMs in complex tool use s...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.442 | Time: 3872ms

[249/1822] How can large language model-based generative agents be used...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2900ms

[250/1822] Using LLM-powered learner simulators with memory and reflect...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.706 | Time: 4213ms

[251/1822] How does global batch load balancing loss improve expert spe...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3226ms

[252/1822] Training Mixture-of-Experts models with global frequency syn...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.599 | Time: 2775ms

[253/1822] Best practices and design considerations for conducting exte...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.937 | Time: 3619ms

[254/1822] How can AI developers implement external red teaming framewo...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2530ms

[255/1822] Unified generative model for speech and singing voice enhanc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 5620ms

[256/1822] How can masked generative models be used for zero-shot voice...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2539ms

[257/1822] Scalable deep graph neural networks for crystal property pre...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 2945ms

[258/1822] Advanced graph neural network models for molecules and mater...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2693ms

[259/1822] How to map hidden knowledge of neural networks into the mult...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2527ms

[260/1822] Scalable method for validating AI models and identifying neu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3085ms

[261/1822] Methods for training retrieval augmented generation models t...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 4621ms

[262/1822] Improving RAG performance through test-time compute scaling ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.884 | Time: 3743ms

[263/1822] comprehensive review of the methods and algorithms used to e...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.948 | Time: 2615ms

[264/1822] how are multi-turn interactions in large language models eva...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 3287ms

[265/1822] How to use tree search algorithms like MCTS to mitigate hall...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2706ms

[266/1822] Applying dual process theory and slow thinking generation wi...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 4170ms

[267/1822] collaborative framework for large language models and small ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 3627ms

[268/1822] how to integrate cloud-based LLMs with on-device small recom...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.906 | Time: 4336ms

[269/1822] Best practices for optimizing Retrieval-Augmented Generation...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 4590ms

[270/1822] How do factors like document chunk size, retrieval stride, a...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.621 | Time: 2462ms

[271/1822] How can self-updating libraries and task decomposition impro...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.608 | Time: 4226ms

[272/1822] Frameworks for large language models that use dynamic memory...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 2821ms

[273/1822] benchmarking text to image diffusion models for toxicity fai...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2972ms

[274/1822] comprehensive safety assessment framework for evaluating bia...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2465ms

[275/1822] Do different large language models produce similar creative ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4063ms

[276/1822] Study comparing the population-level diversity and homogenei...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3232ms

[277/1822] how effective are LLM-based software repair agents at fixing...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3934ms

[278/1822] comparison of agentic program repair performance between ope...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3006ms

[279/1822] benchmark for evaluating large language model hallucinations...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3065ms

[280/1822] new taxonomy for large language model hallucinations disting...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2759ms

[281/1822] how to improve automated feature interpretability in large l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4425ms

[282/1822] natural language descriptions for llm features that capture ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4172ms

[283/1822] How to improve multi-step reasoning in RAG systems using pro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3457ms

[284/1822] Addressing early-step bias in process reward models through ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3853ms

[285/1822] How do advanced second-order optimizers like Self-Scaled BFG...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3719ms

[286/1822] Performance of self-scaled quasi-Newton methods like SSBFGS ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 2992ms

[287/1822] How to use adaptive projective gradient descent and shared s...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3915ms

[288/1822] Constrained optimization methods for multi-task model mergin...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.591 | Time: 3707ms

[289/1822] How does displaying AI confidence levels influence human sel...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4984ms

[290/1822] Research on the alignment between artificial intelligence co...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 3746ms

[291/1822] Performance comparison between large language models and tra...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 5138ms

[292/1822] Analyzing the trade-off between F1-score and inference time ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3800ms

[293/1822] comprehensive review of techniques for distinguishing betwee...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 4045ms

[294/1822] comparison of probabilistic methods and ensemble learning fo...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4862ms

[295/1822] discrete diffusion models for multi-task drug discovery usin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3666ms

[296/1822] how to use non-autoregressive bidirectional parallel decodin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3240ms

[297/1822] semi-supervised split learning frameworks for addressing int...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3218ms

[298/1822] how to implement split learning for resource-constrained LEO...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.713 | Time: 2523ms

[299/1822] How to adapt ConvNeXt architectures for facial emotion recog...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 4831ms

[300/1822] Deep learning frameworks for facial expression recognition t...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3200ms

[301/1822] Reducing hallucinations in legal question answering systems ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.844 | Time: 3445ms

[302/1822] Benchmarking and evaluating large language model factuality ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.821 | Time: 4297ms

[303/1822] How can modified harmful datasets bypass moderation guardrai...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3969ms

[304/1822] Evaluating the effectiveness of guardrail moderation in prev...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.889 | Time: 3877ms

[305/1822] How to integrate natural language, algorithmic, and symbolic...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2507ms

[306/1822] Progressive paradigm training strategies for unifying multip...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 2983ms

[307/1822] training large language models to adaptively allocate infere...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3327ms

[308/1822] Inference Budget-Constrained Policy Optimization for improvi...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 4244ms

[309/1822] What are the common limitations and reliability issues of us...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 3613ms

[310/1822] Frameworks and algorithms for improving the alignment and cr...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3449ms

[311/1822] synthetic benchmarks for evaluating the long-context reasoni...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2704ms

[312/1822] how do large language models perform on long-context logical...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5801ms

[313/1822] How can sparse autoencoder features be optimized for precise...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4764ms

[314/1822] Comparison of Feature Guided Activation Additions with Contr...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 4709ms

[315/1822] how to automatically convert open ended visual question answ...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.641 | Time: 4662ms

[316/1822] agent based framework for generating challenging distractors...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.681 | Time: 2302ms

[317/1822] How to adapt a small auto-regressive model like Qwen2 for mu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 5253ms

[318/1822] What are the most effective training data cleaning and sampl...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3712ms

[319/1822] How can Kolmogorov-Arnold Networks be combined with recurren...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2436ms

[320/1822] Applying learnable temporal spline functions and edge-based ...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.730 | Time: 2688ms

[321/1822] how to use causal reward modeling and counterfactual invaria...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 4446ms

[322/1822] improving the fairness and reliability of large language mod...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.748 | Time: 2784ms

[323/1822] comprehensive review of mitigation strategies for large lang...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.849 | Time: 4906ms

[324/1822] recent advancements in responsible AI for enhancing large la...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 3048ms

[325/1822] How can matching user queries against pre-generated question...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3022ms

[326/1822] Techniques for reducing information dilution in RAG systems ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3041ms

[327/1822] vision transformer framework using foundation models for ene...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.543 | Time: 4475ms

[328/1822] automated generation of training data from immunofluorescenc...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 4748ms

[329/1822] How to prevent attention distribution flattening in long con...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.720 | Time: 2958ms

[330/1822] Improving length generalization and key information retrieva...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 4316ms

[331/1822] How can external document knowledge be integrated directly i...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 4865ms

[332/1822] Techniques for parameterizing retrieved knowledge into model...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 3386ms

[333/1822] What are the limitations of graph neural networks for solvin...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3463ms

[334/1822] Investigating the computational power of message passing GNN...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 2943ms

[335/1822] How to implement dynamic workflow adjustment and modular sub...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.811 | Time: 4972ms

[336/1822] Research on enhancing multi-agent framework performance thro...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.951 | Time: 3809ms

[337/1822] How can developers build a structured safety case to prove t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3195ms

[338/1822] Evaluating AI control safety by using red teaming and conser...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 3704ms

[339/1822] Comprehensive survey on explainable artificial intelligence ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 3025ms

[340/1822] How can large language models and vision-language frameworks...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 4612ms

[341/1822] benchmarking the performance of constrained decoding framewo...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 2961ms

[342/1822] how do different constrained decoding tools compare in terms...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3462ms

[343/1822] how to merge multiple deep learning models sequentially with...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2901ms

[344/1822] training-free methods for scalable continual model merging t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 4877ms

[345/1822] How to use audio large language models for natural language-...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3588ms

[346/1822] Alignment approach with LLM distillation for enhancing audio...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3426ms

[347/1822] benchmarking page-level and layout-level retrieval systems f...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3384ms

[348/1822] evaluation of visual retrievers versus text-based retrievers...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2628ms

[349/1822] How to improve multilingual reasoning in large language mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3375ms

[350/1822] Efficient methods for multilingual reasoning alignment that ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3137ms

[351/1822] How can large language models be trained to reason over user...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3583ms

[352/1822] Self-training frameworks for personalized LLMs that utilize ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 4072ms

[353/1822] methods for reducing hallucinations in multimodal large lang...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 3554ms

[354/1822] improving multimodal LLM alignment through cross-modal prefe...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.894 | Time: 4611ms

[355/1822] How does Meta use Large Language Models for mutation-guided ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.871 | Time: 2399ms

[356/1822] LLM-based test generation framework for hardening Android Ko...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.761 | Time: 2833ms

[357/1822] How to implement a neuro-fuzzy system on an FPGA for real-ti...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2623ms

[358/1822] FPGA-based intelligent sensor for personalizing time headway...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3966ms

[359/1822] impact of AWQ and GPTQ low-bit quantization on the mathemati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 3894ms

[360/1822] effective fine-tuning strategies to restore mathematical rea...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 2157ms

[361/1822] systematic review of large language models for natural disas...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 2802ms

[362/1822] how are generative artificial intelligence and large languag...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.412 | Time: 4045ms

[363/1822] Does OpenAI's o3 model represent true artificial general int...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.970 | Time: 3629ms

[364/1822] Critique of massive trialling of predefined operations as a ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.491 | Time: 3601ms

[365/1822] Evaluating implicit sociodemographic bias in large language ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2777ms

[366/1822] Do advanced large language models exhibit greater implicit b...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3261ms

[367/1822] Systematic literature review of large language models in CHI...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2543ms

[368/1822] How are large language models being used as research tools a...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 3487ms

[369/1822] Evaluating personalized long-form text generation using larg...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 3228ms

[370/1822] How to extract atomic aspects and evidence from LLM generate...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3827ms

[371/1822] large scale dataset of high quality science problem solution...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5180ms

[372/1822] automated extraction pipeline for building scientific reason...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3692ms

[373/1822] comprehensive survey of gradient-based multi-objective optim...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.936 | Time: 3055ms

[374/1822] recent advancements in learning continuous Pareto sets and f...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3981ms

[375/1822] Comprehensive survey of deep reinforcement learning algorith...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4766ms

[376/1822] How does deep reinforcement learning compare to traditional ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 4352ms

[377/1822] Survey of model optimization and system architecture strateg...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.905 | Time: 3520ms

[378/1822] How can cognitive edge computing frameworks balance latency,...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 3048ms

[379/1822] how to generate 360 panoramas using multi-view diffusion mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 4884ms

[380/1822] diffusion based method for high resolution 360 degree panora...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 5054ms

[381/1822] How to improve concept erasure in diffusion models by dynami...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 2690ms

[382/1822] Adaptive Guided Erasure method for selectively removing harm...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.777 | Time: 4066ms

[383/1822] How does a hybrid attention mechanism combining global and l...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.757 | Time: 2883ms

[384/1822] Comparison of RoPE, NoPE, and QK-Normalization patterns in t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 4838ms

[385/1822] How to design a multi-agent framework using large language m...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2363ms

[386/1822] LLM-based approach for mapping learner goals to skills and o...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3334ms

[387/1822] how do large language models exhibit conformity bias and gro...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.599 | Time: 4163ms

[388/1822] benchmarking social influence in AI agents using BenchForm t...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3968ms

[389/1822] benchmarking framework for evaluating large language model b...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3777ms

[390/1822] holistic platform for testing ai agents using microservice f...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2496ms

[391/1822] How can multimodal prompts like images videos and humming be...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2790ms

[392/1822] A generalized framework for symbolic music generation that u...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 2736ms

[393/1822] how to implement training-free visual token pruning for mult...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3206ms

[394/1822] reducing visual redundancy in MLLMs like LLaVA-NeXT through ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4583ms

[395/1822] distributed training of large language models using asynchro...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2855ms

[396/1822] how to improve DiLoCo for distributed training by overlappin...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2266ms

[397/1822] How can count-based exploration and optimistic reward estima...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.757 | Time: 4715ms

[398/1822] A practical algorithm for online RLHF that uses a coin-flip ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 4286ms

[399/1822] How can large language models be used to identify adversaria...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 3699ms

[400/1822] Improving the safety and robustness of autonomous vehicles t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 4261ms

[401/1822] Observational study on how programming students use generati...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.813 | Time: 3841ms

[402/1822] How do computer science students interact with large languag...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3684ms

[403/1822] Video reasoning segmentation using multimodal large language...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.937 | Time: 2651ms

[404/1822] Improving video reasoning segmentation accuracy on ReVOS ben...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2203ms

[405/1822] How can concept activation vectors be used to steer large la...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 4087ms

[406/1822] Lightweight framework for granular control of LLM outputs by...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2710ms

[407/1822] How does GPT-4o perform on multimodal physics concept invent...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3016ms

[408/1822] Evaluating the multimodal and multilingual capabilities of l...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 3582ms

[409/1822] Evaluation of large language models for translating natural ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 3691ms

[410/1822] Fine-tuning small language models with distilled high-qualit...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 4027ms

[411/1822] Fine-grained complexity analysis of visual autoregressive mo...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2375ms

[412/1822] What are the computational limits and efficiency criteria fo...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3469ms

[413/1822] best practices and lessons learned from red teaming generati...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3380ms

[414/1822] what are the key methodologies and threat model ontologies u...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3252ms

[415/1822] How can large language models be fine-tuned to improve their...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3415ms

[416/1822] Techniques for training large language models to ignore coun...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 4539ms

[417/1822] relationship between non-smooth convex optimization theory a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3612ms

[418/1822] how to use optimization theory bounds to transfer optimal le...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.614 | Time: 4610ms

[419/1822] How can multi-modal large language models be improved for fi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.889 | Time: 5072ms

[420/1822] Enhancing fine-grained recognition in MLLMs using contrastiv...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2540ms

[421/1822] enhancing knowledge base question answering through agentic ...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.595 | Time: 2708ms

[422/1822] improving low resource kbqa performance with mcts guided exp...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2780ms

[423/1822] mathematical analysis of transformer layer dynamics using Vl...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2906ms

[424/1822] understanding the evolution of data anisotropy and clusterin...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3855ms

[425/1822] adaptive retrieval methods for overcoming bounded recall in ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 2517ms

[426/1822] improving retrieval recall by using listwise LLM rankers to ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 3766ms

[427/1822] How can large language model agents be used for goal-driven ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2547ms

[428/1822] Evaluation framework and novel dataset for assessing the qua...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.850 | Time: 3123ms

[429/1822] The relationship between softmax numerical stability and the...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 2350ms

[430/1822] Achieving grokking without regularization by mitigating soft...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 3286ms

[431/1822] automated techniques for optimizing prompts to improve test ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3913ms

[432/1822] how to automatically generate model-specific prompts for sof...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3070ms

[433/1822] How can interdisciplinary collaboration between physicists a...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 3624ms

[434/1822] A roadmap for creating large physics models using foundation...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.730 | Time: 4103ms

[435/1822] How can small language models achieve high performance in re...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 5030ms

[436/1822] Efficient retrieval augmented generation framework for resou...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 4327ms

[437/1822] How to reduce error accumulation in online test-time prompt ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4676ms

[438/1822] Techniques for adaptive prompt selection based on prediction...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2628ms

[439/1822] Multi-modal sequential recommendation models using hierarchi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3082ms

[440/1822] How can hierarchical mixture of experts and contrastive lear...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2399ms

[441/1822] A comprehensive review of Mixture of Experts architectures, ...
2026-02-19 17:09:54 | WARNING  | arxiv-rag.retriever | Dense search returned no results
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.719 | Time: 2544ms

[442/1822] How does the Mixture of Experts framework improve the perfor...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 3009ms

[443/1822] comprehensive guide to pre-training generative models and al...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 2805ms

[444/1822] academic reference for understanding the fundamental pillars...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.422 | Time: 2695ms

[445/1822] training free inference time methods for mitigating hallucin...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2636ms

[446/1822] How can contrastive decoding mechanisms and masking signific...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2629ms

[447/1822] How does the extended context window of large language model...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.910 | Time: 4101ms

[448/1822] Leveraging long context large language models for text to SQ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 3080ms

[449/1822] enhancing the performance of LLM input guardrails through ch...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.846 | Time: 4071ms

[450/1822] how can fine-tuning large language models as judges with cha...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3124ms

[451/1822] comprehensive review of text data augmentation methods using...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3270ms

[452/1822] current challenges and future opportunities in using generat...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 5112ms

[453/1822] How can deep learning models combining LSTM and CNN with att...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3012ms

[454/1822] Using synthetic minority over-sampling technique and hybrid ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4328ms

[455/1822] state of the art 8B small language model as a judge for gene...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 3813ms

[456/1822] techniques for training small language models for automated ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.798 | Time: 2696ms

[457/1822] How can self-reflection frameworks and simulated psychologic...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.844 | Time: 3530ms

[458/1822] Investigating the inconsistency between explicit and implici...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.860 | Time: 4671ms

[459/1822] How does GPU dynamic voltage and frequency scaling impact th...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2514ms

[460/1822] Analyzing the relationship between input sequence characteri...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3618ms

[461/1822] How can transformers perform full Bayesian inference using i...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 2211ms

[462/1822] Comparing transformer in-context learning with Markov Chain ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 4937ms

[463/1822] evaluating the effectiveness and limitations of reinforcemen...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 4902ms

[464/1822] hybrid training approaches combining RL and SFT to address r...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4162ms

[465/1822] How to apply speculative sampling techniques to accelerate i...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.945 | Time: 4833ms

[466/1822] Speeding up diffusion model generation using speculative dec...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2741ms

[467/1822] How to use synthetic persona data from Persona Hub to improv...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2355ms

[468/1822] Large-scale synthetic data generation strategies for trainin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.821 | Time: 4864ms

[469/1822] How can temperature sensitivity be used to detect if instruc...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2396ms

[470/1822] Assessing the vulnerability of vision-language models to mem...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 2341ms

[471/1822] How has constructionism evolved as an educational framework ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 3554ms

[472/1822] Applying constructionist principles to smart education model...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.883 | Time: 3458ms

[473/1822] agentic workflows for program synthesis using LLM quality ch...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3951ms

[474/1822] how to implement a dynamic multi-agent system for program sy...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2680ms

[475/1822] How to use hierarchical feature trees and high-level abstrac...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5653ms

[476/1822] Iterative feature tree synthesis framework for generating hi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4847ms

[477/1822] Recent surveys on large vision-language model alignment and ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3706ms

[478/1822] What are the primary causes of multimodal misalignment in vi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4501ms

[479/1822] performance comparison of GPT-4o and Claude 3.5 against trad...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3602ms

[480/1822] evaluating the accuracy of large language models for line-by...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4876ms

[481/1822] benchmarking the security and vulnerability of retrieval-aug...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3688ms

[482/1822] how do external knowledge injections and unverified context ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 5581ms

[483/1822] automated extraction of olympiad level math problems from on...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4694ms

[484/1822] benchmarking mathematical reasoning in large language models...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3283ms

[485/1822] benchmarking the performance of large language model agents ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4239ms

[486/1822] standardized evaluation framework for testing the planning a...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.873 | Time: 3547ms

[487/1822] benchmarking framework for evaluating the effectiveness of h...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.862 | Time: 4116ms

[488/1822] how well do modern hate speech detection systems perform aga...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 3729ms

[489/1822] comprehensive survey of generative artificial intelligence t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 4684ms

[490/1822] how are diffusion models and multimodal AI being applied to ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 2540ms

[491/1822] How can diffusion priors be used to balance perceptual quali...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 4939ms

[492/1822] A unified image restoration model using diffusion priors wit...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3742ms

[493/1822] validated tools and metrics for evaluating the quality and a...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2570ms

[494/1822] how to assess the structural and substantive validity of aut...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.763 | Time: 2250ms

[495/1822] investigating the emergence and formation of localized task ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4962ms

[496/1822] using task vector prompting loss to enhance task representat...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3985ms

[497/1822] How can large language models achieve higher knowledge densi...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 5209ms

[498/1822] Machine writing frameworks that simulate human-like cognitiv...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 4441ms

[499/1822] How can modified Algorithm-of-Thoughts techniques like AoT+ ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2525ms

[500/1822] Investigating the use of enhanced Algorithm-of-Thoughts fram...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.879 | Time: 4457ms

[501/1822] How can multi-agent systems and agentic AI frameworks like O...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 2607ms

[502/1822] Evaluation of hallucination mitigation strategies using hier...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.931 | Time: 2424ms

[503/1822] How can knowledge distillation and conditional variational a...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.871 | Time: 4203ms

[504/1822] Adaptive diversity distillation techniques for math word pro...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.945 | Time: 3880ms

[505/1822] How can large language models be used to generate functional...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2432ms

[506/1822] Leveraging LLMs and design layout graphs to automate the cre...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 4177ms

[507/1822] How well do large language models perform on Allen's interva...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2379ms

[508/1822] Evaluating large language model capabilities in temporal und...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2396ms

[509/1822] segment-level direct preference optimization for improving m...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 2260ms

[510/1822] how to optimize social agents using segment-based direct pre...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 4469ms

[511/1822] How to improve RAG systems for industrial applications using...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.929 | Time: 3900ms

[512/1822] Techniques for knowledge atomizing and knowledge-aware task ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3737ms

[513/1822] How can large language models be integrated into an analytic...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4796ms

[514/1822] A framework for analyzing multi-user XR sessions using a pla...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2995ms

[515/1822] generating diverse and customizable synthetic Q&A pairs for ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.839 | Time: 2426ms

[516/1822] a two-stage framework for producing lexically and semantical...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.716 | Time: 2674ms

[517/1822] How can large language models like fine-tuned BART and BERT ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3181ms

[518/1822] Proactive intrusion prediction framework for IoT security us...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3915ms

[519/1822] Evaluating the performance of machine-generated text detecto...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3001ms

[520/1822] Can machine learning models robustly detect AI-generated con...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 2636ms

[521/1822] How to improve safety visual reasoning in large vision-langu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3881ms

[522/1822] Reducing attack success rate in vision-language models by ad...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 4310ms

[523/1822] How to achieve temporally consistent video relighting using ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4779ms

[524/1822] Frameworks for video relighting that preserve illumination p...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3733ms

[525/1822] How to improve Sharpness-Aware Minimization performance by e...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3458ms

[526/1822] Analysis of SAM training dynamics using third-order stochast...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4907ms

[527/1822] evaluating uncertainty estimation versus self-knowledge for ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 2899ms

[528/1822] comprehensive analysis of uncertainty estimation techniques ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 2539ms

[529/1822] improving mathematical reasoning in large language models us...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.894 | Time: 5167ms

[530/1822] enhancing LLM math capabilities with a first-try strategy an...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.750 | Time: 3721ms

[531/1822] benchmarks for evaluating long-context language models on co...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.484 | Time: 4555ms

[532/1822] how do current large language models perform on tasks requir...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 5254ms

[533/1822] How can domain prompts and semantic prototypes be used in a ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3701ms

[534/1822] Diffusion-based time series generation using prototype assig...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3378ms

[535/1822] How can we effectively detect AI-generated text that has bee...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2411ms

[536/1822] A data-centric augmentation approach for building robust mod...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 4044ms

[537/1822] How does the presence of citations in large language model r...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3558ms

[538/1822] Experimental research examining if users trust LLM generated...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3010ms

[539/1822] How can retrieval-augmented dynamic prompt tuning improve th...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 1799ms

[540/1822] A framework for incomplete multimodal learning using retriev...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3171ms

[541/1822] How can model editing and attention head analysis be used to...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4815ms

[542/1822] Improving large language model robustness by identifying key...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4526ms

[543/1822] evaluating large language models on everyday moral dilemmas ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 4800ms

[544/1822] how do large language models perform on complex social ethic...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3923ms

[545/1822] Effective post-training quantization methods for Mamba archi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3369ms

[546/1822] How to use variance aligned rotation and Karhunen-Loeve Tran...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2331ms

[547/1822] How can black-box adversarial attacks disrupt decision-makin...
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.689 | Time: 3319ms

[548/1822] Evaluating the robustness of vision-language models in auton...
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.673 | Time: 3757ms

[549/1822] How to use optimal transport and Monge-Kantorovich vector ra...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 3575ms

[550/1822] Improving conformal prediction for multi-output regression a...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.937 | Time: 2903ms

[551/1822] How can direct preference optimization be used to personaliz...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 2483ms

[552/1822] Aligning diffusion models with multiple individual user rewa...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.885 | Time: 5387ms

[553/1822] Survey of recent research on how embodiment, symbol groundin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 4074ms

[554/1822] How are researchers addressing the limitations of large lang...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2569ms

[555/1822] academic benchmark for evaluating multi-hop tool use in larg...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.846 | Time: 2944ms

[556/1822] evaluation dataset for testing complex function calling and ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 3725ms

[557/1822] benchmarking cultural bias towards Western entities in Arabi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2135ms

[558/1822] impact of pre-training data frequency and subword tokenizati...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 2655ms

[559/1822] How can internal activation steering improve the safety and ...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.501 | Time: 3890ms

[560/1822] Methods for revising multimodal model activations during gen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4273ms

[561/1822] How can multi-agent large language model frameworks improve ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3537ms

[562/1822] Implementing multi-agent orchestration for geospatial tasks ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3391ms

[563/1822] How can we design just-in-time and human-verifiable security...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 4196ms

[564/1822] A framework for generating contextual security policies for ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2945ms

[565/1822] External safety evaluation results and red teaming findings ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 3667ms

[566/1822] Systematic generation of unsafe test inputs to assess the pr...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 3285ms

[567/1822] flexible and scalable training system for sparse mixture of ...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 2625ms

[568/1822] optimizing task scheduling and communication overhead in lar...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3391ms

[569/1822] How can retrieval augmented generation and large language mo...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.557 | Time: 4988ms

[570/1822] Implementing RAG-based systems for real-time phone call frau...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 4007ms

[571/1822] benchmarking the performance of multimodal large language mo...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.749 | Time: 2439ms

[572/1822] evaluating the ability of MLLMs to determine chronological e...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2714ms

[573/1822] Improving quantum machine learning performance by training t...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.543 | Time: 3402ms

[574/1822] End-to-end differentiable framework for learning parameteriz...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 3634ms

[575/1822] Best practices and training recipes for domain-adaptive post...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 4451ms

[576/1822] How does combining continual pre-training with instruction-f...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.911 | Time: 3194ms

[577/1822] how to model long-range dependencies in brain networks using...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3015ms

[578/1822] using biased random walks in brain graph transformers to mod...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3651ms

[579/1822] adaptive perturbation methods for mitigating harmful fine-tu...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2732ms

[580/1822] how to recover large language model safety alignment after h...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 4996ms

[581/1822] comprehensive benchmark for evaluating undergraduate level m...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2775ms

[582/1822] evaluating large reasoning models using metrics like effecti...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2526ms

[583/1822] How to improve table understanding in language models using ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.924 | Time: 4071ms

[584/1822] A framework for enhancing table reasoning in LLMs by extract...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.970 | Time: 3650ms

[585/1822] comprehensive review of foundation models in computational p...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2448ms

[586/1822] what are the current challenges and methodologies for buildi...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2925ms

[587/1822] adaptive world model reinforcement learning for autonomous d...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.891 | Time: 3949ms

[588/1822] how to address distribution shift in world model based plann...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.973 | Time: 3952ms

[589/1822] A comprehensive review of recent deep learning architectures...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 5088ms

[590/1822] How do modern deep learning foundation models categorize the...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 4776ms

[591/1822] evaluation of explainability methods for encoder based langu...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.820 | Time: 3102ms

[592/1822] comparative analysis of lime shap and lrp techniques for tra...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2857ms

[593/1822] How do attention modules in different transformer layers con...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2608ms

[594/1822] Analyzing the role of early versus late transformer layers i...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3417ms

[595/1822] evaluating the reliability of large language models as judge...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 3148ms

[596/1822] comparing human rater performance with LLM as judge models u...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 3791ms

[597/1822] How to generate task-specific LoRA weights using Conditional...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2945ms

[598/1822] Using meta-learning and CVAE generators to produce task-awar...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3362ms

[599/1822] how to use a world knowledge tree and self-reflection refine...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.848 | Time: 3409ms

[600/1822] framework for scaling supervised fine-tuning data through kn...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.396 | Time: 2944ms

[601/1822] Challenges and future research directions for integrating mu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4919ms

[602/1822] What are the essential requirements and desiderata for devel...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3830ms

[603/1822] how to use prioritized depth-first search and large language...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 1906ms

[604/1822] combining embedding based retrieval with search heuristics t...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 2783ms

[605/1822] taxonomy of interaction types between software developers an...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 3186ms

[606/1822] how do developers interact with generative AI and large lang...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.778 | Time: 3653ms

[607/1822] A comprehensive analysis and taxonomy of common error types ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4262ms

[608/1822] New methods for efficient error detection and automated repa...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 5061ms

[609/1822] comprehensive survey of large language models in bioinformat...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 2907ms

[610/1822] what are the current challenges and future directions for us...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.638 | Time: 4852ms

[611/1822] comprehensive review of test-time compute scaling methods fo...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3986ms

[612/1822] survey on inference time computation techniques such as self...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2382ms

[613/1822] integrating large language model multi-agent frameworks with...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.874 | Time: 3702ms

[614/1822] performance evaluation of Autogen based multi-agent systems ...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.770 | Time: 2449ms

[615/1822] How do large language models respond to malicious jailbreaki...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4121ms

[616/1822] Investigating the impact of using fabricated scientific argu...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4000ms

[617/1822] factors influencing high school students' acceptance of gene...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3772ms

[618/1822] research on the role of perceived enjoyment and compatibilit...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.922 | Time: 3671ms

[619/1822] dataset for evaluating vision language models on handwritten...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 3703ms

[620/1822] how well do vision language models perform on reasoning task...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 3751ms

[621/1822] adaptive interpolation methods for knowledge distillation to...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3446ms

[622/1822] how to prevent mode collapse and mode averaging in language ...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3541ms

[623/1822] automated methods for optimizing large language model pretra...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 4146ms

[624/1822] how to improve LLM training efficiency with UtiliMax and MED...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2810ms

[625/1822] Diffusion transformer models for joint image and video virtu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 3830ms

[626/1822] How to maintain temporal consistency in long video virtual t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 3449ms

[627/1822] how to mitigate systematic misalignment in reinforcement lea...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 3398ms

[628/1822] the impact of providing evaluators with simulated future con...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.564 | Time: 2796ms

[629/1822] How to perform precise free-form grounding across multiple i...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2782ms

[630/1822] Improving multi-image grounding capabilities in MLLMs throug...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.580 | Time: 2563ms

[631/1822] How can large language models be integrated with high-order ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3374ms

[632/1822] Framework for mixed-type data imputation using bidirectional...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3663ms

[633/1822] How can large language models and CTGANs be used to generate...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 2859ms

[634/1822] Evaluating the performance of GPT-based models and CTGAN in ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4208ms

[635/1822] How can optimized soft prompts in the textual embedding spac...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4123ms

[636/1822] Effective safety alignment for diffusion models using catego...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4104ms

[637/1822] using reinforcement learning and representation space guidan...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 4866ms

[638/1822] interpretable reinforcement learning methods for LLM jailbre...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 3888ms

[639/1822] how to improve diversity in mixture of experts for low-rank ...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 4446ms

[640/1822] using the Gram-Schmidt process and Stiefel manifold to enhan...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.923 | Time: 4140ms

[641/1822] How can hierarchical autoregressive transformers combine cha...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.900 | Time: 4008ms

[642/1822] Large language models using character-to-word hierarchical a...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.935 | Time: 4036ms

[643/1822] How does the increase in energy loss in the final layer of l...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 3986ms

[644/1822] Effective methods for mitigating reward hacking in RLHF by p...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3471ms

[645/1822] certified robustness of large language models using randomiz...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 3777ms

[646/1822] how to calculate tight lower bounds for the worst-case robus...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 3886ms

[647/1822] How do scaling laws for large language models change when in...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.641 | Time: 3059ms

[648/1822] Empirical analysis of scaling laws for training differential...
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.698 | Time: 4261ms

[649/1822] How consistent are large language models like GPT-4 and Clau...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3873ms

[650/1822] Measuring the instability of LLM outputs for legal decision ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3262ms

[651/1822] Assessing the ability of large language models to trace exec...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3680ms

[652/1822] Measuring the gap between code generation performance and st...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.949 | Time: 3758ms

[653/1822] impact of using problem-solving data versus general mathemat...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.944 | Time: 3929ms

[654/1822] comparing mathematical reasoning performance of LLMs trained...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 4133ms

[655/1822] How to modify Chinchilla scaling laws to include inference l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3378ms

[656/1822] Training language models for inference efficiency by co-opti...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4388ms

[657/1822] How to use a hierarchical mixture-of-experts framework to mo...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3277ms

[658/1822] Advanced multimodal fake news detection methods focusing on ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.952 | Time: 4890ms

[659/1822] Evaluating instruction-following large language models for z...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3440ms

[660/1822] How can large language models leverage natural language infe...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4154ms

[661/1822] accelerating diffusion model inference by exploiting tempora...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5019ms

[662/1822] how to design a hardware accelerator that leverages temporal...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3834ms

[663/1822] A comprehensive survey of five hundred seventy two code benc...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4329ms

[664/1822] The HOW2BENCH framework and checklists for improving the qua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4679ms

[665/1822] How does complexity control and neuron condensation influenc...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 3586ms

[666/1822] Investigating the internal information circuits and complexi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4085ms

[667/1822] using fine-tuned small language models like Llama 3 to gener...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 4651ms

[668/1822] evaluating the effectiveness of quantized small language mod...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 4837ms

[669/1822] Evaluating end-to-end spoken language models on knowledge un...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 4736ms

[670/1822] A benchmark for assessing the robustness and world knowledge...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4333ms

[671/1822] dynamic self-adaptation of large language models through sin...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 5356ms

[672/1822] efficient alternative to LoRA for real-time task specific ad...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 4164ms

[673/1822] fine-tuning T5-small for scalable and topic-controlled quest...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3919ms

[674/1822] how to generate semantically aligned and topic-specific ques...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 3825ms

[675/1822] Open source TypeScript framework for building autonomous AI ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 3165ms

[676/1822] How to integrate large language models with web3 application...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3818ms

[677/1822] Theoretical analysis of prompt optimization as an alternativ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2825ms

[678/1822] How can prompt optimization be formulated as an optimization...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.564 | Time: 3634ms

[679/1822] adaptive sublayer skipping techniques to accelerate prefilli...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2265ms

[680/1822] how to improve long-context LLM inference efficiency by iden...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 3892ms

[681/1822] Dynamic structured pruning for large language models that ad...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.484 | Time: 3731ms

[682/1822] Using a sparse mask predictor to dynamically select relevant...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3015ms

[683/1822] efficient large language model tool learning methods using p...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 4801ms

[684/1822] how to improve LLM tool calling efficiency by dividing compl...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3951ms

[685/1822] How does in-execution self-debugging using intermediate stat...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4005ms

[686/1822] Improving large language model programming performance throu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3885ms

[687/1822] how to implement curriculum learning for large language mode...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.442 | Time: 4319ms

[688/1822] dynamic pretraining data selection strategies based on chang...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2905ms

[689/1822] How well do large language models like GPT-4 and Claude perf...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3026ms

[690/1822] Benchmarking different large language models and prompt engi...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.599 | Time: 3603ms

[691/1822] How can we identify and edit specific gender neurons in larg...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 4814ms

[692/1822] A method for mitigating gender bias in LLMs through interpre...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 4710ms

[693/1822] Strategies for accelerating deep learning inference on resou...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4881ms

[694/1822] A comprehensive review of techniques like pruning, quantizat...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.877 | Time: 3205ms

[695/1822] How to improve text-to-CAD generation using large language m...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.942 | Time: 2767ms

[696/1822] Training large language models for CAD model creation throug...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.863 | Time: 2746ms

[697/1822] How can internal representations and hidden states of large ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 3632ms

[698/1822] Evaluating LLM code generation quality by analyzing latent s...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 3879ms

[699/1822] how to measure and detect conversational bias in multi-agent...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.713 | Time: 2462ms

[700/1822] evaluating why traditional questionnaire-based bias detectio...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 4732ms

[701/1822] enhancing graph retrieval-augmented generation for medical r...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 3816ms

[702/1822] improving large language model explainability in high-stakes...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 2585ms

[703/1822] How can attackers manipulate voting-based LLM leaderboards l...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.940 | Time: 4794ms

[704/1822] Security vulnerabilities and mitigation strategies for crowd...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 4126ms

[705/1822] improving mathematical reasoning in large language models by...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2508ms

[706/1822] methods for optimizing the intermediate steps of chain-of-th...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 3848ms

[707/1822] critic-free reinforcement learning from human feedback using...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3261ms

[708/1822] comparison of local versus global advantage normalization in...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 3807ms

[709/1822] How to implement multimodal large language model multi-agent...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.876 | Time: 5102ms

[710/1822] Designing no-code frameworks for multimodal multi-agent syst...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 3880ms

[711/1822] foundation model for automating systematic reviews through h...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3734ms

[712/1822] performance of large language models compared to clinicians ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 3072ms

[713/1822] How to train neural networks with brain-like topographic org...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 4538ms

[714/1822] Developing spatially organized artificial neural networks th...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 4300ms

[715/1822] Expert annotated datasets for legal information retrieval an...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 5432ms

[716/1822] What are the available benchmarks for evaluating retrieval s...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3590ms

[717/1822] decentralized framework for specialized large language model...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3789ms

[718/1822] how can blockchain technology be integrated with fine-tuned ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.611 | Time: 2733ms

[719/1822] Multi-agent system for question answering using routing and ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2919ms

[720/1822] How can routing and planning in multi-agent RAG systems impr...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2686ms

[721/1822] How can crowdsourced LLM leaderboards like Chatbot Arena be ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.910 | Time: 4232ms

[722/1822] Analysis of the vulnerability of Elo rating systems in large...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 2813ms

[723/1822] How to improve the transparency and verification of intermed...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.853 | Time: 3790ms

[724/1822] Segmenting the chain of thought reasoning process into layer...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 4114ms

[725/1822] using crowdsourced metaphors to analyze public perception of...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3057ms

[726/1822] how do open-ended mental models and metaphors predict trust ...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3653ms

[727/1822] how to automatically verify the factual accuracy of large la...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.591 | Time: 2746ms

[728/1822] benchmarking automated systems for fact-checking medical dis...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2648ms

[729/1822] enhancing clinical reasoning in small language models throug...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 2394ms

[730/1822] self-evolving framework for medical reasoning using soft dua...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2470ms

[731/1822] how to improve the diversity of language model outputs in RL...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.938 | Time: 4032ms

[732/1822] techniques for balancing human preference alignment and resp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1916ms

[733/1822] Synthesizing long-context training data for large language m...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 2723ms

[734/1822] How can we effectively train long-context large language mod...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.564 | Time: 3463ms

[735/1822] How do next-generation intelligent tutoring systems like Soc...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3601ms

[736/1822] Using generative AI and JSON-based prompts to create adaptiv...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.879 | Time: 3159ms

[737/1822] How to perform hierarchical code summarization for entire so...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2547ms

[738/1822] Improving repository-level software documentation through sy...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.749 | Time: 3377ms

[739/1822] How can Simulation Theory and task decomposition be used to ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3908ms

[740/1822] Enhancing LLM performance on higher-order Theory of Mind tas...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3604ms

[741/1822] How can a taxonomy of user information needs guide the integ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3151ms

[742/1822] Synergies between large language models and knowledge graphs...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2546ms

[743/1822] how to use large language models and constraint logic progra...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.614 | Time: 4621ms

[744/1822] combining LLMs with logic programs to improve the accuracy a...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2861ms

[745/1822] Controllable video generation using blob representations and...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 3006ms

[746/1822] Methods for enhancing compositional text-to-video generation...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2988ms

[747/1822] Empirical study evaluating the proficiency of code large lan...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 4416ms

[748/1822] How do the internal biases of code LLMs affect their ability...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2702ms

[749/1822] How to achieve faster LLM inference on mobile devices by usi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3697ms

[750/1822] Optimization techniques for on-device large language model i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 3722ms

[751/1822] applying superstatistical methods and q-Gaussian distributio...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 1947ms

[752/1822] using the Informer transformer model and LightGBM for long-t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 3712ms

[753/1822] Multi-agent systems using small language models and retrieva...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3880ms

[754/1822] Using fine-tuned language models and RAG to democratize bioi...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 2781ms

[755/1822] semi-supervised learning methods for fine-grained action rec...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2616ms

[756/1822] how to improve fine-grained action recognition with limited ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4849ms

[757/1822] Scalable graph neural network framework for recommendation u...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3906ms

[758/1822] How to improve the efficiency and robustness of graph based ...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3624ms

[759/1822] evaluating the reliability and cultural sensitivity of large...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 2992ms

[760/1822] benchmarking value misalignment in open-source large languag...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.776 | Time: 3348ms

[761/1822] how to improve multimodal large language model performance o...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 3086ms

[762/1822] benchmarking numerical reasoning and structure recognition i...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2853ms

[763/1822] How can internal chain of thought reasoning steps in customi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4218ms

[764/1822] Stealthy backdoor attacks on large language models that use ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4334ms

[765/1822] how to improve contextual faithfulness in retrieval-augmente...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3760ms

[766/1822] enhancing faithfulness in long-form question answering by tr...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4172ms

[767/1822] How to use language-guided cross-attention mechanisms to pru...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 4257ms

[768/1822] A simple plug-and-play method for vision token pruning in ML...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3289ms

[769/1822] How to use Transformer models for controllable multitrack MI...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.604 | Time: 2971ms

[770/1822] Generative music models for computer-assisted composition th...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 3048ms

[771/1822] How can structured prompt design and in-context learning tec...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 2912ms

[772/1822] Evaluating the effectiveness of general-purpose large langua...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 3011ms

[773/1822] research on context-aware safety benchmarks for large langua...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2805ms

[774/1822] how to evaluate large language model safety by considering c...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2756ms

[775/1822] how to use knowledge graph retrieval augmented generation to...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 3258ms

[776/1822] graph based retrieval methods for resolving semantic ambigui...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2854ms

[777/1822] How to measure and reduce redundancy in multi-modality large...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 3675ms

[778/1822] Quantitative analysis of redundant test questions and overla...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 3981ms

[779/1822] How does changing the reward function shape with an alpha pa...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3077ms

[780/1822] Using AlphaPO to mitigate likelihood displacement and over-o...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 3845ms

[781/1822] Recent survey of foundation model based agents capable of co...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3578ms

[782/1822] What are the current research gaps and taxonomies for digita...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5236ms

[783/1822] machine learning models and explainable AI techniques for de...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 4764ms

[784/1822] comparison of XGBoost and Random Forest performance against ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 4022ms

[785/1822] How can a multi-agent LLM system provide decision interpreta...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 3936ms

[786/1822] Improving the reliability of LLM-based RTL code generation t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.800 | Time: 3236ms

[787/1822] efficient document compression methods for retrieval augment...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 4083ms

[788/1822] how to achieve high compression rates for RAG context window...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.687 | Time: 4845ms

[789/1822] how does using chain of thought reasoning influence the conf...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 3864ms

[790/1822] impact of providing reasoning steps on the overconfidence of...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 4753ms

[791/1822] how can chain of thought prompting be used to generate empat...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 3069ms

[792/1822] two-stage training approach for speech-based large language ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2997ms

[793/1822] evaluating instruction tuning data quality by measuring the ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3062ms

[794/1822] how to improve synthetic dataset integrity by filtering out ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2885ms

[795/1822] Evaluating the performance of causal sequence decoding model...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3347ms

[796/1822] How do language model decoders trained with cross-entropy lo...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.757 | Time: 3286ms

[797/1822] automatic prompt engineering for multi-step LLM pipelines us...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4580ms

[798/1822] how to optimize complex LLM workflows with feedback-based pr...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2733ms

[799/1822] training-free approach to long video understanding using con...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.611 | Time: 2423ms

[800/1822] how to process arbitrarily long videos in video question ans...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.389 | Time: 3580ms

[801/1822] How can large language model serving systems achieve both pr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 3741ms

[802/1822] The impact of Deficit Longest Prefix Match and D2LPM on thro...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3448ms

[803/1822] using entropy-based selective classifiers to estimate confid...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 3474ms

[804/1822] comparing model calibration and error detection performance ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3040ms

[805/1822] how to improve retrieval-augmented generation performance us...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.885 | Time: 2912ms

[806/1822] impact of multi-granular and self-contained document chunkin...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2280ms

[807/1822] Performance comparison of monolingual versus multilingual BE...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 3817ms

[808/1822] Introduction of a public UPOS-tagged dataset and evaluation ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.850 | Time: 2773ms

[809/1822] AI-powered learning platform using Retrieval-Augmented Gener...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.933 | Time: 5371ms

[810/1822] How can agentic Large Language Model assistants provide pers...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 2574ms

[811/1822] How does the Graph-PReFLexOR framework use graph reasoning a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4154ms

[812/1822] Integrating category theory and knowledge graph growth strat...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3584ms

[813/1822] high quality Chinese datasets for large language model pretr...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.705 | Time: 3326ms

[814/1822] curated Chinese language model training corpora for improvin...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.599 | Time: 2867ms

[815/1822] How to improve many-shot in-context learning performance in ...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.786 | Time: 4351ms

[816/1822] Techniques for addressing performance degradation in many-sh...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.660 | Time: 4934ms

[817/1822] How to improve context selection in multimodal RAG using rel...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 2774ms

[818/1822] Adaptive context selection and re-ranking methods for improv...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.389 | Time: 3793ms

[819/1822] benchmarks for evaluating adversarial robustness and composi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2965ms

[820/1822] how to improve the reliability of audio-visual LLMs using ca...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3784ms

[821/1822] How to improve GUI action grounding in novel environments us...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.953 | Time: 4500ms

[822/1822] Using MLLM based agents and Q-value-Incentive In-Context Rei...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.626 | Time: 3518ms

[823/1822] impact of learning rates and training data size on the out-o...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2785ms

[824/1822] how to optimize instruction tuning for table tasks while mai...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3030ms

[825/1822] How to use rank-wise mixture of experts in LoRA to improve m...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.874 | Time: 3653ms

[826/1822] Efficient parameter fine-tuning for multiple tasks by treati...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.789 | Time: 4372ms

[827/1822] evaluation datasets for measuring the instruction following ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 3026ms

[828/1822] how do multilingual retrieval models perform when given comp...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 2648ms

[829/1822] Applying social choice theory and maximal lotteries to impro...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.576 | Time: 3519ms

[830/1822] How does Nash Learning from Human Feedback approximate maxim...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 2551ms

[831/1822] LLM retrieval methods that align complex questions with data...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.567 | Time: 3249ms

[832/1822] How to use relationship exploration between data objects to ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.639 | Time: 2593ms

[833/1822] How can decoder-only LLMs be used for extractive schema link...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 2792ms

[834/1822] Improving schema linking accuracy and computational efficien...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.827 | Time: 3184ms

[835/1822] graph prompt tuning for heterophily graphs using distributio...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2127ms

[836/1822] how to use hop-specific prompts and generalized low-rank ada...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2777ms

[837/1822] How to implement hierarchical backpressure for autoscaling l...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 3687ms

[838/1822] Improving GPU utilization and SLO attainment in LLM inferenc...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.608 | Time: 4063ms

[839/1822] zero-shot hallucination detection in large language models u...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2951ms

[840/1822] how can attention weights and query categorization be used t...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2971ms

[841/1822] How vulnerable is GraphRAG to data poisoning attacks compare...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 3926ms

[842/1822] Evaluation of poisoning attacks on knowledge graph based RAG...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 3266ms

[843/1822] How to evaluate the coverage of diverse factual information ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3138ms

[844/1822] Automated framework for measuring information diversity and ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2692ms

[845/1822] theoretical framework for contrastive pre-training using app...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2166ms

[846/1822] sample complexity guarantees and joint generative hierarchic...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2378ms

[847/1822] How can retrieval-augmented dialogue knowledge aggregation i...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.835 | Time: 2386ms

[848/1822] A multi-granularity graph-based approach for aggregating sem...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3234ms

[849/1822] how can large language models be used for zero-shot and few-...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3700ms

[850/1822] large scale source code authorship identification using a to...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.703 | Time: 3807ms

[851/1822] Auto-regressive transformer models for graph generation usin...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 3918ms

[852/1822] How to use pre-trained transformers for graph property predi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3613ms

[853/1822] framework for evaluating multimodal retrieval augmented gene...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3917ms

[854/1822] measuring the reliability of multimodal RAG systems by asses...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2462ms

[855/1822] detailed training logs and implementation strategies for bui...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2922ms

[856/1822] open source resources and technical documentation for addres...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 2330ms

[857/1822] hybrid framework for automated log analysis using uncertaint...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2384ms

[858/1822] how to improve log analysis performance using large language...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2613ms

[859/1822] self-supervised quantization methods for integrating knowled...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 3657ms

[860/1822] how to use quantized entity representations to improve large...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.729 | Time: 2830ms

[861/1822] generalizing the logistic loss function for language modelin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 3061ms

[862/1822] evaluating alpha-divergence based loss functions and paralle...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 3851ms

[863/1822] How do current large language models perform on tasks evalua...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2879ms

[864/1822] A comprehensive evaluation framework and synthetic benchmark...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 4325ms

[865/1822] How do function encoders using least-squares optimization co...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 5113ms

[866/1822] A geometric approach to transfer learning characterizing int...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 4932ms

[867/1822] How does a block causal transformer architecture improve nex...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4909ms

[868/1822] Foundation models for fluid dynamics using block causal tran...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 6154ms

[869/1822] how to obtain valid confidence intervals when using machine ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3613ms

[870/1822] prediction powered inference bootstrap methods for debiasing...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3482ms

[871/1822] How can mixture of experts architectures improve the perform...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3115ms

[872/1822] Using rectified flow pose diffusion and multi-modal LLMs for...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 5681ms

[873/1822] using parameter trust regions to mitigate knowledge conflict...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4623ms

[874/1822] training-free techniques for multi-task model merging that p...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 3966ms

[875/1822] How to use probabilistic federated search to improve retriev...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 3210ms

[876/1822] Improving RAG performance for multi-product QA by aggregatin...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 3371ms

[877/1822] multilingual dataset for evaluating consistency of large lan...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2910ms

[878/1822] methodology for comparing health-related inquiry consistency...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2781ms

[879/1822] How can LLM-based multi-agent systems automate the entire fi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2970ms

[880/1822] A collaborative framework using multiple language model agen...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3310ms

[881/1822] How to precisely control camera extrinsic and intrinsic para...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 4136ms

[882/1822] Methods for adjusting camera angles and lens distortions in ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3935ms

[883/1822] How can attackers use training loss information from proprie...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4831ms

[884/1822] Vulnerability of Google Gemini models to adversarial prompt ...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 4459ms

[885/1822] Diffusion models for large scale neural network parameter ge...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.907 | Time: 5124ms

[886/1822] Techniques for generating full network parameters for vision...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 3107ms

[887/1822] How can abductive reasoning be used to infer user personas f...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3908ms

[888/1822] Improving LLM personalization by training on preference data...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.627 | Time: 3315ms

[889/1822] How can hierarchical attention mechanisms be used to improve...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2418ms

[890/1822] Recent approaches to zero-shot video-to-music generation usi...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3966ms

[891/1822] LLM text-to-SQL framework using statistical conformal predic...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.945 | Time: 3128ms

[892/1822] How can human-in-the-loop and branching point prediction imp...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.615 | Time: 3028ms

[893/1822] How can optimal transport-based alignment loss and attention...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3949ms

[894/1822] State-of-the-art methods for integrating visual cues into au...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4024ms

[895/1822] How are large language models being used to detect hate spee...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.900 | Time: 3730ms

[896/1822] Recent advancements in using cutting edge language models fo...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.501 | Time: 4125ms

[897/1822] how to generate counterfactual and contrastive explanations ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2091ms

[898/1822] model intrusive methods for interpreting DCNN image classifi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2494ms

[899/1822] How do dimensionality reduction techniques like PCA and UMAP...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 3804ms

[900/1822] Analyzing the multidimensional and layer-wise characteristic...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2734ms

[901/1822] how to optimize the scaling factor in low-rank adaptation to...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 3099ms

[902/1822] efficient methods for accuracy recovery when fine-tuning pru...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 4635ms

[903/1822] Dataset for long-form video understanding instruction tuning...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.862 | Time: 2954ms

[904/1822] How to improve video large language model performance on lon...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3882ms

[905/1822] evaluation of ChatGPT's ability to generate FEniCS and MATLA...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.977 | Time: 3904ms

[906/1822] using prompt engineering to implement numerical models for u...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 3130ms

[907/1822] comprehensive survey of recent advances in deep learning for...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2581ms

[908/1822] review of foundation models and specialized transformer arch...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3154ms

[909/1822] How can image-to-text conversion and chain-of-thought improv...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.517 | Time: 3958ms

[910/1822] Methods to mitigate modality imbalance in vision language mo...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.696 | Time: 4298ms

[911/1822] How can deep neural decision trees and forests be used to im...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2824ms

[912/1822] A comparative study on using deep neural decision forests an...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 2530ms

[913/1822] how to use program-driven verification and dual refinement t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 3147ms

[914/1822] enhancing large language model self-correction using self-ge...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4289ms

[915/1822] How can large language models be integrated with symbolic ac...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.482 | Time: 2865ms

[916/1822] Using action languages to bridge the gap between natural lan...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.479 | Time: 3538ms

[917/1822] how to improve the robustness of retrieval augmented generat...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.491 | Time: 2827ms

[918/1822] a training free plug and play framework for filtering malici...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4092ms

[919/1822] How can large language models use iterative self-questioning...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5644ms

[920/1822] State of the art methods for generating chronological news s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4007ms

[921/1822] How to perform efficient stylized question answering in larg...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.713 | Time: 4364ms

[922/1822] Lightweight and train-free methods for controlling LLM respo...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.729 | Time: 3891ms

[923/1822] How can large language models resolve conflicts between edit...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3315ms

[924/1822] A retrieval-based framework for updating large language mode...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3423ms

[925/1822] How to generate efficient Shapley value explanations for tim...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2451ms

[926/1822] Time-series transformer architectures that use Shapley-based...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 3292ms

[927/1822] speculative decoding for large language models using draft a...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 4719ms

[928/1822] how to implement lossless speculative decoding for accelerat...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2619ms

[929/1822] enhancing neural theorem proving with large language models ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3620ms

[930/1822] efficient recursive proving algorithms for automated theorem...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 3622ms

[931/1822] autonomous red teaming of multi-host networks using large la...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.730 | Time: 3529ms

[932/1822] evaluating the effectiveness of LLM-based penetration testin...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.905 | Time: 4105ms

[933/1822] dataset for panoptic segmentation-captioning with instance-s...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.595 | Time: 3255ms

[934/1822] improving region-level comprehension and language generation...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3058ms

[935/1822] how do large language models handle time-sensitive factual k...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 2996ms

[936/1822] improving the accuracy and consistency of large language mod...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 2640ms

[937/1822] How to evaluate large language models in personalized recomm...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2729ms

[938/1822] Assessing the capability of large language models to capture...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.904 | Time: 3312ms

[939/1822] applying test-time training to large language models for imp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3787ms

[940/1822] how to improve medical reasoning in LLMs using high quality ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3059ms

[941/1822] training medical patient simulators using dialogue strategie...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3506ms

[942/1822] investigating the relationship between medical inquiry quali...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 2950ms

[943/1822] Fine-grained medical vision-language pre-training using larg...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 2662ms

[944/1822] How to improve medical image analysis through knowledge inje...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.832 | Time: 4092ms

[945/1822] Using LLaVA and multimodal-to-text prompt engineering for id...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.839 | Time: 3806ms

[946/1822] How can large language models and feature embeddings be appl...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.900 | Time: 2612ms

[947/1822] evaluating sparse autoencoders for llm interpretability usin...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2234ms

[948/1822] how do sparse autoencoders distinguish different meanings of...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2775ms

[949/1822] how to improve self-adaptation in configurable systems using...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 3034ms

[950/1822] techniques for continuous configuration optimization in inte...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 5723ms

[951/1822] How to use large-scale synthetic data to improve spoken dial...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.876 | Time: 3938ms

[952/1822] Multi-turn spoken dialogue systems utilizing heterogeneous f...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 3578ms

[953/1822] multi-agent conversational bandit framework for online large...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.776 | Time: 2261ms

[954/1822] how to improve online evaluation and filtering of LLM respon...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.757 | Time: 2478ms

[955/1822] How to evaluate and improve long-context language models for...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 3065ms

[956/1822] Improving retrieval performance in long-context LLMs through...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 3921ms

[957/1822] comprehensive benchmarks for evaluating multi-modal large la...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3613ms

[958/1822] research on the performance of mainstream multi-modal assist...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2554ms

[959/1822] comprehensive benchmark for evaluating large language models...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 3165ms

[960/1822] standardized evaluation framework for testing the performanc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 4066ms

[961/1822] How to use unlearning strategies and layer-level patching to...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 5173ms

[962/1822] Defending against LLM jailbreak attacks by identifying vulne...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3621ms

[963/1822] Integrating domain knowledge from large language models with...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.543 | Time: 5087ms

[964/1822] How can large language models be used to guide Bayesian opti...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.578 | Time: 3842ms

[965/1822] synthetic data generation using multi-hop reasoning on conte...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.889 | Time: 3522ms

[966/1822] improving document-level fact checking and grounded factuali...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2847ms

[967/1822] How to implement ultra-low latency deep learning inference o...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.663 | Time: 3681ms

[968/1822] Optimizing lookup table based neural networks on FPGAs using...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 4036ms

[969/1822] Improving Tip-Adapter for vision-language models by incorpor...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3377ms

[970/1822] Theoretical understanding of training-free few-shot CLIP ada...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3227ms

[971/1822] framework for quantifying the extent of model distillation a...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 1781ms

[972/1822] how to evaluate the degree of knowledge distillation from te...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 4201ms

[973/1822] Improving cross-lingual knowledge consistency in large langu...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 4344ms

[974/1822] How to use self-consistent responses across different langua...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3808ms

[975/1822] How can in-context learning and retrieval-augmented generati...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.933 | Time: 2746ms

[976/1822] Comparing the performance of few-shot in-context learning an...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 4027ms

[977/1822] How to combine slot attention with pre-trained diffusion mod...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2610ms

[978/1822] Improving compositional image generation and object discover...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2714ms

[979/1822] How effective is fine-tuning large language models for the a...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 2601ms

[980/1822] Investigating the relationship between reasoning complexity ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4212ms

[981/1822] How can multi-listwise preference optimization be used to im...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 1779ms

[982/1822] Finetuning protein large language models for multi-attribute...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3338ms

[983/1822] evaluating the safety and vulnerability of large audio langu...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.881 | Time: 3947ms

[984/1822] benchmarking large audio language models for safety alignmen...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.868 | Time: 4141ms

[985/1822] learning-based multi-turn jailbreak attack framework for lar...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2884ms

[986/1822] How can multi-turn red-teaming strategies using turn-level L...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.641 | Time: 4007ms

[987/1822] What are the primary technical challenges in developing agen...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 4695ms

[988/1822] How to integrate large language models with neural graph dat...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.482 | Time: 2568ms

[989/1822] structured prompt engineering frameworks for developing task...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.874 | Time: 3256ms

[990/1822] using natural language specifications to design conversation...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 1907ms

[991/1822] How to use hybrid attention mechanisms and large language mo...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.567 | Time: 4819ms

[992/1822] Academic papers on combining textual statistical features wi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3238ms

[993/1822] How to improve large language model performance by aggregati...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2888ms

[994/1822] Fine-tuning techniques for large language models to synthesi...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3103ms

[995/1822] Can Looped Transformers perform neural algorithmic reasoning...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2975ms

[996/1822] Extending the neural algorithmic reasoning capabilities of L...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 4359ms

[997/1822] How to improve text-to-video generation models using text em...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.962 | Time: 4754ms

[998/1822] Enhancing text-to-video synthesis by identifying optimal tex...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.948 | Time: 4508ms

[999/1822] multilingual dataset for hate speech and abusive language de...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2617ms

[1000/1822] benchmarking machine learning models for hate speech classif...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 4487ms

[1001/1822] Bridging the gap between instruction tuning and pre-training...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4632ms

[1002/1822] Using adaptive data selection and controlled rewriting of pr...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3496ms

[1003/1822] How do padding tokens in text encoders affect the image gene...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.973 | Time: 4954ms

[1004/1822] Causal analysis of padding token representations in text-to-...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 4941ms

[1005/1822] How does adding random punctuation to mathematical prompts a...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.576 | Time: 3987ms

[1006/1822] Evaluating the vulnerability of math-specialized large langu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 2973ms

[1007/1822] benchmarking multimodal large language models for complex ge...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2298ms

[1008/1822] agent based framework for improving MLLM performance on geol...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.840 | Time: 3759ms

[1009/1822] How can adaptive projector fusion driven by user instruction...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3539ms

[1010/1822] Video large language models using instruction-based dynamic ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4663ms

[1011/1822] how does inter-model response agreement and focal loss impro...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4067ms

[1012/1822] improving large language model calibration using auxiliary m...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3637ms

[1013/1822] How to implement black-box watermarking for retrieval augmen...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4688ms

[1014/1822] Robust knowledge-based watermarking methods for LLM retrieva...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4038ms

[1015/1822] How effective is ChatGPT-4o at generating WCAG compliant web...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 3922ms

[1016/1822] Evaluating the utility of large language models in identifyi...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4770ms

[1017/1822] How can goal-conditioned reinforcement learning policies tra...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3267ms

[1018/1822] Achieving horizon generalization in RL through planning inva...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 3178ms

[1019/1822] How can segment-level reward models improve reinforcement le...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 3797ms

[1020/1822] Using dynamic text segmentation and location-aware normalize...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2850ms

[1021/1822] how to integrate AI-driven intrusion detection systems with ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 4598ms

[1022/1822] multilevel defense strategies combining artificial intellige...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 5878ms

[1023/1822] how to add early exit branches to pre-trained deep neural ne...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2948ms

[1024/1822] optimizing the speed accuracy tradeoff in deep learning usin...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3148ms

[1025/1822] how to improve the robustness of large language models again...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4765ms

[1026/1822] techniques for maintaining faithful integrity in LLMs to pre...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4047ms

[1027/1822] How can we evaluate the ability of large language models to ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 3283ms

[1028/1822] Benchmarks and datasets for assessing and improving the mark...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3561ms

[1029/1822] how to improve retrieval augmented generation for scientific...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 5074ms

[1030/1822] utilizing contextualized graph representations and dense-spa...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2998ms

[1031/1822] How can 2D Gaussian splatting be integrated with vector quan...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.614 | Time: 3173ms

[1032/1822] Using flexible 2D Gaussian features and splatting operations...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3731ms

[1033/1822] How can overlapping messages in text-based human-AI interact...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4934ms

[1034/1822] Designing conversational AI systems that support concurrent ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.899 | Time: 3396ms

[1035/1822] How can neural language models be used to prioritize configu...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4237ms

[1036/1822] Techniques for accelerating configuration performance bug te...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3624ms

[1037/1822] synthetic benchmarks for evaluating deductive reasoning in l...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3571ms

[1038/1822] how do state of the art reasoning models perform compared to...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 5182ms

[1039/1822] flexible modular framework for knowledge graph retrieval aug...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3275ms

[1040/1822] improving knowledge graph retrieval augmented generation by ...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 3255ms

[1041/1822] evaluating theory of mind in large language models using ver...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4679ms

[1042/1822] a multi-choice question answering benchmark for assessing fi...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 3053ms

[1043/1822] interpretable multiple instance learning for whole slide ima...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.505 | Time: 2706ms

[1044/1822] how to use clinical concepts and vision language models for ...
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 2252ms

[1045/1822] how to map multimodal llm hidden states to interpretable vis...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3896ms

[1046/1822] training free method for steering multimodal large language ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3765ms

[1047/1822] How to predict the accuracy of black-box language models by ...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3694ms

[1048/1822] Using follow-up query responses to identify misrepresented m...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 3837ms

[1049/1822] How to adaptively select semantically similar translation de...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 1984ms

[1050/1822] Improving neural machine translation in large language model...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2984ms

[1051/1822] How to improve Transformer attention mechanisms by incorpora...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2920ms

[1052/1822] Efficient fine-tuning of foundation models using sparse GIN-...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2552ms

[1053/1822] How can transformer architectures be optimized for generaliz...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.614 | Time: 5066ms

[1054/1822] Improving the out-of-distribution generalization of transfor...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3737ms

[1055/1822] How can multimodal large language models be used to help end...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 3809ms

[1056/1822] Interactive systems for authoring custom AI vision sensors u...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3781ms

[1057/1822] How can multi-agent large language model frameworks be used ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.611 | Time: 2297ms

[1058/1822] Framework for implementing collaborative LLM agents with spe...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.723 | Time: 2773ms

[1059/1822] How do language and text-to-image models exhibit religious b...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.942 | Time: 4099ms

[1060/1822] Measuring religious stereotypes in generative AI using natur...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 3522ms

[1061/1822] benchmarking generative AI models versus traditional ion exc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 4226ms

[1062/1822] how to improve generative materials discovery using post-gen...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2846ms

[1063/1822] large time series models with billion scale parameters incor...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 4088ms

[1064/1822] how to use patch convolutional embedding and human feedback ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2478ms

[1065/1822] Hierarchical tree-structured recommendation system using ret...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 3658ms

[1066/1822] How can RAG-enhanced hierarchical models improve the accurac...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.451 | Time: 2642ms

[1067/1822] How to use self-questioning techniques to reduce hallucinati...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2630ms

[1068/1822] Multi-round training framework for MLLMs that uses heuristic...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2720ms

[1069/1822] Analyzing phase transitions and emergent capabilities in lar...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 3527ms

[1070/1822] How to estimate the internal dimension and parameter suffici...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.538 | Time: 3768ms

[1071/1822] How can digital twins and ray tracing be used together with ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 4145ms

[1072/1822] Using a multi-step tuning process with AI to bridge the gap ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3933ms

[1073/1822] How can large language models be used to automatically gener...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2411ms

[1074/1822] Automating the transformation of REST APIs into AI compatibl...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.911 | Time: 3293ms

[1075/1822] robust nonlinear subspace clustering using data-driven kerne...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 2897ms

[1076/1822] how to improve kernel-based subspace clustering by learning ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 2833ms

[1077/1822] Evaluation of fine-tuned GPT-4o-mini for cost-effective and ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2827ms

[1078/1822] How can fine-tuned large language models improve de-identifi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 2458ms

[1079/1822] How to improve document retrieval accuracy using zero-shot r...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2609ms

[1080/1822] Techniques for zero-shot document re-ranking that use pre-tr...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 3498ms

[1081/1822] How can natural language inference models be improved to rec...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.937 | Time: 2573ms

[1082/1822] Evaluating the performance of large language models on impli...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2958ms

[1083/1822] Research comparing the emotional variance and sentiment posi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 4223ms

[1084/1822] How does the EmoXpt framework analyze differences in sentime...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3581ms

[1085/1822] current research roadmap and challenges for advancing large ...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 3346ms

[1086/1822] six-layer vision framework for analyzing orchestration and v...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 4654ms

[1087/1822] graph-based framework for generating stealthy jailbreak prom...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.751 | Time: 3321ms

[1088/1822] how can interconnected graph structures with pruning improve...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3872ms

[1089/1822] How can zero-shot large language models and prompt engineeri...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.486 | Time: 2938ms

[1090/1822] Effective frameworks for using LLMs to evaluate student comp...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3902ms

[1091/1822] How does the AIN bilingual multimodal model achieve state-of...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.827 | Time: 4027ms

[1092/1822] Development of a large multimodal model for Arabic and Engli...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 4745ms

[1093/1822] mathematical framework that unifies preference optimization ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 3896ms

[1094/1822] impact of optimization objectives and explicit reward models...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2500ms

[1095/1822] how to reduce activation memory during transformer fine-tuni...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 3831ms

[1096/1822] efficient fine-tuning methods for large language models that...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2395ms

[1097/1822] Bayes-optimal generalisation error for shallow two-layer neu...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 3448ms

[1098/1822] Phase transition from universal to specialisation learning i...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.953 | Time: 2921ms

[1099/1822] How can RAG and chain-of-thought reasoning be used with larg...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3642ms

[1100/1822] Using RAG-based agentic LLMs and chain-of-thought prompting ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.957 | Time: 4186ms

[1101/1822] evaluating large language models for longitudinal clinical s...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.442 | Time: 3390ms

[1102/1822] performance of retrieval augmented generation and chain of t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2829ms

[1103/1822] How to improve large language model reasoning accuracy by co...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3757ms

[1104/1822] Research on formalizing natural language reasoning tasks for...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4068ms

[1105/1822] How to improve resource efficiency in compound AI systems us...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 3374ms

[1106/1822] Architectural designs for decoupling orchestration and resou...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 3250ms

[1107/1822] Using pre-trained foundational vision transformers like DINO...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.884 | Time: 4911ms

[1108/1822] Machine learning of material properties from microstructures...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 3912ms

[1109/1822] how to perform knowledge distillation from large transformer...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.807 | Time: 5202ms

[1110/1822] distilling large scale transformer language models into rwkv...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2570ms

[1111/1822] how to trigger hallucinations in multimodal large language m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 4644ms

[1112/1822] transferable visual adversarial attacks against multimodal m...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.833 | Time: 2392ms

[1113/1822] how to combine large language models with symbolic solvers u...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2451ms

[1114/1822] neurosymbolic framework for improving large language model p...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.730 | Time: 2873ms

[1115/1822] survey of AI researchers' opinions on existential risk and t...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3181ms

[1116/1822] empirical study on why AI experts disagree about catastrophi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3537ms

[1117/1822] preference datasets and benchmarks for evaluating reward mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5467ms

[1118/1822] how to develop reward models for trustworthy long-context ge...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2781ms

[1119/1822] How can legal concept generation and Determinantal Point Pro...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4064ms

[1120/1822] Improving legal document retrieval by augmenting query facts...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 4210ms

[1121/1822] AI storytelling system using character symbol manipulation a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3847ms

[1122/1822] How can symbolic motions from toy-playing be used to guide l...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3970ms

[1123/1822] how to optimize large language model serving for multiple se...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2886ms

[1124/1822] efficient LLM inference systems that utilize hardware-aware ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3801ms

[1125/1822] how to use agentic frameworks and character knowledge graphs...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 3096ms

[1126/1822] improving the factual accuracy of large language model summa...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2751ms

[1127/1822] How to optimize confidence thresholds in large language mode...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3778ms

[1128/1822] Probabilistic modeling of joint error distributions to tune ...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.723 | Time: 4124ms

[1129/1822] Efficient LLM serving systems that co-locate latency-sensiti...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.412 | Time: 3315ms

[1130/1822] How can interference-aware scheduling and latency prediction...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3858ms

[1131/1822] How to reduce communication overhead in distributed large la...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4109ms

[1132/1822] Techniques for overlapping communication with computation in...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3072ms

[1133/1822] instruction tuning for video facial expression captioning an...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2759ms

[1134/1822] how can multimodal large language models be trained to provi...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.604 | Time: 4810ms

[1135/1822] How can multi-neuromodulatory systems like dopamine and nora...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 5002ms

[1136/1822] Bio-inspired learning rules using multi-scale neuromodulatio...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4366ms

[1137/1822] How to effectively integrate multiple vision encoders in mul...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.589 | Time: 5018ms

[1138/1822] Advanced fusion strategies for hybrid multimodal large langu...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.512 | Time: 4774ms

[1139/1822] What are the current state of the art techniques for early e...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.951 | Time: 5011ms

[1140/1822] Comprehensive review of methodologies using intermediate lay...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4340ms

[1141/1822] how to implement remote and automatic cognitive impairment s...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2549ms

[1142/1822] performance of large language models like DistilBERT in clas...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2560ms

[1143/1822] How do large language models reflect and amplify stereotypes...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.479 | Time: 3515ms

[1144/1822] Measuring representational harms and social bias against non...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2678ms

[1145/1822] how to exploit inter-iteration and intra-iteration output sp...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 2880ms

[1146/1822] software-hardware co-design for diffusion models using ffn r...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 4009ms

[1147/1822] how to achieve low-bit quantization of text-to-image diffusi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 4116ms

[1148/1822] addressing activation outliers and cross-attention distribut...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.543 | Time: 3973ms

[1149/1822] How to combine spatial layout information and semantic text ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2579ms

[1150/1822] A hybrid method for document segmentation using bounding box...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4606ms

[1151/1822] improving visual reasoning in large vision-language models u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4046ms

[1152/1822] DrivingVQA dataset for complex visual question answering in ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 4098ms

[1153/1822] Using explainable AI and machine learning to distinguish bet...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4231ms

[1154/1822] Differentiating between multiple LLM sources using deep lear...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 2736ms

[1155/1822] How can large language models be used for document-level tex...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2492ms

[1156/1822] Improving document simplification performance in large langu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 5572ms

[1157/1822] how can we evaluate the clinical appropriateness of conversa...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 1606ms

[1158/1822] benchmarking large language model based psychiatric agents t...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2325ms

[1159/1822] How can transformer-based models like mT5 and BanglaT5 be ap...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3954ms

[1160/1822] Fine-tuning pre-trained language models for solving Bengali ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3225ms

[1161/1822] How to improve text-image alignment in diffusion-based style...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.952 | Time: 4784ms

[1162/1822] Recent methods for balancing textual semantics and stylistic...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2944ms

[1163/1822] adversarial attacks on LLM routing systems to manipulate mod...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3816ms

[1164/1822] how can an adversary use confounder gadgets to compromise th...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2868ms

[1165/1822] A large-scale dataset of cybersecurity-specific prompts for ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.795 | Time: 4896ms

[1166/1822] Evaluating large language model security using close-ended p...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.877 | Time: 3291ms

[1167/1822] How can Large Language Models be used to perform interpretab...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.567 | Time: 3732ms

[1168/1822] Using adaptive retrieval-augmented generation to bridge soci...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 2753ms

[1169/1822] How can retrieval augmented generation and in-context learni...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.702 | Time: 5153ms

[1170/1822] A framework for dynamic retrieval of regional cultural value...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.599 | Time: 3138ms

[1171/1822] How does information bottleneck theory explain the internal ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2364ms

[1172/1822] Improving large language model reasoning and inference effic...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 4419ms

[1173/1822] How does data contamination in pre-training sets affect the ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 5086ms

[1174/1822] Controlled study measuring the impact of source and target d...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 3272ms

[1175/1822] Why do adaptive optimizers like Adam outperform SGD in trans...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 1853ms

[1176/1822] Analysis of how layer normalization placement affects gradie...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.937 | Time: 4790ms

[1177/1822] How can Generative Adversarial Networks be used to predict s...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2576ms

[1178/1822] Detecting smart grid instability and adversarial attacks usi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3618ms

[1179/1822] how to erase NSFW concepts from text to image diffusion mode...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.851 | Time: 4111ms

[1180/1822] efficient concept erasure for diffusion models using a seman...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 4728ms

[1181/1822] benchmark for evaluating the effectiveness of large language...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 3086ms

[1182/1822] comparing advanced reasoning models and classical LLMs on th...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 4435ms

[1183/1822] generative multimodal large language models for explicit sem...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2325ms

[1184/1822] large scale dataset and framework for training event-based v...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.378 | Time: 1984ms

[1185/1822] How can we automatically generate high-quality dialogue benc...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2601ms

[1186/1822] Using query-based subgraph retrieval and multi-stage LLM pip...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.532 | Time: 3478ms

[1187/1822] How effective are vision-language models like GPT and Gemini...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4032ms

[1188/1822] A study on using VLMs for the automated assessment of AR sce...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4189ms

[1189/1822] How to use ensemble models and committee voting strategies t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 4946ms

[1190/1822] Improving dataset distillation performance by leveraging col...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3607ms

[1191/1822] How to use ChatGPT and tag-based data analysis to generate p...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3816ms

[1192/1822] A method for converting student learning behavior data into ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3044ms

[1193/1822] retrosynthesis prediction using dual graph representations f...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 6294ms

[1194/1822] how to improve retrosynthesis prediction by combining dual g...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 5682ms

[1195/1822] How can reconfigurable optical circuit switching improve com...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.571 | Time: 4413ms

[1196/1822] Improving training cost efficiency of MoE models using a reg...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 3451ms

[1197/1822] how to use gpt models and hierarchical summarization for the...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4645ms

[1198/1822] leveraging large language models and prompt engineering for ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 4724ms

[1199/1822] automated neural architecture search and compression techniq...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4052ms

[1200/1822] how to design low latency neural networks for bragg peak fin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 4584ms

[1201/1822] How can large language models be used as automated simulator...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.821 | Time: 3663ms

[1202/1822] Evaluating concept-based explanation methods through automat...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3823ms

[1203/1822] How to reduce hallucinations in multimodal large language mo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3274ms

[1204/1822] A post-pretraining method for improving visual representatio...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 3327ms

[1205/1822] How to use multiple large language models to generate high q...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.713 | Time: 4999ms

[1206/1822] Improving scientific figure captioning by using multimodal L...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2879ms

[1207/1822] How can we identify and neutralize backdoor trigger tokens i...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2916ms

[1208/1822] Defending against backdoor attacks in natural language proce...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3206ms

[1209/1822] How do large language models simplify or omit representation...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2680ms

[1210/1822] Evaluating cultural representational gaps and power inequiti...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 4169ms

[1211/1822] How can semantic graphs and uncertainty propagation between ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2998ms

[1212/1822] Improving uncertainty-based hallucination detection by model...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3795ms

[1213/1822] how to generate counterfactual examples for natural language...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2747ms

[1214/1822] framework for automatic counterfactual generation using labe...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2362ms

[1215/1822] autonomous driving framework using dual-process decision mak...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2355ms

[1216/1822] improving autonomous vehicle navigation through cognitive pe...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3738ms

[1217/1822] how to improve video large language models by integrating mu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2597ms

[1218/1822] leveraging multiple frozen vision backbones to create unifie...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2648ms

[1219/1822] lightweight and explainable intrusion detection systems for ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3961ms

[1220/1822] how to improve transparency and computational efficiency in ...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 4680ms

[1221/1822] applying compositional diffusion models for 6 degree-of-free...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2431ms

[1222/1822] how can generative diffusion policies be used for few-shot a...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.768 | Time: 2408ms

[1223/1822] How can block-wise mixed format quantization using FP4 diale...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3584ms

[1224/1822] Fine-grained block-level quantization techniques for LLMs th...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3180ms

[1225/1822] evaluating multi-step tool use reasoning in large language m...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4161ms

[1226/1822] comparison of process supervised reward models and outcome s...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 3469ms

[1227/1822] how to use hadamard rotations to mitigate activation and wei...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2660ms

[1228/1822] low-precision fine-tuning of transformers using hadamard-ass...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 2968ms

[1229/1822] How to implement efficient context pruning using sequence la...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 4902ms

[1230/1822] A robust approach for combining context reranking and sequen...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3346ms

[1231/1822] How can large language models and knowledge graphs be used t...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3083ms

[1232/1822] Multi-branched reaction pathway search algorithm for identif...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.845 | Time: 3002ms

[1233/1822] datasets for training foundation models on linked business t...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.564 | Time: 3434ms

[1234/1822] what datasets are available for research in multi-table repr...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 2869ms

[1235/1822] hybrid retrieval-augmented generation framework for universi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2545ms

[1236/1822] effective strategies for implementing unified RAG systems in...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2906ms

[1237/1822] How to achieve zero-shot vision to speech generalization in ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2888ms

[1238/1822] Open-source omnimodal models for real-time emotional speech ...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3261ms

[1239/1822] How can large language models improve their own critique and...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.552 | Time: 3175ms

[1240/1822] Enhancing LLM self-critique capabilities through self-genera...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 4590ms

[1241/1822] How can large language models improve long-term memory in vo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3739ms

[1242/1822] Enhancing in-car voice assistant memory by using LLMs to ext...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4590ms

[1243/1822] How to use Conditional Value at Risk in reinforcement learni...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 3548ms

[1244/1822] Risk-averse fine-tuning methods for large language models to...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 5559ms

[1245/1822] How to use large language models for lexicon-based text embe...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.757 | Time: 4304ms

[1246/1822] Improving sparse or lexicon-based embedding performance on t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 4682ms

[1247/1822] How can domain-adversarial fine-tuning improve the generaliz...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 4088ms

[1248/1822] Enhancing small language model chain of thought performance ...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.782 | Time: 4140ms

[1249/1822] using large language models for automated regression test ge...
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.689 | Time: 3957ms

[1250/1822] feedback directed zero shot LLM approach for generating repr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 3657ms

[1251/1822] How to address influence score bias in data selection for in...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3005ms

[1252/1822] Balanced and influential data selection techniques for instr...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 5231ms

[1253/1822] how to use large language models for knowledge graph complet...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.580 | Time: 2694ms

[1254/1822] automated curriculum modeling and topic extraction from lect...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.933 | Time: 2197ms

[1255/1822] applying large language models to few-shot multivariate time...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 3863ms

[1256/1822] how to leverage pre-trained large language models for few-sh...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2727ms

[1257/1822] Evaluating the effectiveness of open-source and commercial l...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 3795ms

[1258/1822] How do large language models compare to human teaching assis...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 4121ms

[1259/1822] evaluating the fidelity and constraint satisfaction of large...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.884 | Time: 3517ms

[1260/1822] systematic framework and error-correction mechanisms for ass...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.637 | Time: 3963ms

[1261/1822] how can large language models perform zero-shot image and au...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4471ms

[1262/1822] a training-free approach using gradient-free optimization an...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4386ms

[1263/1822] methods for separating motion concepts from appearance in te...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.517 | Time: 3981ms

[1264/1822] using temporal attention purification and appearance highway...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2607ms

[1265/1822] How can chain-of-thought reasoning be applied to improve lar...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4205ms

[1266/1822] Using proactive intent analysis and reasoning steps to enhan...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3102ms

[1267/1822] fine-tuning large language models using ensembles of LoRA ex...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 3926ms

[1268/1822] how can clustering training data by gradient directions and ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 2954ms

[1269/1822] How can trie-based prefix tree structures be used to optimiz...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 4487ms

[1270/1822] Efficient beam search algorithms for LLMs that use shared pr...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.812 | Time: 2826ms

[1271/1822] how effective are large language models at extracting specie...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4445ms

[1272/1822] using large language models for automated extraction of spec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3170ms

[1273/1822] limitations of current large language model cybersecurity ev...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3856ms

[1274/1822] comprehensive risk assessment framework for LLM cyber capabi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2919ms

[1275/1822] How do one-layer attention-only transformers develop interna...
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.667 | Time: 4791ms

[1276/1822] Understanding how training data features shape the internal ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 3631ms

[1277/1822] How to use preference optimization to intrinsically reduce h...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.876 | Time: 4858ms

[1278/1822] Improving the reliability of LLM machine translation by fine...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.763 | Time: 4337ms

[1279/1822] comparing the performance of LSTM deep learning models and A...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2459ms

[1280/1822] how do Long Short-Term Memory networks compare to traditiona...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2117ms

[1281/1822] How does the integration of generative AI tools and experien...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 2429ms

[1282/1822] Research on human-AI collaboration in business education foc...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3941ms

[1283/1822] How can multimodal large language models perform GUI groundi...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.589 | Time: 5084ms

[1284/1822] State of the art methods for visual GUI agent grounding and ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.841 | Time: 4097ms

[1285/1822] How to improve the safety of large language model reward mod...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 3643ms

[1286/1822] Dynamic selection of the most important safety rules to maxi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2780ms

[1287/1822] How to optimize the order of in-context learning examples fo...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3603ms

[1288/1822] Dataset-free methods for finding the best sequence of few-sh...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3907ms

[1289/1822] systematic hyperparameter tuning for LLM-as-a-judge using mu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 4207ms

[1290/1822] how to find cost-efficient open-weight LLM judges through mu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 3902ms

[1291/1822] how to integrate domain specific large language models into ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3682ms

[1292/1822] leveraging a suite of general and specialized LLMs within an...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3243ms

[1293/1822] Evaluating large language models and artificial intelligence...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 3617ms

[1294/1822] Implications of AI personhood and moral status for the ethic...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 3623ms

[1295/1822] how to improve large language model reasoning and explainabi...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 2790ms

[1296/1822] using selective tree exploration and supervised fine tuning ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.807 | Time: 3077ms

[1297/1822] how to perform open-set test-time adaptation for multimodal ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3141ms

[1298/1822] robust multimodal test-time adaptation methods for identifyi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2429ms

[1299/1822] evaluating the correlation between cross-attention weights i...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2498ms

[1300/1822] investigating the plausibility of using cross-attention weig...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3512ms

[1301/1822] How can specific attention heads in LLMs be used for trainin...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 2834ms

[1302/1822] Techniques for improving large language model performance on...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.907 | Time: 4884ms

[1303/1822] How can Mixture-of-Experts models be improved by removing ro...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2811ms

[1304/1822] Research on expert self-selection mechanisms in language mod...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4253ms

[1305/1822] How to evaluate fact-conflicting hallucinations in small lan...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.944 | Time: 5202ms

[1306/1822] Benchmarking the factual analysis and context reasoning capa...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 3952ms

[1307/1822] using direct preference optimization to generate multiple ch...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.942 | Time: 1987ms

[1308/1822] training a model to predict student choices for generating m...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2478ms

[1309/1822] How can large language models be jailbroken using positive s...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.811 | Time: 2538ms

[1310/1822] Effective jailbreak attack methods for large language models...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.953 | Time: 4973ms

[1311/1822] benchmark for text-driven image editing evaluation with huma...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 3883ms

[1312/1822] multi-modality source-aware quality assessment metric for ev...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 2301ms

[1313/1822] How can Pointwise V-Information based fine-tuning improve th...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.623 | Time: 4493ms

[1314/1822] A comprehensive dataset and evaluation framework for trainin...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.673 | Time: 4144ms

[1315/1822] Scalable signature-based algorithm for path-dependent hedgin...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 3513ms

[1316/1822] Mathematical framework for deep hedging alternatives using u...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2940ms

[1317/1822] alternative memory architectures for AI inference that optim...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3114ms

[1318/1822] how does managed-retention memory improve energy efficiency ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.893 | Time: 3914ms

[1319/1822] How to improve short text classification using graph learnin...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 5005ms

[1320/1822] Novel methods for addressing semantic sparsity in short text...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3257ms

[1321/1822] How to use class-specific visual prompts to improve the inte...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 2756ms

[1322/1822] Improving saliency maps in Vision Transformers for fine-grai...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.422 | Time: 6036ms

[1323/1822] How can an autonomous multi-agent framework using dynamic ro...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.883 | Time: 2633ms

[1324/1822] Large language model based multi-agent systems for psycholog...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 3196ms

[1325/1822] iterative reinforced fine tuning using Monte Carlo Tree Sear...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.799 | Time: 3772ms

[1326/1822] addressing fragment deficiency and parameter errors in LLM t...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.599 | Time: 2978ms

[1327/1822] how to use large language models and graph embedding techniq...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4530ms

[1328/1822] fine-tuning pre-trained large language models with graph and...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3343ms

[1329/1822] architectural methods for overlapping communication and comp...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2506ms

[1330/1822] improving distributed transformer inference speed by decoupl...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3714ms

[1331/1822] how to improve large language model performance on complex c...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.525 | Time: 4089ms

[1332/1822] effective agentic frameworks for overcoming reasoning and lo...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 3105ms

[1333/1822] How do large language models perform in detecting smart cont...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 4029ms

[1334/1822] Reducing false positive rates in LLM-based smart contract se...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3736ms

[1335/1822] How to use model weight averaging during sequential fine-tun...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 2890ms

[1336/1822] Mitigating forgetting in diverse domain continual learning b...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2864ms

[1337/1822] How can influence functions be used to identify labeler bias...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 3891ms

[1338/1822] Efficient methods for measuring the impact of specific human...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3164ms

[1339/1822] comparing the performance of large language models and encod...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.663 | Time: 3879ms

[1340/1822] how well do large language models perform at segment-level q...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.705 | Time: 3621ms

[1341/1822] comprehensive evaluation framework for assessing large langu...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.526 | Time: 3694ms

[1342/1822] how to benchmark financial large language models on professi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 4351ms

[1343/1822] systematic mapping study on ethical risks and mitigation str...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3973ms

[1344/1822] current challenges and frameworks for mitigating ethical con...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2817ms

[1345/1822] Open source Python library for evaluating bias and fairness ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 4321ms

[1346/1822] How can I assess algorithmic bias in LLM responses using an ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3658ms

[1347/1822] How to use visual Hopfield networks and associative memory t...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3611ms

[1348/1822] Improving LLM-based medical report generation by mining dise...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.900 | Time: 2908ms

[1349/1822] Using zero-shot prompting with open-source large language mo...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2914ms

[1350/1822] Evaluation of large language models for scoring transcribed ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2521ms

[1351/1822] How to improve the inference efficiency of LLM-based recomme...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 3728ms

[1352/1822] Optimizing accuracy and latency in large language model reco...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.899 | Time: 3973ms

[1353/1822] How can explicit visual prompts like markers and pointers be...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.757 | Time: 2517ms

[1354/1822] Framework for integrating medical entity extraction and visu...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.517 | Time: 2818ms

[1355/1822] How can model predictive control frameworks improve the plan...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 3193ms

[1356/1822] Using large language models as implicit cost function minimi...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3634ms

[1357/1822] How does transfer learning work in variational quantum circu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.907 | Time: 4404ms

[1358/1822] Analytical fine-tuning methods for adapting pretrained varia...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 3062ms

[1359/1822] benchmarks for evaluating multimodal large language models o...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3720ms

[1360/1822] large scale dataset for cross-event and within-event reasoni...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5487ms

[1361/1822] How can image-based multimodal large language models be used...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 5992ms

[1362/1822] Techniques for improving the black-box transferability of ad...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 5966ms

[1363/1822] comprehensive review of methodologies for applying large lan...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3398ms

[1364/1822] what are the differences between graph2text and graph2token ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5687ms

[1365/1822] How can token-level uncertainty and attention mechanisms be ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 3432ms

[1366/1822] Lightweight decoding strategies for improving faithfulness t...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4332ms

[1367/1822] impact of explicit symmetry breaking and geometric reference...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 3833ms

[1368/1822] benchmarking trade-offs between group equivariance and symme...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3377ms

[1369/1822] How can the IDADP framework help large language models detec...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 4621ms

[1370/1822] Effective prompting strategies for zero-shot irony comprehen...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4048ms

[1371/1822] How do large language models compare to humans in open-ended...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 3857ms

[1372/1822] Analyzing the limitations of LLM exploration through sparse ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.821 | Time: 4409ms

[1373/1822] Systematization of knowledge on security vulnerabilities and...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 3974ms

[1374/1822] A comprehensive study on the risks of code hallucinations an...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 3778ms

[1375/1822] evaluating the fairness and robustness of commercial AI safe...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3825ms

[1376/1822] how do safety moderation classifiers used as LLM guardrails ...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.442 | Time: 3957ms

[1377/1822] how to improve multimodal language models for mental health ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2474ms

[1378/1822] multimodal framework for fine grained anxiety symptom detect...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 2963ms

[1379/1822] enhancing retrieval augmented generation for complex reasoni...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.789 | Time: 2571ms

[1380/1822] how to improve agentic RAG performance with self-consistency...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2610ms

[1381/1822] how can multimodal large language models use image represent...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.784 | Time: 4820ms

[1382/1822] using spatial intelligence of multimodal LLMs and visual gra...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2891ms

[1383/1822] how to use large language models and retrieval augmented gen...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.457 | Time: 2530ms

[1384/1822] enhancing automated short answer grading reliability with RA...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 4346ms

[1385/1822] How to improve membership inference attack accuracy in large...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2319ms

[1386/1822] Detecting pretraining data leakage in large language models ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3140ms

[1387/1822] How can lightweight models be used for evidence extraction t...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3834ms

[1388/1822] Comparison of quote-first-then-answer strategies versus full...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2999ms

[1389/1822] meta-learning for parameter initialization in variational qu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4556ms

[1390/1822] how to apply MAML-based classical neural networks for findin...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3103ms

[1391/1822] LLM-based framework for automated scientific research using ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3145ms

[1392/1822] How to automate the scientific research process through iter...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4753ms

[1393/1822] CharToM benchmark for evaluating theory of mind in large lan...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 2615ms

[1394/1822] Comparing human and AI theory of mind performance when reaso...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.482 | Time: 3194ms

[1395/1822] Theoretical analysis of diffusion models for nonparametric d...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3988ms

[1396/1822] How do sparse weight-sharing neural network architectures in...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.433 | Time: 5150ms

[1397/1822] Improving large language model reasoning performance through...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 5086ms

[1398/1822] How can recursive sub-task breakdown and advanced scoring me...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3543ms

[1399/1822] Implementation of Retrieval-Augmented Generation using BGE-M...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.807 | Time: 3083ms

[1400/1822] Evaluating localized RAG systems for data privacy and perfor...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.759 | Time: 3404ms

[1401/1822] iterative pruning methods for diffusion models using gradien...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.905 | Time: 3733ms

[1402/1822] how to apply progressive soft pruning and gradient flow crit...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2405ms

[1403/1822] large scale dataset and benchmark for fake news detection in...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 1498ms

[1404/1822] performance of large language models with quantized low rank...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 2302ms

[1405/1822] evaluating large language models on aerospace manufacturing ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 2637ms

[1406/1822] assessing the accuracy and hallucination risks of GPT-4 and ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 3045ms

[1407/1822] Improving multimodal hierarchical classification accuracy by...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3567ms

[1408/1822] How to integrate hierarchical class relationships into multi...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.793 | Time: 3947ms

[1409/1822] evaluating the performance of large language models for ment...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3997ms

[1410/1822] benchmarking multilingual and bilingual LLMs on Arabic menta...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.757 | Time: 4182ms

[1411/1822] How to use a proximal operator and local correlation regular...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2862ms

[1412/1822] Efficient methods for inducing structured 2:4 sparsity in ne...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2827ms

[1413/1822] How do task-in-prompt adversarial attacks use sequence-to-se...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.793 | Time: 4259ms

[1414/1822] Evaluating LLM jailbreak vulnerabilities using the PHRYGE be...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 3975ms

[1415/1822] consensus-based optimization methods for derivative-free non...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2808ms

[1416/1822] how to apply mirror maps and bregman distances to particle-b...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 2724ms

[1417/1822] How to use open-source large language models for automatic d...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3001ms

[1418/1822] Improving zero-shot classification performance of open-sourc...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 4162ms

[1419/1822] How can privacy guardrails like OneShield be used to detect ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4520ms

[1420/1822] Effective frameworks for mitigating privacy risks in enterpr...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 5289ms

[1421/1822] How can specialized datasets like PlanGTG with reordering an...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.703 | Time: 3012ms

[1422/1822] Improving graph to text generation in large language models ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 2803ms

[1423/1822] How to use Z-order curves and dimensionality reduction to en...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 2555ms

[1424/1822] Efficient top-k attention mechanisms for causal transformers...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.641 | Time: 4938ms

[1425/1822] How can semantic analysis and natural language processing be...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.947 | Time: 2993ms

[1426/1822] A computational architecture for linking systematic analytic...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3710ms

[1427/1822] Scaling in-context reinforcement learning for cross-domain a...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 5472ms

[1428/1822] How does algorithm distillation compare to expert distillati...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.615 | Time: 2400ms

[1429/1822] generating realistic limit order book simulations using tran...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 4845ms

[1430/1822] how to evaluate the realism of synthetic market data generat...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3094ms

[1431/1822] How to use embedding-based data perturbation and Tsetlin Mac...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 5090ms

[1432/1822] Improving adversarial attacks on text detectors by combining...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3609ms

[1433/1822] convergence rate of probability flow ODE samplers in diffusi...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.627 | Time: 4693ms

[1434/1822] how do probability flow ODEs in score-based generative model...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3929ms

[1435/1822] Automated black-box safety testing of large language models ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2707ms

[1436/1822] How can LLMs be used as test oracles to identify harmful res...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.412 | Time: 4179ms

[1437/1822] How can hierarchical contrastive learning and concept memori...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.842 | Time: 3325ms

[1438/1822] Improving the accuracy of generative language models in pred...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 2868ms

[1439/1822] information theoretic framework for multi-bit watermarking i...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.564 | Time: 2747ms

[1440/1822] optimal schemes for distributional information embedding to ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2431ms

[1441/1822] Can fine-tuned large language models effectively control spa...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 2927ms

[1442/1822] Comparison of fine-tuned foundation models and traditional d...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 2604ms

[1443/1822] hybrid machine learning and biophysical models for predictin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3581ms

[1444/1822] combining neural networks with physiological models to impro...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.900 | Time: 3783ms

[1445/1822] How to finetune large language models using a diffusion fram...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 3895ms

[1446/1822] Integrating diffusion processes into autoregressive models f...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2823ms

[1447/1822] datasets and benchmarks for fine grained span level chinese ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 3193ms

[1448/1822] evaluating the performance of large language models on ident...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 3599ms

[1449/1822] how to apply negative feedback mechanisms from control theor...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.571 | Time: 2562ms

[1450/1822] efficient weight-only quantization for large language models...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3631ms

[1451/1822] comprehensive survey of deep learning architectures and thei...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3520ms

[1452/1822] how neural networks like CNNs and autoencoders are used for ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4591ms

[1453/1822] machine learning based qubit readout workflow using QICK and...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4125ms

[1454/1822] how to implement hardware efficient neural networks for sing...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3778ms

[1455/1822] Causal pre-processing methods for resolving the fairness-acc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 3833ms

[1456/1822] How can approximating a fictitious and normatively desired w...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2816ms

[1457/1822] Improving process reward modeling for mathematical reasoning...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 3070ms

[1458/1822] How does hierarchical refinement and step merging affect the...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3108ms

[1459/1822] efficient methods for mitigating catastrophic forgetting in ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 2654ms

[1460/1822] how to balance domain adaptation and general knowledge prese...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3052ms

[1461/1822] Does professional artistic expertise improve the quality of ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 3761ms

[1462/1822] Experimental study on the transfer of traditional artistic s...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.874 | Time: 4145ms

[1463/1822] How can generative large language models be used for ranking...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2726ms

[1464/1822] Unified framework for multi-modal question answering that co...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3064ms

[1465/1822] evaluating large language models mathematical reasoning capa...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 4068ms

[1466/1822] how does randomizing variables in math word problems help de...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4155ms

[1467/1822] evaluation of large language models on their ability to tran...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 3236ms

[1468/1822] do current automatic evaluation metrics like BLEU and COMET ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.938 | Time: 5078ms

[1469/1822] Multi-agent deep reinforcement learning for target localizat...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.603 | Time: 5059ms

[1470/1822] Collaborative multi-agent system for radioactive source loca...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3053ms

[1471/1822] How do professional software developers perceive the readabi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 3877ms

[1472/1822] Investigating the impact of LLM-based software development a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4562ms

[1473/1822] multimodal masked autoencoder framework for denoising modula...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3597ms

[1474/1822] self-supervised pretraining methods for automatic modulation...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3838ms

[1475/1822] How can reinforcement learning and dynamic early exit strate...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5349ms

[1476/1822] Frameworks for optimizing the trade-off between energy consu...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3683ms

[1477/1822] How to perform domain adaptation of Llama 3.1 for e-commerce...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3481ms

[1478/1822] Techniques for adapting large language models to the e-comme...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4332ms

[1479/1822] analysis of the Gaussian distribution and statistical proper...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3432ms

[1480/1822] understanding why large foundation model weights follow Gaus...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4752ms

[1481/1822] How can prompt-based Monte Carlo Tree Search with dynamic ex...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.479 | Time: 2905ms

[1482/1822] Improving large model reasoning on SciEval datasets using ad...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.905 | Time: 4093ms

[1483/1822] supervised contrastive knowledge distillation for few-shot c...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 1714ms

[1484/1822] how to alleviate catastrophic forgetting in class-incrementa...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3655ms

[1485/1822] How can text-driven adaptation and modality alignment be use...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 4277ms

[1486/1822] Methodology for aligning image and text embeddings to perfor...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3292ms

[1487/1822] How to use weighted maximum likelihood estimation in RLHF to...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 3753ms

[1488/1822] Improving reinforcement learning from human feedback by addr...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2740ms

[1489/1822] How can multimodal AI agents in augmented reality proactivel...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.864 | Time: 5026ms

[1490/1822] Determining optimal intervention timing for proactive AI ass...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3789ms

[1491/1822] benchmarking multilingual gender neutral translation capabil...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2993ms

[1492/1822] systematic evaluation of inclusive translation and gender ne...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3571ms

[1493/1822] How do large language models perform on patient data extract...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.605 | Time: 3860ms

[1494/1822] Benchmarking Llama2 and Meditron for structured clinical dat...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 4335ms

[1495/1822] How to improve the ability of large language models to follo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2580ms

[1496/1822] Techniques for training large language models to satisfy mul...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3031ms

[1497/1822] How can weight recompute and computational graph rearrangeme...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 5192ms

[1498/1822] Efficient fine-tuning methods for sparse large language mode...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4853ms

[1499/1822] How can multimodal large language models be used for zero-sh...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 4530ms

[1500/1822] Improving the alignment of multimodal LLMs with human aesthe...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2484ms

[1501/1822] How to improve text to speech for research papers with compl...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.505 | Time: 4097ms

[1502/1822] Text-to-speech systems for visually impaired researchers to ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 4064ms

[1503/1822] How to detect machine-generated academic essays in English a...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 4441ms

[1504/1822] Advanced techniques for identifying AI-written academic pape...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3283ms

[1505/1822] How to improve the efficiency of tree search in large langua...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3216ms

[1506/1822] Reducing computational costs and redundancy in LLM multi-ste...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3796ms

[1507/1822] Applying two-fold curriculum learning and proximal policy op...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 3641ms

[1508/1822] How can curriculum learning and variational autoencoders be ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 3901ms

[1509/1822] How to use video-grounded entailment tree reasoning to impro...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3735ms

[1510/1822] Frameworks for integrating entailment tree construction and ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.933 | Time: 4108ms

[1511/1822] How can stochastic distribution embeddings and Wasserstein s...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2363ms

[1512/1822] Deep learning models for knowledge tracing that incorporate ...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3873ms

[1513/1822] How to evaluate social bias in large language models specifi...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2458ms

[1514/1822] Benchmarks and metrics for assessing fairness and social bia...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.581 | Time: 3946ms

[1515/1822] How can self-knowledge distillation and logit standardizatio...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.614 | Time: 2913ms

[1516/1822] State of the art generative dataset distillation methods foc...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 5301ms

[1517/1822] benchmarking large language models on linguistic reasoning t...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 2427ms

[1518/1822] evaluating the performance of LLMs on self-contained linguis...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 4095ms

[1519/1822] How to improve large language model chain of thought reasoni...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.484 | Time: 2721ms

[1520/1822] Techniques for handling implicit or missing information in L...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 3119ms

[1521/1822] How can hierarchical reinforcement learning and biometric fe...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 5075ms

[1522/1822] Integrating neurobiological data and multi-agent systems for...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 5688ms

[1523/1822] How to protect the copyright of hardware description languag...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 4067ms

[1524/1822] Methods for embedding robust watermarks into Verilog RTL and...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 2815ms

[1525/1822] using curiosity-driven reinforcement learning to automatical...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 3438ms

[1526/1822] automated generation of adversarial prompts to identify bias...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2262ms

[1527/1822] how to improve x-ray prohibited item detection performance w...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3599ms

[1528/1822] data augmentation methods for robust object detection in x-r...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2518ms

[1529/1822] search-based software engineering framework for automated to...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2265ms

[1530/1822] how can evolutionary algorithms and iterative prompt generat...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 3660ms

[1531/1822] How to use natural language processing on corporate 10-K fil...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2563ms

[1532/1822] A data-driven methodology for measuring firm-level AI engage...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2668ms

[1533/1822] benchmark dataset for evaluating the linguistic diversity an...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.943 | Time: 3994ms

[1534/1822] how do current open-vocabulary 3D visual grounding methods p...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.903 | Time: 5079ms

[1535/1822] dynamic context-aware positional encoding for improving long...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 2764ms

[1536/1822] improving transformer performance using equivariant position...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4924ms

[1537/1822] zero-shot multi-hop question answering over hybrid sources o...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 3824ms

[1538/1822] how to construct a unified hybrid graph from tabular and tex...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3973ms

[1539/1822] How to use hybrid static and dynamic fingerprinting techniqu...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2820ms

[1540/1822] Identifying large language models in multi-agent systems and...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3075ms

[1541/1822] Analyzing the risks of software supply chain attacks through...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 3773ms

[1542/1822] Relationship between HumanEval coding benchmarks and the pro...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.457 | Time: 3272ms

[1543/1822] How to design a flexible LLM re-ranker with configurable dep...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 2775ms

[1544/1822] Efficient large language model re-ranking techniques for pas...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3157ms

[1545/1822] How can projection-free algorithms be used to solve online c...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 3148ms

[1546/1822] projection-free online learning policies that utilize linear...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2657ms

[1547/1822] How to detect backdoors in deep neural networks by analyzing...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2753ms

[1548/1822] Trojan scanning methods for deep learning models that work a...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3245ms

[1549/1822] How do open-source contributions impact Large Language Model...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3974ms

[1550/1822] A comparative analysis of open-source versus proprietary Lar...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3033ms

[1551/1822] In-context reinforcement learning for few-shot budget alloca...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.611 | Time: 4901ms

[1552/1822] How to optimize budget allocation across stages in online ad...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 3643ms

[1553/1822] How to use GraphRAG and retrieve-divide-solve agent pipeline...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2602ms

[1554/1822] Knowledge graph based RAG framework for exploring protein-pr...
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 3163ms

[1555/1822] How to use unsupervised domain adaptation and graph-based kn...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 3031ms

[1556/1822] Improving cross-modal feature representation in text-to-imag...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 5189ms

[1557/1822] How to perform zero-shot verification of large language mode...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2488ms

[1558/1822] Methods for evaluating and guiding large language model reas...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 3741ms

[1559/1822] How to use diffusion models for efficient neural video compr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 2415ms

[1560/1822] Applying foundational diffusion models to video compression ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 5226ms

[1561/1822] How to use the Fisher information matrix for active learning...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 3868ms

[1562/1822] Active learning strategies for sequential tasks that balance...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 4299ms

[1563/1822] How can large language models be used to provide intelligent...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2789ms

[1564/1822] Improving IR-based bug localization through automated query ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3539ms

[1565/1822] how to improve multi-modal large language models for expert-...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2387ms

[1566/1822] integrating vision language models with physics simulators f...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3745ms

[1567/1822] How can large language models be used to automate the TinyML...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 3590ms

[1568/1822] Framework for leveraging large language models to streamline...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.901 | Time: 4319ms

[1569/1822] graph contrastive learning for short text classification wit...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 2677ms

[1570/1822] how to use multi-view text embeddings from graph learning to...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4836ms

[1571/1822] How can artificial intelligence be used to identify social b...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 2831ms

[1572/1822] Applications of AI for social inclusion including sign langu...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 3211ms

[1573/1822] evaluating the accuracy of large language models for transla...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 3582ms

[1574/1822] benchmarking llama and gpt-4o for the automated translation ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.753 | Time: 3896ms

[1575/1822] How to perform zero-shot cyber threat intelligence informati...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.615 | Time: 3580ms

[1576/1822] Scalable AI framework for extracting STIX compliant named en...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.768 | Time: 3885ms

[1577/1822] multi-parallel document-level translation corpus for African...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 3937ms

[1578/1822] evaluating the performance of large language models versus N...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 3749ms

[1579/1822] automated prompt engineering framework using Knowledge-Gradi...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.946 | Time: 4842ms

[1580/1822] How to apply sequential optimal learning and mixed-integer o...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 1975ms

[1581/1822] How do different floating-point quantization parameters like...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3256ms

[1582/1822] What is the optimal bit-width and exponent-mantissa ratio fo...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2671ms

[1583/1822] How can the concept of applied multiplexity be used to mitig...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 4960ms

[1584/1822] Using multi-agent systems to improve cultural inclusivity an...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 2465ms

[1585/1822] how to improve graph retrieval augmented generation by recon...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.815 | Time: 2794ms

[1586/1822] methods for mitigating information loss in knowledge graphs ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 3391ms

[1587/1822] automated evaluation metrics for retrieval augmented generat...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3456ms

[1588/1822] how to measure the conversational faithfulness and context r...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.511 | Time: 3877ms

[1589/1822] Large language model error patterns in mathematical word pro...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 4868ms

[1590/1822] Improving mathematical reasoning in LLMs through error-aware...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.962 | Time: 3875ms

[1591/1822] impact of real-world spelling mistakes and wikipedia edit hi...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4011ms

[1592/1822] comparing the robustness of mt5 and bloom models against hum...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3534ms

[1593/1822] automated network configuration translation between differen...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3708ms

[1594/1822] how can large language models be used to migrate legacy netw...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2555ms

[1595/1822] how to model fine-grained time-dependent user interests from...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2519ms

[1596/1822] using time-gap-aware attention and retrieval mechanisms to c...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3218ms

[1597/1822] How to use logit-based knowledge distillation to optimize de...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 2531ms

[1598/1822] Research on distillation frameworks for deep spiking neural ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.953 | Time: 2563ms

[1599/1822] framework for building domain-specific AI agents using natur...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 2654ms

[1600/1822] improving long-horizon planning in LLM agents by integrating...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.646 | Time: 1300ms

[1601/1822] Applying knowledge graph completion and relational graph att...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.703 | Time: 3869ms

[1602/1822] Knowledge graph based framework for modeling complex relatio...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2567ms

[1603/1822] Automated prompt optimization techniques that use task-aware...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3574ms

[1604/1822] How to implement task-referenced adaptation and multi-metric...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3927ms

[1605/1822] benefits of introducing positive friction in conversational ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2955ms

[1606/1822] how slowing down AI interactions and adding deliberate frict...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3058ms

[1607/1822] how to use llm-based line-level filtering to improve the qua...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2963ms

[1608/1822] improving training data quality through fine-grained line-le...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.890 | Time: 4050ms

[1609/1822] How to improve time series reasoning in multi-modal language...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3733ms

[1610/1822] Multi-modal language models for complex time series reasonin...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.844 | Time: 1903ms

[1611/1822] How can heterogeneous graph neural networks be applied to re...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2496ms

[1612/1822] Deep learning approaches for multimodal emotion prediction i...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4582ms

[1613/1822] How to improve Quranic question answering using cross-langua...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2922ms

[1614/1822] Cross-language strategies for addressing linguistic disparit...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3703ms

[1615/1822] How can sparse autoencoders be used to perform feature-level...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 2497ms

[1616/1822] Improving LLM response consistency for paraphrased inputs us...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 3908ms

[1617/1822] How to use Partial Information Decomposition principles to q...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2784ms

[1618/1822] Decomposing causal power using the Möbius function of the re...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.936 | Time: 5018ms

[1619/1822] evaluating the reliability of membership inference attacks o...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2261ms

[1620/1822] do membership inference attacks on LLMs actually detect trai...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4056ms

[1621/1822] How can retrieval-augmented generation and evidence-based me...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3685ms

[1622/1822] Advanced frameworks for medical LLMs that utilize evidence s...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3719ms

[1623/1822] How can self-learning agents using curriculum learning princ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.953 | Time: 3291ms

[1624/1822] Using large language model agents and iterative exploration ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 4116ms

[1625/1822] How do multimodal large language models perform when users p...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3899ms

[1626/1822] Benchmarking the vulnerability of vision-language models to ...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3741ms

[1627/1822] How does implementing a retrieval-augmented generation frame...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.672 | Time: 4042ms

[1628/1822] Evaluating the performance of retriever and generative model...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 2967ms

[1629/1822] multilingual end-to-end speech recognition using mixture of ...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.567 | Time: 5128ms

[1630/1822] how to improve LID-based routers in MoE architectures for mu...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2734ms

[1631/1822] How to prevent explicit content generation in text-to-image ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 4425ms

[1632/1822] Research on using embedding space distortion to defend again...
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.456 | Time: 5430ms

[1633/1822] research on federated multimodal instruction tuning framewor...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2545ms

[1634/1822] how to use mixture of adapters and adaptive parameter aggreg...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2841ms

[1635/1822] deep learning approaches for idiom detection in Sorani Kurdi...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 4485ms

[1636/1822] evaluating the performance of transformer models for identif...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.655 | Time: 3033ms

[1637/1822] lightweight deep learning models for energy efficient weed d...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3892ms

[1638/1822] how to achieve high accuracy weed detection on low power har...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 3696ms

[1639/1822] How can large multimodal language models be used to automati...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 3163ms

[1640/1822] Large scale dataset and specialized multimodal vision encode...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3555ms

[1641/1822] how to implement large language models for educational manag...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.634 | Time: 2368ms

[1642/1822] frameworks for applying fine-tuned large language models to ...
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.618 | Time: 3680ms

[1643/1822] How can metric learning with proxy anchor methods and tri-tr...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2784ms

[1644/1822] Using BLIP-2 pre-trained encoders and cross-modal transforme...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.703 | Time: 4077ms

[1645/1822] using low rank adaptation to finetune language models for be...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 2796ms

[1646/1822] improving the efficiency of sparse autoencoder reconstructio...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2053ms

[1647/1822] how to design fair pricing mechanisms for LLM training data ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2919ms

[1648/1822] economic frameworks for large language model data markets fo...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 4194ms

[1649/1822] Benchmarking AI agents on scientific discovery tasks through...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 3135ms

[1650/1822] How can autonomous agents be evaluated on their ability to d...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.623 | Time: 3660ms

[1651/1822] How to use large language models to automatically synthesize...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3705ms

[1652/1822] Integrating LLM-generated heuristics into search algorithms ...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 4423ms

[1653/1822] How does the length of tokenized Java code affect the accura...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 4223ms

[1654/1822] Evaluating the impact of input context window size on the pe...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.850 | Time: 3797ms

[1655/1822] how to use large language models and retrieval augmented gen...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2966ms

[1656/1822] automated g-code generation for cnc machines using self-corr...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3673ms

[1657/1822] How can Large Language Models be used for semantic consisten...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2762ms

[1658/1822] Semi-supervised sentiment classification using entity extrac...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3465ms

[1659/1822] Combining width and depth pruning strategies for efficient s...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 3822ms

[1660/1822] How to perform two-stage structured pruning on LLMs by remov...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.905 | Time: 4336ms

[1661/1822] how to improve large language model logical reasoning using ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3249ms

[1662/1822] enhancing llm problem solving through town hall style debate...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.854 | Time: 4069ms

[1663/1822] How can participatory design methods be used to create large...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.666 | Time: 3125ms

[1664/1822] Challenges and opportunities of developing a journalist-cont...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.878 | Time: 2979ms

[1665/1822] How can large language models be used as reference-aware cri...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 4113ms

[1666/1822] Benchmarking execution-free evaluation methods for code agen...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3572ms

[1667/1822] Performance comparison of YOLOv7 and Faster R-CNN for vision...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 3940ms

[1668/1822] How can deep learning models like YOLOv7 be applied to autom...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2619ms

[1669/1822] How can few-shot optimization and iterative prompt engineeri...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 3923ms

[1670/1822] Fine-tuning Mistral-7B-Instruct-v0.3 for robust hallucinatio...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3252ms

[1671/1822] using in-context learning with transformer models to detect ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3960ms

[1672/1822] how can large language model architectures and in-context le...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3049ms

[1673/1822] using large language models and retrieval augmented generati...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.907 | Time: 2212ms

[1674/1822] evaluating how interactive browser extensions and chat inter...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.845 | Time: 3218ms

[1675/1822] methods for 2-bit KV cache quantization in vision-language m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 3546ms

[1676/1822] how to optimize vision-language model memory consumption usi...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 4739ms

[1677/1822] Evaluating random forest classifiers for the detection of Na...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3395ms

[1678/1822] How can researchers develop accurate language identification...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.582 | Time: 2810ms

[1679/1822] How can analytical decomposition of first-layer attention we...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 3582ms

[1680/1822] Weight-based methods for analyzing how transformer models ma...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 4052ms

[1681/1822] Adaptive reward function exploration for action-level backdo...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.730 | Time: 3981ms

[1682/1822] How to perform stealthy backdoor attacks in continuous reinf...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4185ms

[1683/1822] How can fine-tuned large language models like LLaMA improve ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 4424ms

[1684/1822] Comparative evaluation of neural machine translation and fin...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2769ms

[1685/1822] Limitations and biases of automated factuality metrics in ev...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3817ms

[1686/1822] How reliable are automated factuality evaluators at measurin...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3710ms

[1687/1822] how to optimize kv cache storage in large language models us...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 4689ms

[1688/1822] adaptive kv cache pruning techniques that distinguish betwee...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 2607ms

[1689/1822] methods for converting dense large language models into mixt...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3046ms

[1690/1822] how to apply differentiable dynamic pruning to transform MLP...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 4283ms

[1691/1822] How can diversity-based adaptive random testing using string...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.900 | Time: 3756ms

[1692/1822] Effective test selection and prioritization strategies for L...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.905 | Time: 3157ms

[1693/1822] How to improve retrieval-augmented medical question answerin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.913 | Time: 3057ms

[1694/1822] Methods for improving the accuracy of medical QA systems usi...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.705 | Time: 2764ms

[1695/1822] A unified framework for general mobility trajectory modeling...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 5249ms

[1696/1822] How can masked conditional diffusion models with contextual ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 5088ms

[1697/1822] How can large language models be used for few-shot harmful c...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.827 | Time: 3826ms

[1698/1822] Evaluating the effectiveness of multimodal in-context learni...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.560 | Time: 4502ms

[1699/1822] How to improve perceptual consistency and visual sharpness i...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 2904ms

[1700/1822] Perception-inspired loss functions for neural edge detection...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 5073ms

[1701/1822] empirical study evaluating the performance of deep learning ...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4727ms

[1702/1822] challenges and performance degradation of deep learning appr...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 6133ms

[1703/1822] How can large language models be used to develop autonomous ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 6205ms

[1704/1822] Large language model based proactive dialogue systems for pr...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 3758ms

[1705/1822] relationship between Ehrenfeucht-Haussler rank of Boolean fu...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 3917ms

[1706/1822] how many Chain of Thought steps are required for a single-la...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 4722ms

[1707/1822] A comprehensive analysis of statistical methodology errors i...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 4820ms

[1708/1822] Investigating the prevalence of inadequate data analysis tec...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3483ms

[1709/1822] How can selective boosting of attention weights for local an...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3095ms

[1710/1822] Mitigating hallucinations in LVLMs by intervening in self-at...
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 4429ms

[1711/1822] new optimization algorithms for large language model pre-tra...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 2964ms

[1712/1822] improving Signum optimizer stability for GPT-2 training by i...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.928 | Time: 3708ms

[1713/1822] How to implement adaptive PII mitigation and policy-driven m...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3957ms

[1714/1822] Advanced NLP techniques for context-aware analysis and anony...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 2569ms

[1715/1822] How can node influence maximization and decoupled influence ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 2841ms

[1716/1822] A scalable graph unlearning framework that uses influence fu...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 4086ms

[1717/1822] understanding the sequential learning of skills in neural ne...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3916ms

[1718/1822] research on the domino effect in deep learning training and ...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3617ms

[1719/1822] How can large vision-language models be used to identify inc...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2539ms

[1720/1822] Using large vision-language models and reference images to v...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3404ms

[1721/1822] How to use retrieval-augmented generation and large language...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 2668ms

[1722/1822] Open source RAG-based systems for automatic extraction of pr...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 3083ms

[1723/1822] Benchmark datasets for Norwegian question answering evaluati...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3352ms

[1724/1822] Performance evaluation of language models on new Norwegian q...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2475ms

[1725/1822] How well does GPT-4o understand the ironic use of emojis com...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.789 | Time: 3786ms

[1726/1822] Comparative study of large language models and human percept...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2587ms

[1727/1822] improving direct preference optimization for large language ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2477ms

[1728/1822] alternative DPO methods that down-weight misranked samples a...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.655 | Time: 2900ms

[1729/1822] How does pyramid-descent visual position encoding enhance mu...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3896ms

[1730/1822] Improving vision-language model performance by using periphe...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.650 | Time: 4119ms

[1731/1822] Empirical study analyzing gender and ethnicity bias in Stabl...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 4927ms

[1732/1822] How do text-to-image generative models like Stable Diffusion...
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 4222ms

[1733/1822] How can 4-bit quantization reduce memory storage and speed u...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2562ms

[1734/1822] Using low-precision 4-bit integer quantization to optimize t...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.875 | Time: 3936ms

[1735/1822] Effective multi-stage training strategies for bilingual neur...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 3867ms

[1736/1822] Developing a lightweight bilingual Islamic LLM for document ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 3962ms

[1737/1822] What are the results of a thematic analysis regarding the ad...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 3653ms

[1738/1822] Qualitative research on the ethical integration of generativ...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.792 | Time: 4034ms

[1739/1822] efficient structured pruning techniques for large language m...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 4384ms

[1740/1822] how to prune large language models quickly using structured ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.715 | Time: 3474ms

[1741/1822] How can generative AI and quantum computing be integrated in...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.730 | Time: 3022ms

[1742/1822] Prototyping platform and dataset for 6G semantic communicati...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 3582ms

[1743/1822] how to improve document-level machine translation consistenc...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4807ms

[1744/1822] using doc-guided memory and agent-based approaches to ensure...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.710 | Time: 2728ms

[1745/1822] How does instruction tuning affect the fundamental task capa...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2778ms

[1746/1822] Correlation between instruction-tuned performance and base m...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.879 | Time: 3111ms

[1747/1822] multi-modal large language models for single-cell rna sequen...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.594 | Time: 3196ms

[1748/1822] ai models for single-cell analysis capable of cell type anno...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3563ms

[1749/1822] natural language interface for optimization models using lar...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.922 | Time: 3710ms

[1750/1822] how can large language models be used to provide counterfact...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 3614ms

[1751/1822] How to use Large Language Models as an action selection filt...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.821 | Time: 4656ms

[1752/1822] A hybrid approach combining large language model decision ma...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.900 | Time: 3636ms

[1753/1822] Integrating large language models into hierarchical planning...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.621 | Time: 3525ms

[1754/1822] Standardized benchmarks and datasets for evaluating the perf...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 3242ms

[1755/1822] How to improve the reasoning capabilities of mixture of expe...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2480ms

[1756/1822] Enhancing cognitive depth in language models by facilitating...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3603ms

[1757/1822] Norwegian abstractive summarization dataset for benchmarking...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 2560ms

[1758/1822] How to evaluate the performance of Norwegian large language ...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2678ms

[1759/1822] How to apply denoise diffusion models and stochastic differe...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.729 | Time: 2874ms

[1760/1822] Diffusion transformer based signal detection methods for red...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 1393ms

[1761/1822] efficient elastic quantization framework for deploying large...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2894ms

[1762/1822] how to improve memory elasticity and transition granularity ...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2743ms

[1763/1822] How does the magnitude of enriched categories of texts relat...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 4002ms

[1764/1822] Mathematical framework for computing magnitude homology and ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3159ms

[1765/1822] memory efficient on-FPGA training of transformer models usin...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.482 | Time: 3690ms

[1766/1822] hardware accelerator design for end-to-end transformer train...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.651 | Time: 3643ms

[1767/1822] How can large language models be used to generate synthetic ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.900 | Time: 4735ms

[1768/1822] Effective methods for reducing gender bias in pre-trained mo...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.828 | Time: 5271ms

[1769/1822] Orchestration framework for large language model training us...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.615 | Time: 2380ms

[1770/1822] Implementing joint mining mechanisms and bilateral value sha...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.665 | Time: 3492ms

[1771/1822] Attention-based deep learning framework for interpretable en...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2870ms

[1772/1822] How can Transformer architectures be used to predict multipl...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2327ms

[1773/1822] How to combine BERT sentence embeddings and TF-IDF features ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 2768ms

[1774/1822] Effective methods for Marathi plagiarism detection using a w...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3335ms

[1775/1822] L2 convergence rates and stability of linear Q-learning with...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 2321ms

[1776/1822] stochastic approximation analysis of linear Q-learning diver...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 3431ms

[1777/1822] How can large language models be used for reference-free and...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2271ms

[1778/1822] Automated metrics for counterspeech generation evaluation us...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.910 | Time: 3180ms

[1779/1822] How to use large language models for active knowledge retrie...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 4147ms

[1780/1822] LLM-enhanced knowledge augmentation methods that integrate e...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 5107ms

[1781/1822] How do cognitive forcing functions and different explanation...
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 3924ms

[1782/1822] Comparing the impact of visual explanations and cognitive fo...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 4070ms

[1783/1822] neurosymbolic knowledge base for environmental social and go...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 2219ms

[1784/1822] how to extract actionable sustainability information from co...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.615 | Time: 3827ms

[1785/1822] How can large language models be used as proxies to reduce t...
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.619 | Time: 2593ms

[1786/1822] Using LLM-based pipelines and DNF proper learning to acceler...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 2757ms

[1787/1822] improving best-of-n sampling in large language models using ...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 3584ms

[1788/1822] how to use pairwise comparison and chain of thought reasonin...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 3138ms

[1789/1822] How can masking high perplexity tokens in training data help...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 4399ms

[1790/1822] The effect of LLM-generated synthetic data on reducing perfo...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.875 | Time: 4058ms

[1791/1822] How to identify both latent driving forces and direct causal...
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.763 | Time: 2149ms

[1792/1822] Causal discovery and representation learning framework for u...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 4846ms

[1793/1822] How can debate and scalable oversight be used to improve wea...
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.820 | Time: 2661ms

[1794/1822] Can a weak model extract trustworthy information from a stro...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2353ms

[1795/1822] How can zero-knowledge proofs be used to verify the effectiv...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 4866ms

[1796/1822] Privacy-preserving verification techniques for low-rank adap...
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 5747ms

[1797/1822] How do binary decision biases and sampling methods in large ...
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.558 | Time: 4134ms

[1798/1822] Investigating randomness and decision-making biases in GPT m...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.479 | Time: 3706ms

[1799/1822] How to measure conditional feature importance using adversar...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 3257ms

[1800/1822] Explainable AI methods for estimating on-manifold conditiona...
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.915 | Time: 3716ms

[1801/1822] benchmarking vision language model safety using a dataset of...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.828 | Time: 4184ms

[1802/1822] multilingual evaluation of multimodal safety in vision langu...
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.442 | Time: 5396ms

[1803/1822] How to optimize large scale machine learning training pipeli...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2688ms

[1804/1822] Techniques for improving embedding table lookups and handlin...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 2411ms

[1805/1822] How does language-adaptive fine-tuning with the AfriBERTa mo...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 2852ms

[1806/1822] Evaluating the effectiveness of language-adaptive fine-tunin...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 3180ms

[1807/1822] Methods for adapting decoder-only large language models to p...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 4286ms

[1808/1822] How can combining bidirectional and causal attention in a ge...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3152ms

[1809/1822] Evaluating how large language models like Llama and Claude m...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4123ms

[1810/1822] Comparison of emotional intensity and semantic coherence bet...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 4481ms

[1811/1822] How do large language models exhibit systematic provider bia...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 5441ms

[1812/1822] Empirical study and dataset for evaluating service provider ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 3273ms

[1813/1822] benchmarking vision language models for error detection and ...
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.639 | Time: 2810ms

[1814/1822] how well do current vision language models perform at evalua...
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.815 | Time: 3236ms

[1815/1822] How do large language models perform on recalling notable gl...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.686 | Time: 2883ms

[1816/1822] Evaluating geographic disparities and socioeconomic correlat...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.677 | Time: 2769ms

[1817/1822] Automated pipeline for converting general Word documents int...
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 3549ms

[1818/1822] How to evaluate the quality of AI-generated presentations us...
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 4086ms

[1819/1822] trends in the use of causal inference methods and their impa...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 4449ms

[1820/1822] how does causal narrative complexity and novelty in economic...
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.972 | Time: 4675ms

[1821/1822] optimizing Mixture-of-Experts model inference on serverless ...
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.703 | Time: 2561ms

[1822/1822] distributed deployment of MoE models in serverless computing...
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 3835ms

HYBRID Summary:
  Avg MRR:      0.614
  Avg NDCG@5:   0.548
  Avg NDCG@10:  0.681
  Avg P@5:      0.424
  Avg P@10:     0.370
  Avg Time:     3578ms

============================================================
Evaluating mode: OPENAI
============================================================

[1/1822] enhancing large language model reasoning capabilities throug...
2026-02-19 18:32:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 2342ms

[2/1822] using reinforcement learning to develop emergent self reflec...
2026-02-19 18:32:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.828 | Time: 831ms

[3/1822] How to implement test-time scaling in large language models ...
2026-02-19 18:32:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 305ms

[4/1822] Improving language model reasoning performance on competitio...
2026-02-19 18:32:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.957 | Time: 298ms

[5/1822] Scaling reinforcement learning for large language models usi...
2026-02-19 18:32:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 249ms

[6/1822] How to use long chain of thought training techniques to impr...
2026-02-19 18:32:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.456 | Time: 322ms

[7/1822] open-source world foundation models for physical AI developm...
2026-02-19 18:32:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 329ms

[8/1822] how to use general-purpose world models for training physica...
2026-02-19 18:32:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 279ms

[9/1822] How can agentic search workflows and reason-in-documents mod...
2026-02-19 18:32:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.800 | Time: 217ms

[10/1822] Improving the trustworthiness of large reasoning models usin...
2026-02-19 18:32:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.777 | Time: 242ms

[11/1822] End-to-end native GUI agents that use vision-only screenshot...
2026-02-19 18:32:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 310ms

[12/1822] How does system-2 reasoning and iterative training with refl...
2026-02-19 18:32:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 235ms

[13/1822] challenging multi-modal benchmark for evaluating large langu...
2026-02-19 18:32:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 268ms

[14/1822] dataset for testing state of the art models on advanced acad...
2026-02-19 18:32:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 321ms

[15/1822] survey of collaboration mechanisms and coordination protocol...
2026-02-19 18:32:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 287ms

[16/1822] what are the key dimensions and strategies for organizing la...
2026-02-19 18:32:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.631 | Time: 214ms

[17/1822] how to effectively use llm-as-a-judge and consensus filterin...
2026-02-19 18:32:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 257ms

[18/1822] identifying and mitigating biases in best-of-n evaluation st...
2026-02-19 18:32:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 175ms

[19/1822] improving small language model math reasoning using monte ca...
2026-02-19 18:32:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.933 | Time: 214ms

[20/1822] how to scale mathematical reasoning in small models through ...
2026-02-19 18:32:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 244ms

[21/1822] autonomous LLM agent framework for end-to-end scientific dis...
2026-02-19 18:32:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 218ms

[22/1822] how to use large language model agents as research assistant...
2026-02-19 18:32:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 261ms

[23/1822] how to reduce the inference overhead and reasoning length of...
2026-02-19 18:32:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 299ms

[24/1822] RL-style fine-tuning for pruning reasoning steps and optimiz...
2026-02-19 18:32:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 206ms

[25/1822] comprehensive survey on reinforcement learning methods for t...
2026-02-19 18:32:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 351ms

[26/1822] the impact of test-time scaling and reinforced reasoning on ...
2026-02-19 18:32:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 746ms

[27/1822] survey of autonomous AI agents in retrieval augmented genera...
2026-02-19 18:32:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.626 | Time: 255ms

[28/1822] how do agentic design patterns like reflection and planning ...
2026-02-19 18:32:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.947 | Time: 340ms

[29/1822] How to address the optimization trade-off between reconstruc...
2026-02-19 18:32:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 275ms

[30/1822] Training efficient Diffusion Transformers using VA-VAE and v...
2026-02-19 18:32:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 357ms

[31/1822] How to address underthinking and frequent thought switching ...
2026-02-19 18:32:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 221ms

[32/1822] Improving the performance of reasoning models by using a dec...
2026-02-19 18:32:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[33/1822] How can multimodal large language models use visual reasonin...
2026-02-19 18:32:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 369ms

[34/1822] Improving spatial reasoning in multimodal models by generati...
2026-02-19 18:32:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 320ms

[35/1822] How to apply direct preference optimization to rectified flo...
2026-02-19 18:32:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 306ms

[36/1822] Multi-dimensional video reward models for aligning flow-base...
2026-02-19 18:32:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 292ms

[37/1822] benchmark for evaluating large multimodal models on expert k...
2026-02-19 18:32:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.571 | Time: 292ms

[38/1822] measuring knowledge gain in multimodal learning through perc...
2026-02-19 18:32:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 295ms

[39/1822] How does the Qwen2.5-1M model achieve a one million token co...
2026-02-19 18:32:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 263ms

[40/1822] Inference optimization methods for one million token context...
2026-02-19 18:32:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 282ms

[41/1822] benchmarking different large language model steering methods...
2026-02-19 18:32:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 295ms

[42/1822] performance of sparse autoencoders versus rank-1 representat...
2026-02-19 18:32:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.412 | Time: 230ms

[43/1822] inference-time steering of diffusion models using Feynman-Ka...
2026-02-19 18:32:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 257ms

[44/1822] how to control diffusion model generation with reward functi...
2026-02-19 18:32:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 174ms

[45/1822] How can large multimodal models maintain performance while c...
2026-02-19 18:32:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.935 | Time: 306ms

[46/1822] Efficient large multimodal model architecture that uses moda...
2026-02-19 18:32:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.897 | Time: 221ms

[47/1822] Applying chain of thought reasoning and step by step verific...
2026-02-19 18:32:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 225ms

[48/1822] Using the potential assessment reward model and direct prefe...
2026-02-19 18:32:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 272ms

[49/1822] How does the lightning attention mechanism combined with mix...
2026-02-19 18:32:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 314ms

[50/1822] Scaling large vision-language and text models using efficien...
2026-02-19 18:32:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 247ms

[51/1822] benchmarking expert-level medical reasoning and multimodal u...
2026-02-19 18:32:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 297ms

[52/1822] high difficulty medical question answering dataset with clin...
2026-02-19 18:32:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 236ms

[53/1822] How to protect large language models from universal jailbrea...
2026-02-19 18:32:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 340ms

[54/1822] Evaluating the effectiveness of constitutional classifiers a...
2026-02-19 18:32:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 295ms

[55/1822] evaluating large language models on their ability to navigat...
2026-02-19 18:32:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 287ms

[56/1822] a multi-agent framework using an explore-critic paradigm to ...
2026-02-19 18:32:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 241ms

[57/1822] How to use 3D tracking videos as control signals in diffusio...
2026-02-19 18:32:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 279ms

[58/1822] A unified video diffusion framework that leverages 3D contro...
2026-02-19 18:32:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 239ms

[59/1822] How to implement System 2 reasoning in large language models...
2026-02-19 18:32:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 266ms

[60/1822] Training language models to perform meta-reasoning about the...
2026-02-19 18:32:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 221ms

[61/1822] hierarchical multi-agent framework for mobile task automatio...
2026-02-19 18:32:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 243ms

[62/1822] how can mobile agents learn from past experiences using tips...
2026-02-19 18:32:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 252ms

[63/1822] benchmark for evaluating multi-turn conversation capabilitie...
2026-02-19 18:32:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 269ms

[64/1822] realistic multi-turn evaluation for large language models us...
2026-02-19 18:32:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.854 | Time: 248ms

[65/1822] transformer-based diffusion models for closed-loop autonomou...
2026-02-19 18:32:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 331ms

[66/1822] achieving human-like driving behaviors and joint prediction-...
2026-02-19 18:32:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 207ms

[67/1822] benchmarking large language model performance on competitive...
2026-02-19 18:32:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 290ms

[68/1822] how to measure the reasoning and coding abilities of LLMs th...
2026-02-19 18:32:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 280ms

[69/1822] How does graph-based retrieval-augmented generation solve th...
2026-02-19 18:32:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 329ms

[70/1822] Systematic review of GraphRAG technical foundations includin...
2026-02-19 18:32:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 265ms

[71/1822] benchmarks for evaluating expert level reasoning and domain ...
2026-02-19 18:32:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 246ms

[72/1822] multimodal foundation model evaluation datasets with human e...
2026-02-19 18:32:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 451ms

[73/1822] how to use process reward models with speculative decoding t...
2026-02-19 18:32:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 329ms

[74/1822] efficient large language model inference using a draft model...
2026-02-19 18:32:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 238ms

[75/1822] benchmarking vision language models on physical world unders...
2026-02-19 18:32:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 451ms

[76/1822] how to improve physical reasoning in vision language models ...
2026-02-19 18:32:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 233ms

[77/1822] Technical details of omni-modal models using multi-stage tra...
2026-02-19 18:32:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 305ms

[78/1822] How does the Baichuan-Audio-Tokenizer capture semantic and a...
2026-02-19 18:32:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 297ms

[79/1822] How to implement a temporally-aware knowledge graph system f...
2026-02-19 18:32:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 317ms

[80/1822] Benchmarking Zep against MemGPT using the Deep Memory Retrie...
2026-02-19 18:32:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 267ms

[81/1822] human annotated datasets for large language model safety ali...
2026-02-19 18:32:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 378ms

[82/1822] how to train lightweight LLM safety guardrails using a hybri...
2026-02-19 18:32:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 288ms

[83/1822] Multimodal large language models for real-time full-duplex v...
2026-02-19 18:32:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 255ms

[84/1822] How can multimodal LLMs achieve seamless two-way voice commu...
2026-02-19 18:32:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.964 | Time: 236ms

[85/1822] How to synthesize agent interaction data using backward cons...
2026-02-19 18:32:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 314ms

[86/1822] Data-centric framework for adapting autonomous agents to dig...
2026-02-19 18:32:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 246ms

[87/1822] how can reinforcement learning and oversampling for increase...
2026-02-19 18:32:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 378ms

[88/1822] strategies for enabling inference scaling in large language ...
2026-02-19 18:32:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 294ms

[89/1822] How can multiagent systems and independent model specializat...
2026-02-19 18:32:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 248ms

[90/1822] Finetuning language models on synthetic data from multiagent...
2026-02-19 18:32:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 411ms

[91/1822] preference optimization algorithm for Thinking-LLM-as-a-Judg...
2026-02-19 18:32:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 335ms

[92/1822] how does separating evaluation planning from execution in ch...
2026-02-19 18:32:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 274ms

[93/1822] How can reasoning-based supervised fine-tuning and hard samp...
2026-02-19 18:32:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 238ms

[94/1822] A large-scale dataset with detailed reasoning steps for trai...
2026-02-19 18:32:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 433ms

[95/1822] Can large language models articulate their own learned behav...
2026-02-19 18:32:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 309ms

[96/1822] Research on behavioral self-awareness in LLMs and whether mo...
2026-02-19 18:32:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 228ms

[97/1822] How are large language models being used across the differen...
2026-02-19 18:32:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 334ms

[98/1822] A comprehensive review of the roles and methodologies of lar...
2026-02-19 18:32:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 297ms

[99/1822] How to use open-source large language models for efficient G...
2026-02-19 18:32:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1370ms

[100/1822] State of the art open-source LLM framework for SWE-bench Lit...
2026-02-19 18:32:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: -621ms

[101/1822] reproducing o1 style slow-thinking in multimodal models by f...
2026-02-19 18:32:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 263ms

[102/1822] research on transferring long-form reasoning capabilities fr...
2026-02-19 18:32:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 241ms

[103/1822] How does pre-training data influence the emergence of Chain ...
2026-02-19 18:32:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 219ms

[104/1822] Survey of large language model architectures and scaling mec...
2026-02-19 18:32:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 266ms

[105/1822] How does increasing inference-time compute in reasoning mode...
2026-02-19 18:32:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 249ms

[106/1822] Investigating the impact of test-time compute scaling on the...
2026-02-19 18:32:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 278ms

[107/1822] How to improve multimodal GUI agents using a two-stage fine-...
2026-02-19 18:32:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 263ms

[108/1822] Developing generalist GUI agents with hierarchical reasoning...
2026-02-19 18:32:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 228ms

[109/1822] open source multi-modal reward models for aligning large vis...
2026-02-19 18:32:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 298ms

[110/1822] using reward models for reinforcement learning and test-time...
2026-02-19 18:32:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.954 | Time: 342ms

[111/1822] Recent advancements in large vision-language models for deta...
2026-02-19 18:32:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 281ms

[112/1822] Large multi-modal models that outperform GPT-4o and Gemini 1...
2026-02-19 18:32:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 244ms

[113/1822] How can Monte Carlo Tree Search be integrated with large lan...
2026-02-19 18:32:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 304ms

[114/1822] Improving the exploration of LLM-based heuristic generation ...
2026-02-19 18:32:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 218ms

[115/1822] statistical methods for justifying the replacement of human ...
2026-02-19 18:32:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 246ms

[116/1822] how to evaluate if an LLM is a reliable substitute for human...
2026-02-19 18:32:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 374ms

[117/1822] How can large language model agents be designed to support l...
2026-02-19 18:32:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 346ms

[118/1822] A comprehensive survey of perception, memory, and action mod...
2026-02-19 18:32:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 238ms

[119/1822] Multimodal foundation models for computational pathology pre...
2026-02-19 18:32:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.440 | Time: 278ms

[120/1822] How does incorporating transcriptomic data into the pre-trai...
2026-02-19 18:32:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 272ms

[121/1822] training-free test-time alignment of diffusion models using ...
2026-02-19 18:32:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.945 | Time: 304ms

[122/1822] how to optimize diffusion models for multiple reward objecti...
2026-02-19 18:32:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 247ms

[123/1822] Inference-time alignment techniques for diffusion models usi...
2026-02-19 18:32:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.845 | Time: 216ms

[124/1822] Guide to reward-guided generation and inference-time guidanc...
2026-02-19 18:32:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 228ms

[125/1822] How to use multimodal large language models to convert chart...
2026-02-19 18:32:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 357ms

[126/1822] Multimodal LLM trained on Chart2Code-160k dataset for genera...
2026-02-19 18:32:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 211ms

[127/1822] how to use dynamic trend representation transformers and cro...
2026-02-19 18:32:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 327ms

[128/1822] fusing dynamic trends and static graph attributes for traffi...
2026-02-19 18:32:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 244ms

[129/1822] How can learnable orthogonal and scaling transformations imp...
2026-02-19 18:32:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 301ms

[130/1822] Post-training quantization methods using Quantization Space ...
2026-02-19 18:32:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.943 | Time: 339ms

[131/1822] Scalable parallel transformer architecture for video diffusi...
2026-02-19 18:32:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 279ms

[132/1822] Memory-efficient training frameworks using hybrid parallelis...
2026-02-19 18:32:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 268ms

[133/1822] How can evolutionary search strategies be used to scale infe...
2026-02-19 18:32:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 254ms

[134/1822] Using language models to generate and recombine candidate re...
2026-02-19 18:32:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 222ms

[135/1822] How to improve low-level spatial understanding in vision-lan...
2026-02-19 18:32:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 226ms

[136/1822] Recent advancements in training VLA models that integrate se...
2026-02-19 18:32:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 232ms

[137/1822] How can self-play between a conjecturer and a prover LLM imp...
2026-02-19 18:32:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 228ms

[138/1822] Training large language models for formal mathematical verif...
2026-02-19 18:32:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 194ms

[139/1822] self-supervised music representation learning models that ut...
2026-02-19 18:32:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 229ms

[140/1822] how to train a joint music-text embedding model using contra...
2026-02-19 18:32:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 210ms

[141/1822] How to improve large language model reasoning through test-t...
2026-02-19 18:32:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.864 | Time: 220ms

[142/1822] Efficient test-time computation methods for LLMs that levera...
2026-02-19 18:32:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 239ms

[143/1822] How does scaling long chain of thought data to one million s...
2026-02-19 18:32:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 218ms

[144/1822] Investigating the impact of long-CoT dataset scaling and rei...
2026-02-19 18:32:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[145/1822] How can chain-of-thought reasoning and spatial coordinate al...
2026-02-19 18:32:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 185ms

[146/1822] Advanced spatial reasoning in vision-language models using b...
2026-02-19 18:32:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 268ms

[147/1822] How to improve language model reasoning by training models t...
2026-02-19 18:32:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 251ms

[148/1822] Comparison between critique fine-tuning and imitation learni...
2026-02-19 18:32:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 245ms

[149/1822] What are the primary challenges and open problems in using m...
2026-02-19 18:32:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 243ms

[150/1822] Investigating the limitations of machine unlearning for cybe...
2026-02-19 18:32:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 238ms

[151/1822] How can multimodal large language models use hidden latent s...
2026-02-19 18:32:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 171ms

[152/1822] Compressing textual reasoning chains into compact thinking t...
2026-02-19 18:32:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 194ms

[153/1822] best practices and data-centric strategies for post-training...
2026-02-19 18:32:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 207ms

[154/1822] detailed implementation of post-training data strategies and...
2026-02-19 18:32:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 326ms

[155/1822] benchmarks for evaluating temporal awareness and real-time r...
2026-02-19 18:32:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 229ms

[156/1822] how to evaluate the ability of video LLMs to handle incremen...
2026-02-19 18:32:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.957 | Time: 288ms

[157/1822] How can large language model agents be trained with reinforc...
2026-02-19 18:32:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 228ms

[158/1822] LLM-based autonomous agents for comprehensive literature ret...
2026-02-19 18:32:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[159/1822] Are chain of thought explanations in reasoning models like D...
2026-02-19 18:32:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 172ms

[160/1822] Measuring if large language models can accurately describe h...
2026-02-19 18:32:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 376ms

[161/1822] benchmarking negation understanding in vision language model...
2026-02-19 18:32:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 236ms

[162/1822] improving CLIP model performance on negated text queries thr...
2026-02-19 18:32:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 321ms

[163/1822] multimodal dataset for building damage assessment combining ...
2026-02-19 18:32:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 228ms

[164/1822] globally distributed dataset for training AI models in build...
2026-02-19 18:32:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 238ms

[165/1822] comprehensive survey of parameter-efficient fine-tuning tech...
2026-02-19 18:32:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 278ms

[166/1822] recent developments and systematic review of PEFT methods ac...
2026-02-19 18:32:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 246ms

[167/1822] evaluation framework for assessing the functional correctnes...
2026-02-19 18:32:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 310ms

[168/1822] multilingual benchmarks for measuring whether AI-generated c...
2026-02-19 18:32:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 286ms

[169/1822] How do large language models improve cold-start recommendati...
2026-02-19 18:32:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 213ms

[170/1822] Survey of recent advances and future research directions in ...
2026-02-19 18:32:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 238ms

[171/1822] How to use Monte Carlo Tree Search and iterative self-traini...
2026-02-19 18:32:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 226ms

[172/1822] Training language model agents to perform self-reflection an...
2026-02-19 18:32:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 236ms

[173/1822] How to maintain safety alignment and prevent jailbreaking ri...
2026-02-19 18:32:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.942 | Time: 259ms

[174/1822] Parameter efficient fine-tuning methods that preserve safety...
2026-02-19 18:32:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 288ms

[175/1822] Investigating the stability of features extracted by sparse ...
2026-02-19 18:32:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.951 | Time: 261ms

[176/1822] Do TopK sparse autoencoders identify consistent features acr...
2026-02-19 18:32:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 294ms

[177/1822] Scaling vision language model pretraining with massive high ...
2026-02-19 18:33:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 267ms

[178/1822] How does progressively scaling SFT data quantity and complex...
2026-02-19 18:33:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 315ms

[179/1822] comprehensive survey of large language models used for autom...
2026-02-19 18:33:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.884 | Time: 204ms

[180/1822] recent advancements and challenges in using large language m...
2026-02-19 18:33:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 222ms

[181/1822] Interpretable machine unlearning in diffusion models using s...
2026-02-19 18:33:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 295ms

[182/1822] Applying sparse autoencoders to diffusion model activations ...
2026-02-19 18:33:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 232ms

[183/1822] interactive benchmark for evaluating multimodal large langua...
2026-02-19 18:33:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 250ms

[184/1822] How do state of the art multimodal large language models per...
2026-02-19 18:33:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 238ms

[185/1822] comprehensive architectural framework and modular blueprint ...
2026-02-19 18:33:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 201ms

[186/1822] how to implement reasoning language models using process-bas...
2026-02-19 18:33:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 381ms

[187/1822] training-free methods for consistent character identity in t...
2026-02-19 18:33:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 220ms

[188/1822] how to achieve consistent identity in diffusion models using...
2026-02-19 18:33:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 232ms

[189/1822] Recent survey on the performance and training techniques of ...
2026-02-19 18:33:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.854 | Time: 223ms

[190/1822] Comparing the efficiency and scalability of task-specific sm...
2026-02-19 18:33:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[191/1822] How to align large language model outputs with human prefere...
2026-02-19 18:33:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 202ms

[192/1822] Techniques for test-time preference optimization that use na...
2026-02-19 18:33:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 215ms

[193/1822] large scale multimodal benchmark for evaluating cultural bia...
2026-02-19 18:33:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 223ms

[194/1822] fine-tuning vision language models on the CultureVerse datas...
2026-02-19 18:33:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 239ms

[195/1822] speculative decoding techniques that use a judge model to ac...
2026-02-19 18:33:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 241ms

[196/1822] how to increase the inference speed of large language models...
2026-02-19 18:33:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[197/1822] How can tensor product decomposition be used to reduce KV ca...
2026-02-19 18:33:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.749 | Time: 291ms

[198/1822] Efficient attention mechanisms using contextual low-rank rep...
2026-02-19 18:33:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.904 | Time: 212ms

[199/1822] Comparing the impact of conversational XAI interfaces versus...
2026-02-19 18:33:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1275ms

[200/1822] How does integrating large language model agents into conver...
2026-02-19 18:33:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 339ms

[201/1822] How are large language models being applied to automate and ...
2026-02-19 18:33:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 230ms

[202/1822] A review of current research on using large language models ...
2026-02-19 18:33:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[203/1822] rehearsal-free class incremental learning using decoupled lo...
2026-02-19 18:33:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 212ms

[204/1822] how to achieve a stable stability-plasticity trade-off in co...
2026-02-19 18:33:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 199ms

[205/1822] How to train large language models using FP4 quantization wh...
2026-02-19 18:33:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 239ms

[206/1822] Techniques for ultra-low precision training of LLMs using di...
2026-02-19 18:33:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 235ms

[207/1822] How can instruction tuning with environment-based self-refin...
2026-02-19 18:33:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 241ms

[208/1822] Training LLM agents to correct their own mistakes using envi...
2026-02-19 18:33:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 263ms

[209/1822] How does inference-time scaling and extended reasoning chain...
2026-02-19 18:33:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 300ms

[210/1822] Applying journey learning and systematic clinical reasoning ...
2026-02-19 18:33:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 439ms

[211/1822] comprehensive benchmark for evaluating large language model ...
2026-02-19 18:33:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 196ms

[212/1822] how to assess tool-augmented large language models using cat...
2026-02-19 18:33:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 243ms

[213/1822] how can we align time series data with linguistic logic and ...
2026-02-19 18:33:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.933 | Time: 234ms

[214/1822] using dual scale context alignment graph neural networks to ...
2026-02-19 18:33:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 256ms

[215/1822] benchmarking large language models for factual grounding and...
2026-02-19 18:33:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 220ms

[216/1822] automated evaluation methods and leaderboards for testing wh...
2026-02-19 18:33:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 209ms

[217/1822] how to implement retrieval augmented generation over a large...
2026-02-19 18:33:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 205ms

[218/1822] framework for dynamic video retrieval and informative frame ...
2026-02-19 18:33:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 247ms

[219/1822] benchmarking multi-turn retrieval augmented generation syste...
2026-02-19 18:33:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 171ms

[220/1822] how do state-of-the-art RAG systems handle multi-turn conver...
2026-02-19 18:33:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 297ms

[221/1822] How can process reward models be used to improve multimodal ...
2026-02-19 18:33:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.389 | Time: 275ms

[222/1822] Training multimodal large language models for mathematical r...
2026-02-19 18:33:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 234ms

[223/1822] How do large language models use sparse autoencoders to repr...
2026-02-19 18:33:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 270ms

[224/1822] Investigation into whether multilingual LLMs encode universa...
2026-02-19 18:33:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 228ms

[225/1822] methods for scaling serial and parallel test-time compute to...
2026-02-19 18:33:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: -616ms

[226/1822] using model-generated test voting and multi-turn selection t...
2026-02-19 18:33:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 251ms

[227/1822] Is the performance drop in continual learning of large langu...
2026-02-19 18:33:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 219ms

[228/1822] How does freezing bottom layers of large language models hel...
2026-02-19 18:33:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 207ms

[229/1822] comparing the safety and alignment levels of DeepSeek-R1 and...
2026-02-19 18:33:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.904 | Time: 254ms

[230/1822] systematic evaluation of DeepSeek-R1 and o3-mini reasoning m...
2026-02-19 18:33:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 231ms

[231/1822] techniques for maintaining response diversity in large langu...
2026-02-19 18:33:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 258ms

[232/1822] how to optimize language models to generate more diverse per...
2026-02-19 18:33:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.914 | Time: 229ms

[233/1822] how to bridge the multilingual performance gap in mathematic...
2026-02-19 18:33:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 311ms

[234/1822] improving Korean language math reasoning capabilities in LLM...
2026-02-19 18:33:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 183ms

[235/1822] How does scaling the input vocabulary size using multi-gram ...
2026-02-19 18:33:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 229ms

[236/1822] Investigating the impact of decoupling input and output voca...
2026-02-19 18:33:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 195ms

[237/1822] How can hierarchical memory systems be used to enable traini...
2026-02-19 18:33:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 235ms

[238/1822] Frameworks for real-time video reasoning that use parallel s...
2026-02-19 18:33:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 237ms

[239/1822] how does the inconsistency between comprehension and safety ...
2026-02-19 18:33:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 254ms

[240/1822] using shuffle inconsistency and query-based black-box optimi...
2026-02-19 18:33:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 235ms

[241/1822] How does the accuracy of frequent ChatGPT users compare to a...
2026-02-19 18:33:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 328ms

[242/1822] Can experienced LLM users identify AI-written articles that ...
2026-02-19 18:33:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 229ms

[243/1822] Theoretical analysis of gradient descent dynamics and loss t...
2026-02-19 18:33:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 295ms

[244/1822] How does the parametrization of key and query weights affect...
2026-02-19 18:33:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.604 | Time: 256ms

[245/1822] How does the level of sparsity in mixture-of-experts models ...
2026-02-19 18:33:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 219ms

[246/1822] Investigating the optimal balance between computational effi...
2026-02-19 18:33:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 224ms

[247/1822] Evaluating large language models on multi-step and constrain...
2026-02-19 18:33:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 193ms

[248/1822] How to measure the performance of LLMs in complex tool use s...
2026-02-19 18:33:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 287ms

[249/1822] How can large language model-based generative agents be used...
2026-02-19 18:33:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 287ms

[250/1822] Using LLM-powered learner simulators with memory and reflect...
2026-02-19 18:33:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 236ms

[251/1822] How does global batch load balancing loss improve expert spe...
2026-02-19 18:33:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 285ms

[252/1822] Training Mixture-of-Experts models with global frequency syn...
2026-02-19 18:33:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 240ms

[253/1822] Best practices and design considerations for conducting exte...
2026-02-19 18:33:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 293ms

[254/1822] How can AI developers implement external red teaming framewo...
2026-02-19 18:33:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 275ms

[255/1822] Unified generative model for speech and singing voice enhanc...
2026-02-19 18:33:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 244ms

[256/1822] How can masked generative models be used for zero-shot voice...
2026-02-19 18:33:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 203ms

[257/1822] Scalable deep graph neural networks for crystal property pre...
2026-02-19 18:33:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 285ms

[258/1822] Advanced graph neural network models for molecules and mater...
2026-02-19 18:33:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 220ms

[259/1822] How to map hidden knowledge of neural networks into the mult...
2026-02-19 18:33:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 344ms

[260/1822] Scalable method for validating AI models and identifying neu...
2026-02-19 18:33:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 204ms

[261/1822] Methods for training retrieval augmented generation models t...
2026-02-19 18:33:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.746 | Time: 305ms

[262/1822] Improving RAG performance through test-time compute scaling ...
2026-02-19 18:33:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.732 | Time: 235ms

[263/1822] comprehensive review of the methods and algorithms used to e...
2026-02-19 18:33:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 263ms

[264/1822] how are multi-turn interactions in large language models eva...
2026-02-19 18:33:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 238ms

[265/1822] How to use tree search algorithms like MCTS to mitigate hall...
2026-02-19 18:33:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 268ms

[266/1822] Applying dual process theory and slow thinking generation wi...
2026-02-19 18:33:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 219ms

[267/1822] collaborative framework for large language models and small ...
2026-02-19 18:33:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 211ms

[268/1822] how to integrate cloud-based LLMs with on-device small recom...
2026-02-19 18:33:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 256ms

[269/1822] Best practices for optimizing Retrieval-Augmented Generation...
2026-02-19 18:33:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 262ms

[270/1822] How do factors like document chunk size, retrieval stride, a...
2026-02-19 18:33:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.793 | Time: 238ms

[271/1822] How can self-updating libraries and task decomposition impro...
2026-02-19 18:33:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 205ms

[272/1822] Frameworks for large language models that use dynamic memory...
2026-02-19 18:33:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 256ms

[273/1822] benchmarking text to image diffusion models for toxicity fai...
2026-02-19 18:33:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 277ms

[274/1822] comprehensive safety assessment framework for evaluating bia...
2026-02-19 18:33:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 247ms

[275/1822] Do different large language models produce similar creative ...
2026-02-19 18:33:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 294ms

[276/1822] Study comparing the population-level diversity and homogenei...
2026-02-19 18:33:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 236ms

[277/1822] how effective are LLM-based software repair agents at fixing...
2026-02-19 18:33:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 311ms

[278/1822] comparison of agentic program repair performance between ope...
2026-02-19 18:33:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 210ms

[279/1822] benchmark for evaluating large language model hallucinations...
2026-02-19 18:33:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 235ms

[280/1822] new taxonomy for large language model hallucinations disting...
2026-02-19 18:33:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 237ms

[281/1822] how to improve automated feature interpretability in large l...
2026-02-19 18:33:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 223ms

[282/1822] natural language descriptions for llm features that capture ...
2026-02-19 18:33:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 276ms

[283/1822] How to improve multi-step reasoning in RAG systems using pro...
2026-02-19 18:33:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[284/1822] Addressing early-step bias in process reward models through ...
2026-02-19 18:33:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 261ms

[285/1822] How do advanced second-order optimizers like Self-Scaled BFG...
2026-02-19 18:33:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 483ms

[286/1822] Performance of self-scaled quasi-Newton methods like SSBFGS ...
2026-02-19 18:33:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[287/1822] How to use adaptive projective gradient descent and shared s...
2026-02-19 18:33:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 279ms

[288/1822] Constrained optimization methods for multi-task model mergin...
2026-02-19 18:33:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 248ms

[289/1822] How does displaying AI confidence levels influence human sel...
2026-02-19 18:33:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 220ms

[290/1822] Research on the alignment between artificial intelligence co...
2026-02-19 18:33:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 260ms

[291/1822] Performance comparison between large language models and tra...
2026-02-19 18:33:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 210ms

[292/1822] Analyzing the trade-off between F1-score and inference time ...
2026-02-19 18:33:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.798 | Time: 223ms

[293/1822] comprehensive review of techniques for distinguishing betwee...
2026-02-19 18:33:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 219ms

[294/1822] comparison of probabilistic methods and ensemble learning fo...
2026-02-19 18:33:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 263ms

[295/1822] discrete diffusion models for multi-task drug discovery usin...
2026-02-19 18:33:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.952 | Time: 276ms

[296/1822] how to use non-autoregressive bidirectional parallel decodin...
2026-02-19 18:33:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 232ms

[297/1822] semi-supervised split learning frameworks for addressing int...
2026-02-19 18:33:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 360ms

[298/1822] how to implement split learning for resource-constrained LEO...
2026-02-19 18:33:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[299/1822] How to adapt ConvNeXt architectures for facial emotion recog...
2026-02-19 18:33:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 260ms

[300/1822] Deep learning frameworks for facial expression recognition t...
2026-02-19 18:33:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 198ms

[301/1822] Reducing hallucinations in legal question answering systems ...
2026-02-19 18:33:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 1274ms

[302/1822] Benchmarking and evaluating large language model factuality ...
2026-02-19 18:33:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 220ms

[303/1822] How can modified harmful datasets bypass moderation guardrai...
2026-02-19 18:33:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 237ms

[304/1822] Evaluating the effectiveness of guardrail moderation in prev...
2026-02-19 18:33:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.807 | Time: 295ms

[305/1822] How to integrate natural language, algorithmic, and symbolic...
2026-02-19 18:33:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.839 | Time: 214ms

[306/1822] Progressive paradigm training strategies for unifying multip...
2026-02-19 18:33:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.933 | Time: 249ms

[307/1822] training large language models to adaptively allocate infere...
2026-02-19 18:33:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[308/1822] Inference Budget-Constrained Policy Optimization for improvi...
2026-02-19 18:33:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 191ms

[309/1822] What are the common limitations and reliability issues of us...
2026-02-19 18:33:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 307ms

[310/1822] Frameworks and algorithms for improving the alignment and cr...
2026-02-19 18:33:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[311/1822] synthetic benchmarks for evaluating the long-context reasoni...
2026-02-19 18:33:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 215ms

[312/1822] how do large language models perform on long-context logical...
2026-02-19 18:33:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 198ms

[313/1822] How can sparse autoencoder features be optimized for precise...
2026-02-19 18:33:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 248ms

[314/1822] Comparison of Feature Guided Activation Additions with Contr...
2026-02-19 18:33:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.786 | Time: 202ms

[315/1822] how to automatically convert open ended visual question answ...
2026-02-19 18:33:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 420ms

[316/1822] agent based framework for generating challenging distractors...
2026-02-19 18:33:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[317/1822] How to adapt a small auto-regressive model like Qwen2 for mu...
2026-02-19 18:33:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 231ms

[318/1822] What are the most effective training data cleaning and sampl...
2026-02-19 18:33:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 240ms

[319/1822] How can Kolmogorov-Arnold Networks be combined with recurren...
2026-02-19 18:33:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 262ms

[320/1822] Applying learnable temporal spline functions and edge-based ...
2026-02-19 18:33:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 205ms

[321/1822] how to use causal reward modeling and counterfactual invaria...
2026-02-19 18:33:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 225ms

[322/1822] improving the fairness and reliability of large language mod...
2026-02-19 18:33:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 208ms

[323/1822] comprehensive review of mitigation strategies for large lang...
2026-02-19 18:33:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.853 | Time: 278ms

[324/1822] recent advancements in responsible AI for enhancing large la...
2026-02-19 18:33:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.693 | Time: 221ms

[325/1822] How can matching user queries against pre-generated question...
2026-02-19 18:33:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.944 | Time: 250ms

[326/1822] Techniques for reducing information dilution in RAG systems ...
2026-02-19 18:33:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.371 | Time: 204ms

[327/1822] vision transformer framework using foundation models for ene...
2026-02-19 18:33:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 215ms

[328/1822] automated generation of training data from immunofluorescenc...
2026-02-19 18:33:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 230ms

[329/1822] How to prevent attention distribution flattening in long con...
2026-02-19 18:33:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.907 | Time: 279ms

[330/1822] Improving length generalization and key information retrieva...
2026-02-19 18:33:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 225ms

[331/1822] How can external document knowledge be integrated directly i...
2026-02-19 18:33:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 193ms

[332/1822] Techniques for parameterizing retrieved knowledge into model...
2026-02-19 18:33:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 244ms

[333/1822] What are the limitations of graph neural networks for solvin...
2026-02-19 18:33:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 217ms

[334/1822] Investigating the computational power of message passing GNN...
2026-02-19 18:33:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 226ms

[335/1822] How to implement dynamic workflow adjustment and modular sub...
2026-02-19 18:33:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 211ms

[336/1822] Research on enhancing multi-agent framework performance thro...
2026-02-19 18:33:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 227ms

[337/1822] How can developers build a structured safety case to prove t...
2026-02-19 18:33:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 232ms

[338/1822] Evaluating AI control safety by using red teaming and conser...
2026-02-19 18:33:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.400 | NDCG@10: 0.643 | Time: 213ms

[339/1822] Comprehensive survey on explainable artificial intelligence ...
2026-02-19 18:33:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.867 | Time: 245ms

[340/1822] How can large language models and vision-language frameworks...
2026-02-19 18:33:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 210ms

[341/1822] benchmarking the performance of constrained decoding framewo...
2026-02-19 18:33:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 267ms

[342/1822] how do different constrained decoding tools compare in terms...
2026-02-19 18:33:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 207ms

[343/1822] how to merge multiple deep learning models sequentially with...
2026-02-19 18:33:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.972 | Time: 244ms

[344/1822] training-free methods for scalable continual model merging t...
2026-02-19 18:33:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 212ms

[345/1822] How to use audio large language models for natural language-...
2026-02-19 18:33:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 263ms

[346/1822] Alignment approach with LLM distillation for enhancing audio...
2026-02-19 18:33:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[347/1822] benchmarking page-level and layout-level retrieval systems f...
2026-02-19 18:33:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 313ms

[348/1822] evaluation of visual retrievers versus text-based retrievers...
2026-02-19 18:33:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 225ms

[349/1822] How to improve multilingual reasoning in large language mode...
2026-02-19 18:33:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 223ms

[350/1822] Efficient methods for multilingual reasoning alignment that ...
2026-02-19 18:33:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 199ms

[351/1822] How can large language models be trained to reason over user...
2026-02-19 18:33:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 219ms

[352/1822] Self-training frameworks for personalized LLMs that utilize ...
2026-02-19 18:33:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 164ms

[353/1822] methods for reducing hallucinations in multimodal large lang...
2026-02-19 18:33:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 219ms

[354/1822] improving multimodal LLM alignment through cross-modal prefe...
2026-02-19 18:33:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 185ms

[355/1822] How does Meta use Large Language Models for mutation-guided ...
2026-02-19 18:33:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: -625ms

[356/1822] LLM-based test generation framework for hardening Android Ko...
2026-02-19 18:33:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 266ms

[357/1822] How to implement a neuro-fuzzy system on an FPGA for real-ti...
2026-02-19 18:33:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 230ms

[358/1822] FPGA-based intelligent sensor for personalizing time headway...
2026-02-19 18:33:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 197ms

[359/1822] impact of AWQ and GPTQ low-bit quantization on the mathemati...
2026-02-19 18:33:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 206ms

[360/1822] effective fine-tuning strategies to restore mathematical rea...
2026-02-19 18:33:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 225ms

[361/1822] systematic review of large language models for natural disas...
2026-02-19 18:33:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 247ms

[362/1822] how are generative artificial intelligence and large languag...
2026-02-19 18:33:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 205ms

[363/1822] Does OpenAI's o3 model represent true artificial general int...
2026-02-19 18:33:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 275ms

[364/1822] Critique of massive trialling of predefined operations as a ...
2026-02-19 18:33:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 213ms

[365/1822] Evaluating implicit sociodemographic bias in large language ...
2026-02-19 18:33:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 246ms

[366/1822] Do advanced large language models exhibit greater implicit b...
2026-02-19 18:33:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 227ms

[367/1822] Systematic literature review of large language models in CHI...
2026-02-19 18:33:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[368/1822] How are large language models being used as research tools a...
2026-02-19 18:33:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 210ms

[369/1822] Evaluating personalized long-form text generation using larg...
2026-02-19 18:33:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 210ms

[370/1822] How to extract atomic aspects and evidence from LLM generate...
2026-02-19 18:33:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 186ms

[371/1822] large scale dataset of high quality science problem solution...
2026-02-19 18:33:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 310ms

[372/1822] automated extraction pipeline for building scientific reason...
2026-02-19 18:33:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.905 | Time: 257ms

[373/1822] comprehensive survey of gradient-based multi-objective optim...
2026-02-19 18:33:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 247ms

[374/1822] recent advancements in learning continuous Pareto sets and f...
2026-02-19 18:33:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 220ms

[375/1822] Comprehensive survey of deep reinforcement learning algorith...
2026-02-19 18:33:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 251ms

[376/1822] How does deep reinforcement learning compare to traditional ...
2026-02-19 18:33:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 248ms

[377/1822] Survey of model optimization and system architecture strateg...
2026-02-19 18:33:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.905 | Time: 217ms

[378/1822] How can cognitive edge computing frameworks balance latency,...
2026-02-19 18:33:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 268ms

[379/1822] how to generate 360 panoramas using multi-view diffusion mod...
2026-02-19 18:33:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 218ms

[380/1822] diffusion based method for high resolution 360 degree panora...
2026-02-19 18:33:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 266ms

[381/1822] How to improve concept erasure in diffusion models by dynami...
2026-02-19 18:33:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 224ms

[382/1822] Adaptive Guided Erasure method for selectively removing harm...
2026-02-19 18:33:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 248ms

[383/1822] How does a hybrid attention mechanism combining global and l...
2026-02-19 18:33:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 204ms

[384/1822] Comparison of RoPE, NoPE, and QK-Normalization patterns in t...
2026-02-19 18:33:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 224ms

[385/1822] How to design a multi-agent framework using large language m...
2026-02-19 18:33:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 208ms

[386/1822] LLM-based approach for mapping learner goals to skills and o...
2026-02-19 18:33:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 195ms

[387/1822] how do large language models exhibit conformity bias and gro...
2026-02-19 18:33:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 205ms

[388/1822] benchmarking social influence in AI agents using BenchForm t...
2026-02-19 18:33:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 224ms

[389/1822] benchmarking framework for evaluating large language model b...
2026-02-19 18:33:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 184ms

[390/1822] holistic platform for testing ai agents using microservice f...
2026-02-19 18:33:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 234ms

[391/1822] How can multimodal prompts like images videos and humming be...
2026-02-19 18:33:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 233ms

[392/1822] A generalized framework for symbolic music generation that u...
2026-02-19 18:33:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 237ms

[393/1822] how to implement training-free visual token pruning for mult...
2026-02-19 18:33:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.844 | Time: 236ms

[394/1822] reducing visual redundancy in MLLMs like LLaVA-NeXT through ...
2026-02-19 18:33:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 231ms

[395/1822] distributed training of large language models using asynchro...
2026-02-19 18:33:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 199ms

[396/1822] how to improve DiLoCo for distributed training by overlappin...
2026-02-19 18:33:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[397/1822] How can count-based exploration and optimistic reward estima...
2026-02-19 18:33:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.828 | Time: 247ms

[398/1822] A practical algorithm for online RLHF that uses a coin-flip ...
2026-02-19 18:33:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 248ms

[399/1822] How can large language models be used to identify adversaria...
2026-02-19 18:33:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 234ms

[400/1822] Improving the safety and robustness of autonomous vehicles t...
2026-02-19 18:33:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 232ms

[401/1822] Observational study on how programming students use generati...
2026-02-19 18:33:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.947 | Time: 272ms

[402/1822] How do computer science students interact with large languag...
2026-02-19 18:33:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 277ms

[403/1822] Video reasoning segmentation using multimodal large language...
2026-02-19 18:33:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 232ms

[404/1822] Improving video reasoning segmentation accuracy on ReVOS ben...
2026-02-19 18:33:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 236ms

[405/1822] How can concept activation vectors be used to steer large la...
2026-02-19 18:33:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[406/1822] Lightweight framework for granular control of LLM outputs by...
2026-02-19 18:33:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 213ms

[407/1822] How does GPT-4o perform on multimodal physics concept invent...
2026-02-19 18:33:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 327ms

[408/1822] Evaluating the multimodal and multilingual capabilities of l...
2026-02-19 18:33:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1226ms

[409/1822] Evaluation of large language models for translating natural ...
2026-02-19 18:33:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 234ms

[410/1822] Fine-tuning small language models with distilled high-qualit...
2026-02-19 18:33:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 244ms

[411/1822] Fine-grained complexity analysis of visual autoregressive mo...
2026-02-19 18:33:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 396ms

[412/1822] What are the computational limits and efficiency criteria fo...
2026-02-19 18:33:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 200ms

[413/1822] best practices and lessons learned from red teaming generati...
2026-02-19 18:33:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.939 | Time: 187ms

[414/1822] what are the key methodologies and threat model ontologies u...
2026-02-19 18:33:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 202ms

[415/1822] How can large language models be fine-tuned to improve their...
2026-02-19 18:33:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 209ms

[416/1822] Techniques for training large language models to ignore coun...
2026-02-19 18:33:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 173ms

[417/1822] relationship between non-smooth convex optimization theory a...
2026-02-19 18:33:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[418/1822] how to use optimization theory bounds to transfer optimal le...
2026-02-19 18:33:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.964 | Time: 184ms

[419/1822] How can multi-modal large language models be improved for fi...
2026-02-19 18:33:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 185ms

[420/1822] Enhancing fine-grained recognition in MLLMs using contrastiv...
2026-02-19 18:33:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 181ms

[421/1822] enhancing knowledge base question answering through agentic ...
2026-02-19 18:33:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 188ms

[422/1822] improving low resource kbqa performance with mcts guided exp...
2026-02-19 18:34:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 182ms

[423/1822] mathematical analysis of transformer layer dynamics using Vl...
2026-02-19 18:34:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 284ms

[424/1822] understanding the evolution of data anisotropy and clusterin...
2026-02-19 18:34:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 237ms

[425/1822] adaptive retrieval methods for overcoming bounded recall in ...
2026-02-19 18:34:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 210ms

[426/1822] improving retrieval recall by using listwise LLM rankers to ...
2026-02-19 18:34:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.840 | Time: 230ms

[427/1822] How can large language model agents be used for goal-driven ...
2026-02-19 18:34:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 274ms

[428/1822] Evaluation framework and novel dataset for assessing the qua...
2026-02-19 18:34:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 205ms

[429/1822] The relationship between softmax numerical stability and the...
2026-02-19 18:34:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 218ms

[430/1822] Achieving grokking without regularization by mitigating soft...
2026-02-19 18:34:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 311ms

[431/1822] automated techniques for optimizing prompts to improve test ...
2026-02-19 18:34:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 376ms

[432/1822] how to automatically generate model-specific prompts for sof...
2026-02-19 18:34:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 210ms

[433/1822] How can interdisciplinary collaboration between physicists a...
2026-02-19 18:34:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 214ms

[434/1822] A roadmap for creating large physics models using foundation...
2026-02-19 18:34:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 229ms

[435/1822] How can small language models achieve high performance in re...
2026-02-19 18:34:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 210ms

[436/1822] Efficient retrieval augmented generation framework for resou...
2026-02-19 18:34:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 214ms

[437/1822] How to reduce error accumulation in online test-time prompt ...
2026-02-19 18:34:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 196ms

[438/1822] Techniques for adaptive prompt selection based on prediction...
2026-02-19 18:34:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 252ms

[439/1822] Multi-modal sequential recommendation models using hierarchi...
2026-02-19 18:34:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 234ms

[440/1822] How can hierarchical mixture of experts and contrastive lear...
2026-02-19 18:34:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 241ms

[441/1822] A comprehensive review of Mixture of Experts architectures, ...
2026-02-19 18:34:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 259ms

[442/1822] How does the Mixture of Experts framework improve the perfor...
2026-02-19 18:34:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[443/1822] comprehensive guide to pre-training generative models and al...
2026-02-19 18:34:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.491 | Time: 169ms

[444/1822] academic reference for understanding the fundamental pillars...
2026-02-19 18:34:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.389 | Time: 167ms

[445/1822] training free inference time methods for mitigating hallucin...
2026-02-19 18:34:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 196ms

[446/1822] How can contrastive decoding mechanisms and masking signific...
2026-02-19 18:34:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 196ms

[447/1822] How does the extended context window of large language model...
2026-02-19 18:34:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 248ms

[448/1822] Leveraging long context large language models for text to SQ...
2026-02-19 18:34:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.737 | Time: 206ms

[449/1822] enhancing the performance of LLM input guardrails through ch...
2026-02-19 18:34:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.889 | Time: 235ms

[450/1822] how can fine-tuning large language models as judges with cha...
2026-02-19 18:34:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.840 | Time: 191ms

[451/1822] comprehensive review of text data augmentation methods using...
2026-02-19 18:34:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 213ms

[452/1822] current challenges and future opportunities in using generat...
2026-02-19 18:34:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 468ms

[453/1822] How can deep learning models combining LSTM and CNN with att...
2026-02-19 18:34:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 234ms

[454/1822] Using synthetic minority over-sampling technique and hybrid ...
2026-02-19 18:34:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 214ms

[455/1822] state of the art 8B small language model as a judge for gene...
2026-02-19 18:34:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 162ms

[456/1822] techniques for training small language models for automated ...
2026-02-19 18:34:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 212ms

[457/1822] How can self-reflection frameworks and simulated psychologic...
2026-02-19 18:34:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.973 | Time: 219ms

[458/1822] Investigating the inconsistency between explicit and implici...
2026-02-19 18:34:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.939 | Time: 194ms

[459/1822] How does GPU dynamic voltage and frequency scaling impact th...
2026-02-19 18:34:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 221ms

[460/1822] Analyzing the relationship between input sequence characteri...
2026-02-19 18:34:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 219ms

[461/1822] How can transformers perform full Bayesian inference using i...
2026-02-19 18:34:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 279ms

[462/1822] Comparing transformer in-context learning with Markov Chain ...
2026-02-19 18:34:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 203ms

[463/1822] evaluating the effectiveness and limitations of reinforcemen...
2026-02-19 18:34:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 247ms

[464/1822] hybrid training approaches combining RL and SFT to address r...
2026-02-19 18:34:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 245ms

[465/1822] How to apply speculative sampling techniques to accelerate i...
2026-02-19 18:34:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 248ms

[466/1822] Speeding up diffusion model generation using speculative dec...
2026-02-19 18:34:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 167ms

[467/1822] How to use synthetic persona data from Persona Hub to improv...
2026-02-19 18:34:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 213ms

[468/1822] Large-scale synthetic data generation strategies for trainin...
2026-02-19 18:34:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 218ms

[469/1822] How can temperature sensitivity be used to detect if instruc...
2026-02-19 18:34:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[470/1822] Assessing the vulnerability of vision-language models to mem...
2026-02-19 18:34:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 202ms

[471/1822] How has constructionism evolved as an educational framework ...
2026-02-19 18:34:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 234ms

[472/1822] Applying constructionist principles to smart education model...
2026-02-19 18:34:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 211ms

[473/1822] agentic workflows for program synthesis using LLM quality ch...
2026-02-19 18:34:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 202ms

[474/1822] how to implement a dynamic multi-agent system for program sy...
2026-02-19 18:34:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 170ms

[475/1822] How to use hierarchical feature trees and high-level abstrac...
2026-02-19 18:34:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[476/1822] Iterative feature tree synthesis framework for generating hi...
2026-02-19 18:34:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 228ms

[477/1822] Recent surveys on large vision-language model alignment and ...
2026-02-19 18:34:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 235ms

[478/1822] What are the primary causes of multimodal misalignment in vi...
2026-02-19 18:34:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 227ms

[479/1822] performance comparison of GPT-4o and Claude 3.5 against trad...
2026-02-19 18:34:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 336ms

[480/1822] evaluating the accuracy of large language models for line-by...
2026-02-19 18:34:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 244ms

[481/1822] benchmarking the security and vulnerability of retrieval-aug...
2026-02-19 18:34:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 358ms

[482/1822] how do external knowledge injections and unverified context ...
2026-02-19 18:34:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 187ms

[483/1822] automated extraction of olympiad level math problems from on...
2026-02-19 18:34:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 266ms

[484/1822] benchmarking mathematical reasoning in large language models...
2026-02-19 18:34:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 207ms

[485/1822] benchmarking the performance of large language model agents ...
2026-02-19 18:34:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 342ms

[486/1822] standardized evaluation framework for testing the planning a...
2026-02-19 18:34:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 227ms

[487/1822] benchmarking framework for evaluating the effectiveness of h...
2026-02-19 18:34:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 270ms

[488/1822] how well do modern hate speech detection systems perform aga...
2026-02-19 18:34:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 222ms

[489/1822] comprehensive survey of generative artificial intelligence t...
2026-02-19 18:34:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 259ms

[490/1822] how are diffusion models and multimodal AI being applied to ...
2026-02-19 18:34:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 311ms

[491/1822] How can diffusion priors be used to balance perceptual quali...
2026-02-19 18:34:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: -680ms

[492/1822] A unified image restoration model using diffusion priors wit...
2026-02-19 18:34:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 297ms

[493/1822] validated tools and metrics for evaluating the quality and a...
2026-02-19 18:34:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 269ms

[494/1822] how to assess the structural and substantive validity of aut...
2026-02-19 18:34:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 272ms

[495/1822] investigating the emergence and formation of localized task ...
2026-02-19 18:34:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 178ms

[496/1822] using task vector prompting loss to enhance task representat...
2026-02-19 18:34:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 179ms

[497/1822] How can large language models achieve higher knowledge densi...
2026-02-19 18:34:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 175ms

[498/1822] Machine writing frameworks that simulate human-like cognitiv...
2026-02-19 18:34:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[499/1822] How can modified Algorithm-of-Thoughts techniques like AoT+ ...
2026-02-19 18:34:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 467ms

[500/1822] Investigating the use of enhanced Algorithm-of-Thoughts fram...
2026-02-19 18:34:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 197ms

[501/1822] How can multi-agent systems and agentic AI frameworks like O...
2026-02-19 18:34:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 223ms

[502/1822] Evaluation of hallucination mitigation strategies using hier...
2026-02-19 18:34:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 183ms

[503/1822] How can knowledge distillation and conditional variational a...
2026-02-19 18:34:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 176ms

[504/1822] Adaptive diversity distillation techniques for math word pro...
2026-02-19 18:34:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 204ms

[505/1822] How can large language models be used to generate functional...
2026-02-19 18:34:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 199ms

[506/1822] Leveraging LLMs and design layout graphs to automate the cre...
2026-02-19 18:34:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 201ms

[507/1822] How well do large language models perform on Allen's interva...
2026-02-19 18:34:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 195ms

[508/1822] Evaluating large language model capabilities in temporal und...
2026-02-19 18:34:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 175ms

[509/1822] segment-level direct preference optimization for improving m...
2026-02-19 18:34:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.940 | Time: 201ms

[510/1822] how to optimize social agents using segment-based direct pre...
2026-02-19 18:34:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 171ms

[511/1822] How to improve RAG systems for industrial applications using...
2026-02-19 18:34:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.973 | Time: 265ms

[512/1822] Techniques for knowledge atomizing and knowledge-aware task ...
2026-02-19 18:34:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 271ms

[513/1822] How can large language models be integrated into an analytic...
2026-02-19 18:34:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 206ms

[514/1822] A framework for analyzing multi-user XR sessions using a pla...
2026-02-19 18:34:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 239ms

[515/1822] generating diverse and customizable synthetic Q&A pairs for ...
2026-02-19 18:34:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.944 | Time: 259ms

[516/1822] a two-stage framework for producing lexically and semantical...
2026-02-19 18:34:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 209ms

[517/1822] How can large language models like fine-tuned BART and BERT ...
2026-02-19 18:34:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 209ms

[518/1822] Proactive intrusion prediction framework for IoT security us...
2026-02-19 18:34:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 215ms

[519/1822] Evaluating the performance of machine-generated text detecto...
2026-02-19 18:34:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 218ms

[520/1822] Can machine learning models robustly detect AI-generated con...
2026-02-19 18:34:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 214ms

[521/1822] How to improve safety visual reasoning in large vision-langu...
2026-02-19 18:34:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 170ms

[522/1822] Reducing attack success rate in vision-language models by ad...
2026-02-19 18:34:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 195ms

[523/1822] How to achieve temporally consistent video relighting using ...
2026-02-19 18:34:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[524/1822] Frameworks for video relighting that preserve illumination p...
2026-02-19 18:34:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 162ms

[525/1822] How to improve Sharpness-Aware Minimization performance by e...
2026-02-19 18:34:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 217ms

[526/1822] Analysis of SAM training dynamics using third-order stochast...
2026-02-19 18:34:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[527/1822] evaluating uncertainty estimation versus self-knowledge for ...
2026-02-19 18:34:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.906 | Time: 241ms

[528/1822] comprehensive analysis of uncertainty estimation techniques ...
2026-02-19 18:34:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 241ms

[529/1822] improving mathematical reasoning in large language models us...
2026-02-19 18:34:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 183ms

[530/1822] enhancing LLM math capabilities with a first-try strategy an...
2026-02-19 18:34:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.111 | P@5: 0.000 | NDCG@10: 0.301 | Time: 173ms

[531/1822] benchmarks for evaluating long-context language models on co...
2026-02-19 18:34:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 197ms

[532/1822] how do current large language models perform on tasks requir...
2026-02-19 18:34:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 219ms

[533/1822] How can domain prompts and semantic prototypes be used in a ...
2026-02-19 18:34:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 237ms

[534/1822] Diffusion-based time series generation using prototype assig...
2026-02-19 18:34:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 207ms

[535/1822] How can we effectively detect AI-generated text that has bee...
2026-02-19 18:34:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 243ms

[536/1822] A data-centric augmentation approach for building robust mod...
2026-02-19 18:34:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.837 | Time: 225ms

[537/1822] How does the presence of citations in large language model r...
2026-02-19 18:34:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 275ms

[538/1822] Experimental research examining if users trust LLM generated...
2026-02-19 18:34:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 182ms

[539/1822] How can retrieval-augmented dynamic prompt tuning improve th...
2026-02-19 18:34:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 286ms

[540/1822] A framework for incomplete multimodal learning using retriev...
2026-02-19 18:34:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 267ms

[541/1822] How can model editing and attention head analysis be used to...
2026-02-19 18:34:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.772 | Time: 225ms

[542/1822] Improving large language model robustness by identifying key...
2026-02-19 18:34:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 434ms

[543/1822] evaluating large language models on everyday moral dilemmas ...
2026-02-19 18:34:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 195ms

[544/1822] how do large language models perform on complex social ethic...
2026-02-19 18:34:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 217ms

[545/1822] Effective post-training quantization methods for Mamba archi...
2026-02-19 18:34:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 221ms

[546/1822] How to use variance aligned rotation and Karhunen-Loeve Tran...
2026-02-19 18:34:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 275ms

[547/1822] How can black-box adversarial attacks disrupt decision-makin...
2026-02-19 18:34:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[548/1822] Evaluating the robustness of vision-language models in auton...
2026-02-19 18:34:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 175ms

[549/1822] How to use optimal transport and Monge-Kantorovich vector ra...
2026-02-19 18:34:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 232ms

[550/1822] Improving conformal prediction for multi-output regression a...
2026-02-19 18:34:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 193ms

[551/1822] How can direct preference optimization be used to personaliz...
2026-02-19 18:34:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 215ms

[552/1822] Aligning diffusion models with multiple individual user rewa...
2026-02-19 18:34:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 232ms

[553/1822] Survey of recent research on how embodiment, symbol groundin...
2026-02-19 18:34:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 201ms

[554/1822] How are researchers addressing the limitations of large lang...
2026-02-19 18:34:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.961 | Time: 205ms

[555/1822] academic benchmark for evaluating multi-hop tool use in larg...
2026-02-19 18:34:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 194ms

[556/1822] evaluation dataset for testing complex function calling and ...
2026-02-19 18:34:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 197ms

[557/1822] benchmarking cultural bias towards Western entities in Arabi...
2026-02-19 18:34:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 249ms

[558/1822] impact of pre-training data frequency and subword tokenizati...
2026-02-19 18:34:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 173ms

[559/1822] How can internal activation steering improve the safety and ...
2026-02-19 18:34:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 210ms

[560/1822] Methods for revising multimodal model activations during gen...
2026-02-19 18:34:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 198ms

[561/1822] How can multi-agent large language model frameworks improve ...
2026-02-19 18:34:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 231ms

[562/1822] Implementing multi-agent orchestration for geospatial tasks ...
2026-02-19 18:34:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.982 | Time: 220ms

[563/1822] How can we design just-in-time and human-verifiable security...
2026-02-19 18:34:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 250ms

[564/1822] A framework for generating contextual security policies for ...
2026-02-19 18:34:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 185ms

[565/1822] External safety evaluation results and red teaming findings ...
2026-02-19 18:34:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 197ms

[566/1822] Systematic generation of unsafe test inputs to assess the pr...
2026-02-19 18:34:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.934 | Time: 202ms

[567/1822] flexible and scalable training system for sparse mixture of ...
2026-02-19 18:34:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 228ms

[568/1822] optimizing task scheduling and communication overhead in lar...
2026-02-19 18:34:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 186ms

[569/1822] How can retrieval augmented generation and large language mo...
2026-02-19 18:34:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 220ms

[570/1822] Implementing RAG-based systems for real-time phone call frau...
2026-02-19 18:34:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 214ms

[571/1822] benchmarking the performance of multimodal large language mo...
2026-02-19 18:34:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 224ms

[572/1822] evaluating the ability of MLLMs to determine chronological e...
2026-02-19 18:34:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 238ms

[573/1822] Improving quantum machine learning performance by training t...
2026-02-19 18:34:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 230ms

[574/1822] End-to-end differentiable framework for learning parameteriz...
2026-02-19 18:34:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 1302ms

[575/1822] Best practices and training recipes for domain-adaptive post...
2026-02-19 18:34:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 205ms

[576/1822] How does combining continual pre-training with instruction-f...
2026-02-19 18:34:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 223ms

[577/1822] how to model long-range dependencies in brain networks using...
2026-02-19 18:34:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 275ms

[578/1822] using biased random walks in brain graph transformers to mod...
2026-02-19 18:34:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 188ms

[579/1822] adaptive perturbation methods for mitigating harmful fine-tu...
2026-02-19 18:34:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 178ms

[580/1822] how to recover large language model safety alignment after h...
2026-02-19 18:34:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 201ms

[581/1822] comprehensive benchmark for evaluating undergraduate level m...
2026-02-19 18:34:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 196ms

[582/1822] evaluating large reasoning models using metrics like effecti...
2026-02-19 18:34:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 203ms

[583/1822] How to improve table understanding in language models using ...
2026-02-19 18:34:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[584/1822] A framework for enhancing table reasoning in LLMs by extract...
2026-02-19 18:34:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.905 | Time: 183ms

[585/1822] comprehensive review of foundation models in computational p...
2026-02-19 18:34:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 237ms

[586/1822] what are the current challenges and methodologies for buildi...
2026-02-19 18:34:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 205ms

[587/1822] adaptive world model reinforcement learning for autonomous d...
2026-02-19 18:34:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 283ms

[588/1822] how to address distribution shift in world model based plann...
2026-02-19 18:34:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[589/1822] A comprehensive review of recent deep learning architectures...
2026-02-19 18:34:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 302ms

[590/1822] How do modern deep learning foundation models categorize the...
2026-02-19 18:34:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 224ms

[591/1822] evaluation of explainability methods for encoder based langu...
2026-02-19 18:34:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.925 | Time: 311ms

[592/1822] comparative analysis of lime shap and lrp techniques for tra...
2026-02-19 18:34:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 247ms

[593/1822] How do attention modules in different transformer layers con...
2026-02-19 18:34:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 247ms

[594/1822] Analyzing the role of early versus late transformer layers i...
2026-02-19 18:34:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[595/1822] evaluating the reliability of large language models as judge...
2026-02-19 18:34:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 355ms

[596/1822] comparing human rater performance with LLM as judge models u...
2026-02-19 18:34:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 231ms

[597/1822] How to generate task-specific LoRA weights using Conditional...
2026-02-19 18:34:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 193ms

[598/1822] Using meta-learning and CVAE generators to produce task-awar...
2026-02-19 18:34:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 202ms

[599/1822] how to use a world knowledge tree and self-reflection refine...
2026-02-19 18:34:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 200ms

[600/1822] framework for scaling supervised fine-tuning data through kn...
2026-02-19 18:34:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 220ms

[601/1822] Challenges and future research directions for integrating mu...
2026-02-19 18:34:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 298ms

[602/1822] What are the essential requirements and desiderata for devel...
2026-02-19 18:34:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 232ms

[603/1822] how to use prioritized depth-first search and large language...
2026-02-19 18:34:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.573 | Time: 281ms

[604/1822] combining embedding based retrieval with search heuristics t...
2026-02-19 18:34:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 242ms

[605/1822] taxonomy of interaction types between software developers an...
2026-02-19 18:34:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 335ms

[606/1822] how do developers interact with generative AI and large lang...
2026-02-19 18:34:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 233ms

[607/1822] A comprehensive analysis and taxonomy of common error types ...
2026-02-19 18:34:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 238ms

[608/1822] New methods for efficient error detection and automated repa...
2026-02-19 18:34:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 202ms

[609/1822] comprehensive survey of large language models in bioinformat...
2026-02-19 18:34:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 246ms

[610/1822] what are the current challenges and future directions for us...
2026-02-19 18:34:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 220ms

[611/1822] comprehensive review of test-time compute scaling methods fo...
2026-02-19 18:34:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 206ms

[612/1822] survey on inference time computation techniques such as self...
2026-02-19 18:34:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 195ms

[613/1822] integrating large language model multi-agent frameworks with...
2026-02-19 18:34:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 191ms

[614/1822] performance evaluation of Autogen based multi-agent systems ...
2026-02-19 18:34:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 176ms

[615/1822] How do large language models respond to malicious jailbreaki...
2026-02-19 18:34:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[616/1822] Investigating the impact of using fabricated scientific argu...
2026-02-19 18:34:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 178ms

[617/1822] factors influencing high school students' acceptance of gene...
2026-02-19 18:34:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 264ms

[618/1822] research on the role of perceived enjoyment and compatibilit...
2026-02-19 18:34:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 239ms

[619/1822] dataset for evaluating vision language models on handwritten...
2026-02-19 18:34:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 297ms

[620/1822] how well do vision language models perform on reasoning task...
2026-02-19 18:34:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 195ms

[621/1822] adaptive interpolation methods for knowledge distillation to...
2026-02-19 18:34:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 228ms

[622/1822] how to prevent mode collapse and mode averaging in language ...
2026-02-19 18:34:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 266ms

[623/1822] automated methods for optimizing large language model pretra...
2026-02-19 18:34:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 227ms

[624/1822] how to improve LLM training efficiency with UtiliMax and MED...
2026-02-19 18:34:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 251ms

[625/1822] Diffusion transformer models for joint image and video virtu...
2026-02-19 18:34:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 225ms

[626/1822] How to maintain temporal consistency in long video virtual t...
2026-02-19 18:34:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 189ms

[627/1822] how to mitigate systematic misalignment in reinforcement lea...
2026-02-19 18:34:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 502ms

[628/1822] the impact of providing evaluators with simulated future con...
2026-02-19 18:34:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 212ms

[629/1822] How to perform precise free-form grounding across multiple i...
2026-02-19 18:34:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 248ms

[630/1822] Improving multi-image grounding capabilities in MLLMs throug...
2026-02-19 18:34:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: -665ms

[631/1822] How can large language models be integrated with high-order ...
2026-02-19 18:34:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 193ms

[632/1822] Framework for mixed-type data imputation using bidirectional...
2026-02-19 18:34:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 230ms

[633/1822] How can large language models and CTGANs be used to generate...
2026-02-19 18:34:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[634/1822] Evaluating the performance of GPT-based models and CTGAN in ...
2026-02-19 18:34:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 287ms

[635/1822] How can optimized soft prompts in the textual embedding spac...
2026-02-19 18:34:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 238ms

[636/1822] Effective safety alignment for diffusion models using catego...
2026-02-19 18:34:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 226ms

[637/1822] using reinforcement learning and representation space guidan...
2026-02-19 18:34:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 360ms

[638/1822] interpretable reinforcement learning methods for LLM jailbre...
2026-02-19 18:34:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 217ms

[639/1822] how to improve diversity in mixture of experts for low-rank ...
2026-02-19 18:34:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 194ms

[640/1822] using the Gram-Schmidt process and Stiefel manifold to enhan...
2026-02-19 18:34:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 182ms

[641/1822] How can hierarchical autoregressive transformers combine cha...
2026-02-19 18:34:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.807 | Time: 208ms

[642/1822] Large language models using character-to-word hierarchical a...
2026-02-19 18:34:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[643/1822] How does the increase in energy loss in the final layer of l...
2026-02-19 18:34:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 292ms

[644/1822] Effective methods for mitigating reward hacking in RLHF by p...
2026-02-19 18:34:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 195ms

[645/1822] certified robustness of large language models using randomiz...
2026-02-19 18:34:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 212ms

[646/1822] how to calculate tight lower bounds for the worst-case robus...
2026-02-19 18:34:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 231ms

[647/1822] How do scaling laws for large language models change when in...
2026-02-19 18:34:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 257ms

[648/1822] Empirical analysis of scaling laws for training differential...
2026-02-19 18:34:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 226ms

[649/1822] How consistent are large language models like GPT-4 and Clau...
2026-02-19 18:34:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 233ms

[650/1822] Measuring the instability of LLM outputs for legal decision ...
2026-02-19 18:34:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 204ms

[651/1822] Assessing the ability of large language models to trace exec...
2026-02-19 18:34:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 292ms

[652/1822] Measuring the gap between code generation performance and st...
2026-02-19 18:34:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 290ms

[653/1822] impact of using problem-solving data versus general mathemat...
2026-02-19 18:34:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 192ms

[654/1822] comparing mathematical reasoning performance of LLMs trained...
2026-02-19 18:34:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 203ms

[655/1822] How to modify Chinchilla scaling laws to include inference l...
2026-02-19 18:34:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 211ms

[656/1822] Training language models for inference efficiency by co-opti...
2026-02-19 18:34:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 182ms

[657/1822] How to use a hierarchical mixture-of-experts framework to mo...
2026-02-19 18:34:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 248ms

[658/1822] Advanced multimodal fake news detection methods focusing on ...
2026-02-19 18:34:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 324ms

[659/1822] Evaluating instruction-following large language models for z...
2026-02-19 18:34:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 231ms

[660/1822] How can large language models leverage natural language infe...
2026-02-19 18:34:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 166ms

[661/1822] accelerating diffusion model inference by exploiting tempora...
2026-02-19 18:34:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 230ms

[662/1822] how to design a hardware accelerator that leverages temporal...
2026-02-19 18:34:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.549 | Time: 230ms

[663/1822] A comprehensive survey of five hundred seventy two code benc...
2026-02-19 18:34:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 231ms

[664/1822] The HOW2BENCH framework and checklists for improving the qua...
2026-02-19 18:34:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 231ms

[665/1822] How does complexity control and neuron condensation influenc...
2026-02-19 18:34:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 218ms

[666/1822] Investigating the internal information circuits and complexi...
2026-02-19 18:34:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 468ms

[667/1822] using fine-tuned small language models like Llama 3 to gener...
2026-02-19 18:34:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 261ms

[668/1822] evaluating the effectiveness of quantized small language mod...
2026-02-19 18:34:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 268ms

[669/1822] Evaluating end-to-end spoken language models on knowledge un...
2026-02-19 18:34:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.982 | Time: 327ms

[670/1822] A benchmark for assessing the robustness and world knowledge...
2026-02-19 18:34:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 181ms

[671/1822] dynamic self-adaptation of large language models through sin...
2026-02-19 18:34:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 196ms

[672/1822] efficient alternative to LoRA for real-time task specific ad...
2026-02-19 18:34:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 211ms

[673/1822] fine-tuning T5-small for scalable and topic-controlled quest...
2026-02-19 18:34:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 191ms

[674/1822] how to generate semantically aligned and topic-specific ques...
2026-02-19 18:34:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 194ms

[675/1822] Open source TypeScript framework for building autonomous AI ...
2026-02-19 18:34:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 219ms

[676/1822] How to integrate large language models with web3 application...
2026-02-19 18:34:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 193ms

[677/1822] Theoretical analysis of prompt optimization as an alternativ...
2026-02-19 18:34:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 290ms

[678/1822] How can prompt optimization be formulated as an optimization...
2026-02-19 18:34:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 225ms

[679/1822] adaptive sublayer skipping techniques to accelerate prefilli...
2026-02-19 18:34:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 182ms

[680/1822] how to improve long-context LLM inference efficiency by iden...
2026-02-19 18:34:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 203ms

[681/1822] Dynamic structured pruning for large language models that ad...
2026-02-19 18:34:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 194ms

[682/1822] Using a sparse mask predictor to dynamically select relevant...
2026-02-19 18:34:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 174ms

[683/1822] efficient large language model tool learning methods using p...
2026-02-19 18:34:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 205ms

[684/1822] how to improve LLM tool calling efficiency by dividing compl...
2026-02-19 18:34:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 221ms

[685/1822] How does in-execution self-debugging using intermediate stat...
2026-02-19 18:34:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 235ms

[686/1822] Improving large language model programming performance throu...
2026-02-19 18:34:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 165ms

[687/1822] how to implement curriculum learning for large language mode...
2026-02-19 18:35:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[688/1822] dynamic pretraining data selection strategies based on chang...
2026-02-19 18:35:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 219ms

[689/1822] How well do large language models like GPT-4 and Claude perf...
2026-02-19 18:35:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.798 | Time: 250ms

[690/1822] Benchmarking different large language models and prompt engi...
2026-02-19 18:35:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 724ms

[691/1822] How can we identify and edit specific gender neurons in larg...
2026-02-19 18:35:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 241ms

[692/1822] A method for mitigating gender bias in LLMs through interpre...
2026-02-19 18:35:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 275ms

[693/1822] Strategies for accelerating deep learning inference on resou...
2026-02-19 18:35:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.945 | Time: 215ms

[694/1822] A comprehensive review of techniques like pruning, quantizat...
2026-02-19 18:35:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 220ms

[695/1822] How to improve text-to-CAD generation using large language m...
2026-02-19 18:35:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 196ms

[696/1822] Training large language models for CAD model creation throug...
2026-02-19 18:35:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 177ms

[697/1822] How can internal representations and hidden states of large ...
2026-02-19 18:35:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 231ms

[698/1822] Evaluating LLM code generation quality by analyzing latent s...
2026-02-19 18:35:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 212ms

[699/1822] how to measure and detect conversational bias in multi-agent...
2026-02-19 18:35:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 201ms

[700/1822] evaluating why traditional questionnaire-based bias detectio...
2026-02-19 18:35:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 177ms

[701/1822] enhancing graph retrieval-augmented generation for medical r...
2026-02-19 18:35:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 244ms

[702/1822] improving large language model explainability in high-stakes...
2026-02-19 18:35:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 177ms

[703/1822] How can attackers manipulate voting-based LLM leaderboards l...
2026-02-19 18:35:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.918 | Time: 230ms

[704/1822] Security vulnerabilities and mitigation strategies for crowd...
2026-02-19 18:35:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 169ms

[705/1822] improving mathematical reasoning in large language models by...
2026-02-19 18:35:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 197ms

[706/1822] methods for optimizing the intermediate steps of chain-of-th...
2026-02-19 18:35:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 227ms

[707/1822] critic-free reinforcement learning from human feedback using...
2026-02-19 18:35:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 224ms

[708/1822] comparison of local versus global advantage normalization in...
2026-02-19 18:35:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 204ms

[709/1822] How to implement multimodal large language model multi-agent...
2026-02-19 18:35:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.934 | Time: 200ms

[710/1822] Designing no-code frameworks for multimodal multi-agent syst...
2026-02-19 18:35:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 185ms

[711/1822] foundation model for automating systematic reviews through h...
2026-02-19 18:35:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 332ms

[712/1822] performance of large language models compared to clinicians ...
2026-02-19 18:35:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 221ms

[713/1822] How to train neural networks with brain-like topographic org...
2026-02-19 18:35:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 201ms

[714/1822] Developing spatially organized artificial neural networks th...
2026-02-19 18:35:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 229ms

[715/1822] Expert annotated datasets for legal information retrieval an...
2026-02-19 18:35:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 245ms

[716/1822] What are the available benchmarks for evaluating retrieval s...
2026-02-19 18:35:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 214ms

[717/1822] decentralized framework for specialized large language model...
2026-02-19 18:35:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 219ms

[718/1822] how can blockchain technology be integrated with fine-tuned ...
2026-02-19 18:35:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 236ms

[719/1822] Multi-agent system for question answering using routing and ...
2026-02-19 18:35:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 248ms

[720/1822] How can routing and planning in multi-agent RAG systems impr...
2026-02-19 18:35:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 262ms

[721/1822] How can crowdsourced LLM leaderboards like Chatbot Arena be ...
2026-02-19 18:35:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 183ms

[722/1822] Analysis of the vulnerability of Elo rating systems in large...
2026-02-19 18:35:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.613 | Time: 192ms

[723/1822] How to improve the transparency and verification of intermed...
2026-02-19 18:35:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[724/1822] Segmenting the chain of thought reasoning process into layer...
2026-02-19 18:35:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.957 | Time: 165ms

[725/1822] using crowdsourced metaphors to analyze public perception of...
2026-02-19 18:35:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 278ms

[726/1822] how do open-ended mental models and metaphors predict trust ...
2026-02-19 18:35:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 231ms

[727/1822] how to automatically verify the factual accuracy of large la...
2026-02-19 18:35:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 240ms

[728/1822] benchmarking automated systems for fact-checking medical dis...
2026-02-19 18:35:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 194ms

[729/1822] enhancing clinical reasoning in small language models throug...
2026-02-19 18:35:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 181ms

[730/1822] self-evolving framework for medical reasoning using soft dua...
2026-02-19 18:35:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 169ms

[731/1822] how to improve the diversity of language model outputs in RL...
2026-02-19 18:35:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 231ms

[732/1822] techniques for balancing human preference alignment and resp...
2026-02-19 18:35:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 201ms

[733/1822] Synthesizing long-context training data for large language m...
2026-02-19 18:35:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[734/1822] How can we effectively train long-context large language mod...
2026-02-19 18:35:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.753 | Time: 198ms

[735/1822] How do next-generation intelligent tutoring systems like Soc...
2026-02-19 18:35:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 264ms

[736/1822] Using generative AI and JSON-based prompts to create adaptiv...
2026-02-19 18:35:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 228ms

[737/1822] How to perform hierarchical code summarization for entire so...
2026-02-19 18:35:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 201ms

[738/1822] Improving repository-level software documentation through sy...
2026-02-19 18:35:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 176ms

[739/1822] How can Simulation Theory and task decomposition be used to ...
2026-02-19 18:35:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 184ms

[740/1822] Enhancing LLM performance on higher-order Theory of Mind tas...
2026-02-19 18:35:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 168ms

[741/1822] How can a taxonomy of user information needs guide the integ...
2026-02-19 18:35:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.972 | Time: 263ms

[742/1822] Synergies between large language models and knowledge graphs...
2026-02-19 18:35:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 235ms

[743/1822] how to use large language models and constraint logic progra...
2026-02-19 18:35:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 211ms

[744/1822] combining LLMs with logic programs to improve the accuracy a...
2026-02-19 18:35:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 218ms

[745/1822] Controllable video generation using blob representations and...
2026-02-19 18:35:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 241ms

[746/1822] Methods for enhancing compositional text-to-video generation...
2026-02-19 18:35:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.977 | Time: 257ms

[747/1822] Empirical study evaluating the proficiency of code large lan...
2026-02-19 18:35:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 225ms

[748/1822] How do the internal biases of code LLMs affect their ability...
2026-02-19 18:35:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 219ms

[749/1822] How to achieve faster LLM inference on mobile devices by usi...
2026-02-19 18:35:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 187ms

[750/1822] Optimization techniques for on-device large language model i...
2026-02-19 18:35:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 170ms

[751/1822] applying superstatistical methods and q-Gaussian distributio...
2026-02-19 18:35:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 254ms

[752/1822] using the Informer transformer model and LightGBM for long-t...
2026-02-19 18:35:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 214ms

[753/1822] Multi-agent systems using small language models and retrieva...
2026-02-19 18:35:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 184ms

[754/1822] Using fine-tuned language models and RAG to democratize bioi...
2026-02-19 18:35:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 218ms

[755/1822] semi-supervised learning methods for fine-grained action rec...
2026-02-19 18:35:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 220ms

[756/1822] how to improve fine-grained action recognition with limited ...
2026-02-19 18:35:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 218ms

[757/1822] Scalable graph neural network framework for recommendation u...
2026-02-19 18:35:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 280ms

[758/1822] How to improve the efficiency and robustness of graph based ...
2026-02-19 18:35:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 224ms

[759/1822] evaluating the reliability and cultural sensitivity of large...
2026-02-19 18:35:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.968 | Time: 253ms

[760/1822] benchmarking value misalignment in open-source large languag...
2026-02-19 18:35:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 196ms

[761/1822] how to improve multimodal large language model performance o...
2026-02-19 18:35:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 179ms

[762/1822] benchmarking numerical reasoning and structure recognition i...
2026-02-19 18:35:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 221ms

[763/1822] How can internal chain of thought reasoning steps in customi...
2026-02-19 18:35:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 204ms

[764/1822] Stealthy backdoor attacks on large language models that use ...
2026-02-19 18:35:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 218ms

[765/1822] how to improve contextual faithfulness in retrieval-augmente...
2026-02-19 18:35:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[766/1822] enhancing faithfulness in long-form question answering by tr...
2026-02-19 18:35:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 197ms

[767/1822] How to use language-guided cross-attention mechanisms to pru...
2026-02-19 18:35:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 231ms

[768/1822] A simple plug-and-play method for vision token pruning in ML...
2026-02-19 18:35:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 181ms

[769/1822] How to use Transformer models for controllable multitrack MI...
2026-02-19 18:35:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[770/1822] Generative music models for computer-assisted composition th...
2026-02-19 18:35:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 196ms

[771/1822] How can structured prompt design and in-context learning tec...
2026-02-19 18:35:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.855 | Time: 231ms

[772/1822] Evaluating the effectiveness of general-purpose large langua...
2026-02-19 18:35:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.924 | Time: 185ms

[773/1822] research on context-aware safety benchmarks for large langua...
2026-02-19 18:35:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 172ms

[774/1822] how to evaluate large language model safety by considering c...
2026-02-19 18:35:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 177ms

[775/1822] how to use knowledge graph retrieval augmented generation to...
2026-02-19 18:35:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.768 | Time: -608ms

[776/1822] graph based retrieval methods for resolving semantic ambigui...
2026-02-19 18:35:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 206ms

[777/1822] How to measure and reduce redundancy in multi-modality large...
2026-02-19 18:35:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 214ms

[778/1822] Quantitative analysis of redundant test questions and overla...
2026-02-19 18:35:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 179ms

[779/1822] How does changing the reward function shape with an alpha pa...
2026-02-19 18:35:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 214ms

[780/1822] Using AlphaPO to mitigate likelihood displacement and over-o...
2026-02-19 18:35:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 205ms

[781/1822] Recent survey of foundation model based agents capable of co...
2026-02-19 18:35:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[782/1822] What are the current research gaps and taxonomies for digita...
2026-02-19 18:35:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 179ms

[783/1822] machine learning models and explainable AI techniques for de...
2026-02-19 18:35:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.750 | Time: 273ms

[784/1822] comparison of XGBoost and Random Forest performance against ...
2026-02-19 18:35:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 272ms

[785/1822] How can a multi-agent LLM system provide decision interpreta...
2026-02-19 18:35:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 184ms

[786/1822] Improving the reliability of LLM-based RTL code generation t...
2026-02-19 18:35:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 183ms

[787/1822] efficient document compression methods for retrieval augment...
2026-02-19 18:35:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 193ms

[788/1822] how to achieve high compression rates for RAG context window...
2026-02-19 18:35:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[789/1822] how does using chain of thought reasoning influence the conf...
2026-02-19 18:35:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 189ms

[790/1822] impact of providing reasoning steps on the overconfidence of...
2026-02-19 18:35:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 221ms

[791/1822] how can chain of thought prompting be used to generate empat...
2026-02-19 18:35:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 258ms

[792/1822] two-stage training approach for speech-based large language ...
2026-02-19 18:35:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 238ms

[793/1822] evaluating instruction tuning data quality by measuring the ...
2026-02-19 18:35:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 261ms

[794/1822] how to improve synthetic dataset integrity by filtering out ...
2026-02-19 18:35:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 218ms

[795/1822] Evaluating the performance of causal sequence decoding model...
2026-02-19 18:35:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 209ms

[796/1822] How do language model decoders trained with cross-entropy lo...
2026-02-19 18:35:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 180ms

[797/1822] automatic prompt engineering for multi-step LLM pipelines us...
2026-02-19 18:35:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 227ms

[798/1822] how to optimize complex LLM workflows with feedback-based pr...
2026-02-19 18:35:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 190ms

[799/1822] training-free approach to long video understanding using con...
2026-02-19 18:35:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 223ms

[800/1822] how to process arbitrarily long videos in video question ans...
2026-02-19 18:35:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[801/1822] How can large language model serving systems achieve both pr...
2026-02-19 18:35:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.901 | Time: 200ms

[802/1822] The impact of Deficit Longest Prefix Match and D2LPM on thro...
2026-02-19 18:35:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 227ms

[803/1822] using entropy-based selective classifiers to estimate confid...
2026-02-19 18:35:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 194ms

[804/1822] comparing model calibration and error detection performance ...
2026-02-19 18:35:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 498ms

[805/1822] how to improve retrieval-augmented generation performance us...
2026-02-19 18:35:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 177ms

[806/1822] impact of multi-granular and self-contained document chunkin...
2026-02-19 18:35:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 183ms

[807/1822] Performance comparison of monolingual versus multilingual BE...
2026-02-19 18:35:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 300ms

[808/1822] Introduction of a public UPOS-tagged dataset and evaluation ...
2026-02-19 18:35:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 244ms

[809/1822] AI-powered learning platform using Retrieval-Augmented Gener...
2026-02-19 18:35:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 232ms

[810/1822] How can agentic Large Language Model assistants provide pers...
2026-02-19 18:35:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 222ms

[811/1822] How does the Graph-PReFLexOR framework use graph reasoning a...
2026-02-19 18:35:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 198ms

[812/1822] Integrating category theory and knowledge graph growth strat...
2026-02-19 18:35:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 234ms

[813/1822] high quality Chinese datasets for large language model pretr...
2026-02-19 18:35:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 245ms

[814/1822] curated Chinese language model training corpora for improvin...
2026-02-19 18:35:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 226ms

[815/1822] How to improve many-shot in-context learning performance in ...
2026-02-19 18:35:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 184ms

[816/1822] Techniques for addressing performance degradation in many-sh...
2026-02-19 18:35:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[817/1822] How to improve context selection in multimodal RAG using rel...
2026-02-19 18:35:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 207ms

[818/1822] Adaptive context selection and re-ranking methods for improv...
2026-02-19 18:35:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 221ms

[819/1822] benchmarks for evaluating adversarial robustness and composi...
2026-02-19 18:35:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 211ms

[820/1822] how to improve the reliability of audio-visual LLMs using ca...
2026-02-19 18:35:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 171ms

[821/1822] How to improve GUI action grounding in novel environments us...
2026-02-19 18:35:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 257ms

[822/1822] Using MLLM based agents and Q-value-Incentive In-Context Rei...
2026-02-19 18:35:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 318ms

[823/1822] impact of learning rates and training data size on the out-o...
2026-02-19 18:35:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 223ms

[824/1822] how to optimize instruction tuning for table tasks while mai...
2026-02-19 18:35:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 173ms

[825/1822] How to use rank-wise mixture of experts in LoRA to improve m...
2026-02-19 18:35:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 200ms

[826/1822] Efficient parameter fine-tuning for multiple tasks by treati...
2026-02-19 18:35:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 184ms

[827/1822] evaluation datasets for measuring the instruction following ...
2026-02-19 18:35:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 289ms

[828/1822] how do multilingual retrieval models perform when given comp...
2026-02-19 18:35:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 239ms

[829/1822] Applying social choice theory and maximal lotteries to impro...
2026-02-19 18:35:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 229ms

[830/1822] How does Nash Learning from Human Feedback approximate maxim...
2026-02-19 18:35:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 311ms

[831/1822] LLM retrieval methods that align complex questions with data...
2026-02-19 18:35:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 223ms

[832/1822] How to use relationship exploration between data objects to ...
2026-02-19 18:35:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 206ms

[833/1822] How can decoder-only LLMs be used for extractive schema link...
2026-02-19 18:35:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.942 | Time: 201ms

[834/1822] Improving schema linking accuracy and computational efficien...
2026-02-19 18:35:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 169ms

[835/1822] graph prompt tuning for heterophily graphs using distributio...
2026-02-19 18:35:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 195ms

[836/1822] how to use hop-specific prompts and generalized low-rank ada...
2026-02-19 18:35:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 184ms

[837/1822] How to implement hierarchical backpressure for autoscaling l...
2026-02-19 18:35:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 215ms

[838/1822] Improving GPU utilization and SLO attainment in LLM inferenc...
2026-02-19 18:35:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[839/1822] zero-shot hallucination detection in large language models u...
2026-02-19 18:35:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 265ms

[840/1822] how can attention weights and query categorization be used t...
2026-02-19 18:35:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 191ms

[841/1822] How vulnerable is GraphRAG to data poisoning attacks compare...
2026-02-19 18:35:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 336ms

[842/1822] Evaluation of poisoning attacks on knowledge graph based RAG...
2026-02-19 18:35:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 235ms

[843/1822] How to evaluate the coverage of diverse factual information ...
2026-02-19 18:35:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 273ms

[844/1822] Automated framework for measuring information diversity and ...
2026-02-19 18:35:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 238ms

[845/1822] theoretical framework for contrastive pre-training using app...
2026-02-19 18:35:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 188ms

[846/1822] sample complexity guarantees and joint generative hierarchic...
2026-02-19 18:35:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 170ms

[847/1822] How can retrieval-augmented dialogue knowledge aggregation i...
2026-02-19 18:35:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.927 | Time: 214ms

[848/1822] A multi-granularity graph-based approach for aggregating sem...
2026-02-19 18:35:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 217ms

[849/1822] how can large language models be used for zero-shot and few-...
2026-02-19 18:35:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[850/1822] large scale source code authorship identification using a to...
2026-02-19 18:35:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 207ms

[851/1822] Auto-regressive transformer models for graph generation usin...
2026-02-19 18:35:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 187ms

[852/1822] How to use pre-trained transformers for graph property predi...
2026-02-19 18:35:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 289ms

[853/1822] framework for evaluating multimodal retrieval augmented gene...
2026-02-19 18:35:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 195ms

[854/1822] measuring the reliability of multimodal RAG systems by asses...
2026-02-19 18:35:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 197ms

[855/1822] detailed training logs and implementation strategies for bui...
2026-02-19 18:35:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 327ms

[856/1822] open source resources and technical documentation for addres...
2026-02-19 18:35:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 260ms

[857/1822] hybrid framework for automated log analysis using uncertaint...
2026-02-19 18:35:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 225ms

[858/1822] how to improve log analysis performance using large language...
2026-02-19 18:35:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[859/1822] self-supervised quantization methods for integrating knowled...
2026-02-19 18:35:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 177ms

[860/1822] how to use quantized entity representations to improve large...
2026-02-19 18:35:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[861/1822] generalizing the logistic loss function for language modelin...
2026-02-19 18:35:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 303ms

[862/1822] evaluating alpha-divergence based loss functions and paralle...
2026-02-19 18:35:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 211ms

[863/1822] How do current large language models perform on tasks evalua...
2026-02-19 18:35:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.937 | Time: 202ms

[864/1822] A comprehensive evaluation framework and synthetic benchmark...
2026-02-19 18:35:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 217ms

[865/1822] How do function encoders using least-squares optimization co...
2026-02-19 18:35:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 253ms

[866/1822] A geometric approach to transfer learning characterizing int...
2026-02-19 18:35:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 216ms

[867/1822] How does a block causal transformer architecture improve nex...
2026-02-19 18:35:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 212ms

[868/1822] Foundation models for fluid dynamics using block causal tran...
2026-02-19 18:35:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[869/1822] how to obtain valid confidence intervals when using machine ...
2026-02-19 18:35:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 274ms

[870/1822] prediction powered inference bootstrap methods for debiasing...
2026-02-19 18:35:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 210ms

[871/1822] How can mixture of experts architectures improve the perform...
2026-02-19 18:35:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 186ms

[872/1822] Using rectified flow pose diffusion and multi-modal LLMs for...
2026-02-19 18:35:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 223ms

[873/1822] using parameter trust regions to mitigate knowledge conflict...
2026-02-19 18:35:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.860 | Time: 190ms

[874/1822] training-free techniques for multi-task model merging that p...
2026-02-19 18:35:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 185ms

[875/1822] How to use probabilistic federated search to improve retriev...
2026-02-19 18:35:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 230ms

[876/1822] Improving RAG performance for multi-product QA by aggregatin...
2026-02-19 18:35:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 199ms

[877/1822] multilingual dataset for evaluating consistency of large lan...
2026-02-19 18:35:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 436ms

[878/1822] methodology for comparing health-related inquiry consistency...
2026-02-19 18:35:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 239ms

[879/1822] How can LLM-based multi-agent systems automate the entire fi...
2026-02-19 18:35:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 172ms

[880/1822] A collaborative framework using multiple language model agen...
2026-02-19 18:35:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 191ms

[881/1822] How to precisely control camera extrinsic and intrinsic para...
2026-02-19 18:35:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 269ms

[882/1822] Methods for adjusting camera angles and lens distortions in ...
2026-02-19 18:35:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 209ms

[883/1822] How can attackers use training loss information from proprie...
2026-02-19 18:35:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 186ms

[884/1822] Vulnerability of Google Gemini models to adversarial prompt ...
2026-02-19 18:35:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 206ms

[885/1822] Diffusion models for large scale neural network parameter ge...
2026-02-19 18:35:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 220ms

[886/1822] Techniques for generating full network parameters for vision...
2026-02-19 18:35:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 182ms

[887/1822] How can abductive reasoning be used to infer user personas f...
2026-02-19 18:35:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 223ms

[888/1822] Improving LLM personalization by training on preference data...
2026-02-19 18:35:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 257ms

[889/1822] How can hierarchical attention mechanisms be used to improve...
2026-02-19 18:35:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 203ms

[890/1822] Recent approaches to zero-shot video-to-music generation usi...
2026-02-19 18:35:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.980 | Time: 216ms

[891/1822] LLM text-to-SQL framework using statistical conformal predic...
2026-02-19 18:35:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.937 | Time: 217ms

[892/1822] How can human-in-the-loop and branching point prediction imp...
2026-02-19 18:35:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 237ms

[893/1822] How can optimal transport-based alignment loss and attention...
2026-02-19 18:35:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 198ms

[894/1822] State-of-the-art methods for integrating visual cues into au...
2026-02-19 18:35:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 264ms

[895/1822] How are large language models being used to detect hate spee...
2026-02-19 18:35:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 222ms

[896/1822] Recent advancements in using cutting edge language models fo...
2026-02-19 18:35:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 206ms

[897/1822] how to generate counterfactual and contrastive explanations ...
2026-02-19 18:35:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 254ms

[898/1822] model intrusive methods for interpreting DCNN image classifi...
2026-02-19 18:35:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 192ms

[899/1822] How do dimensionality reduction techniques like PCA and UMAP...
2026-02-19 18:35:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 268ms

[900/1822] Analyzing the multidimensional and layer-wise characteristic...
2026-02-19 18:35:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 201ms

[901/1822] how to optimize the scaling factor in low-rank adaptation to...
2026-02-19 18:35:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 185ms

[902/1822] efficient methods for accuracy recovery when fine-tuning pru...
2026-02-19 18:35:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[903/1822] Dataset for long-form video understanding instruction tuning...
2026-02-19 18:35:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 205ms

[904/1822] How to improve video large language model performance on lon...
2026-02-19 18:35:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 181ms

[905/1822] evaluation of ChatGPT's ability to generate FEniCS and MATLA...
2026-02-19 18:35:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 257ms

[906/1822] using prompt engineering to implement numerical models for u...
2026-02-19 18:35:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 214ms

[907/1822] comprehensive survey of recent advances in deep learning for...
2026-02-19 18:35:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.889 | Time: 199ms

[908/1822] review of foundation models and specialized transformer arch...
2026-02-19 18:35:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.828 | Time: 191ms

[909/1822] How can image-to-text conversion and chain-of-thought improv...
2026-02-19 18:35:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 178ms

[910/1822] Methods to mitigate modality imbalance in vision language mo...
2026-02-19 18:35:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[911/1822] How can deep neural decision trees and forests be used to im...
2026-02-19 18:35:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[912/1822] A comparative study on using deep neural decision forests an...
2026-02-19 18:35:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 225ms

[913/1822] how to use program-driven verification and dual refinement t...
2026-02-19 18:35:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 330ms

[914/1822] enhancing large language model self-correction using self-ge...
2026-02-19 18:35:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 180ms

[915/1822] How can large language models be integrated with symbolic ac...
2026-02-19 18:35:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 164ms

[916/1822] Using action languages to bridge the gap between natural lan...
2026-02-19 18:35:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 229ms

[917/1822] how to improve the robustness of retrieval augmented generat...
2026-02-19 18:35:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 293ms

[918/1822] a training free plug and play framework for filtering malici...
2026-02-19 18:35:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 222ms

[919/1822] How can large language models use iterative self-questioning...
2026-02-19 18:35:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 187ms

[920/1822] State of the art methods for generating chronological news s...
2026-02-19 18:35:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 172ms

[921/1822] How to perform efficient stylized question answering in larg...
2026-02-19 18:35:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 201ms

[922/1822] Lightweight and train-free methods for controlling LLM respo...
2026-02-19 18:35:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: -611ms

[923/1822] How can large language models resolve conflicts between edit...
2026-02-19 18:35:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.642 | Time: 240ms

[924/1822] A retrieval-based framework for updating large language mode...
2026-02-19 18:35:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 216ms

[925/1822] How to generate efficient Shapley value explanations for tim...
2026-02-19 18:35:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 206ms

[926/1822] Time-series transformer architectures that use Shapley-based...
2026-02-19 18:35:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 182ms

[927/1822] speculative decoding for large language models using draft a...
2026-02-19 18:35:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.751 | Time: 185ms

[928/1822] how to implement lossless speculative decoding for accelerat...
2026-02-19 18:35:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.870 | Time: 189ms

[929/1822] enhancing neural theorem proving with large language models ...
2026-02-19 18:35:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 179ms

[930/1822] efficient recursive proving algorithms for automated theorem...
2026-02-19 18:35:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 161ms

[931/1822] autonomous red teaming of multi-host networks using large la...
2026-02-19 18:35:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 246ms

[932/1822] evaluating the effectiveness of LLM-based penetration testin...
2026-02-19 18:35:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.564 | Time: 236ms

[933/1822] dataset for panoptic segmentation-captioning with instance-s...
2026-02-19 18:35:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 313ms

[934/1822] improving region-level comprehension and language generation...
2026-02-19 18:35:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 197ms

[935/1822] how do large language models handle time-sensitive factual k...
2026-02-19 18:35:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 233ms

[936/1822] improving the accuracy and consistency of large language mod...
2026-02-19 18:35:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 173ms

[937/1822] How to evaluate large language models in personalized recomm...
2026-02-19 18:35:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[938/1822] Assessing the capability of large language models to capture...
2026-02-19 18:35:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 199ms

[939/1822] applying test-time training to large language models for imp...
2026-02-19 18:35:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 217ms

[940/1822] how to improve medical reasoning in LLMs using high quality ...
2026-02-19 18:35:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 236ms

[941/1822] training medical patient simulators using dialogue strategie...
2026-02-19 18:35:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 182ms

[942/1822] investigating the relationship between medical inquiry quali...
2026-02-19 18:35:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 188ms

[943/1822] Fine-grained medical vision-language pre-training using larg...
2026-02-19 18:35:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[944/1822] How to improve medical image analysis through knowledge inje...
2026-02-19 18:35:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 189ms

[945/1822] Using LLaVA and multimodal-to-text prompt engineering for id...
2026-02-19 18:35:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 305ms

[946/1822] How can large language models and feature embeddings be appl...
2026-02-19 18:35:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 281ms

[947/1822] evaluating sparse autoencoders for llm interpretability usin...
2026-02-19 18:35:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.942 | Time: 232ms

[948/1822] how do sparse autoencoders distinguish different meanings of...
2026-02-19 18:35:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[949/1822] how to improve self-adaptation in configurable systems using...
2026-02-19 18:35:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 237ms

[950/1822] techniques for continuous configuration optimization in inte...
2026-02-19 18:35:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 297ms

[951/1822] How to use large-scale synthetic data to improve spoken dial...
2026-02-19 18:35:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 194ms

[952/1822] Multi-turn spoken dialogue systems utilizing heterogeneous f...
2026-02-19 18:35:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 236ms

[953/1822] multi-agent conversational bandit framework for online large...
2026-02-19 18:35:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 222ms

[954/1822] how to improve online evaluation and filtering of LLM respon...
2026-02-19 18:35:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 228ms

[955/1822] How to evaluate and improve long-context language models for...
2026-02-19 18:35:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 183ms

[956/1822] Improving retrieval performance in long-context LLMs through...
2026-02-19 18:35:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.564 | Time: 181ms

[957/1822] comprehensive benchmarks for evaluating multi-modal large la...
2026-02-19 18:35:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 204ms

[958/1822] research on the performance of mainstream multi-modal assist...
2026-02-19 18:35:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 245ms

[959/1822] comprehensive benchmark for evaluating large language models...
2026-02-19 18:35:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[960/1822] standardized evaluation framework for testing the performanc...
2026-02-19 18:35:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 187ms

[961/1822] How to use unlearning strategies and layer-level patching to...
2026-02-19 18:35:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.940 | Time: 235ms

[962/1822] Defending against LLM jailbreak attacks by identifying vulne...
2026-02-19 18:35:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 193ms

[963/1822] Integrating domain knowledge from large language models with...
2026-02-19 18:35:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 177ms

[964/1822] How can large language models be used to guide Bayesian opti...
2026-02-19 18:35:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 225ms

[965/1822] synthetic data generation using multi-hop reasoning on conte...
2026-02-19 18:35:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[966/1822] improving document-level fact checking and grounded factuali...
2026-02-19 18:35:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 224ms

[967/1822] How to implement ultra-low latency deep learning inference o...
2026-02-19 18:35:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 194ms

[968/1822] Optimizing lookup table based neural networks on FPGAs using...
2026-02-19 18:36:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 212ms

[969/1822] Improving Tip-Adapter for vision-language models by incorpor...
2026-02-19 18:36:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 282ms

[970/1822] Theoretical understanding of training-free few-shot CLIP ada...
2026-02-19 18:36:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 248ms

[971/1822] framework for quantifying the extent of model distillation a...
2026-02-19 18:36:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 225ms

[972/1822] how to evaluate the degree of knowledge distillation from te...
2026-02-19 18:36:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 202ms

[973/1822] Improving cross-lingual knowledge consistency in large langu...
2026-02-19 18:36:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 202ms

[974/1822] How to use self-consistent responses across different langua...
2026-02-19 18:36:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 178ms

[975/1822] How can in-context learning and retrieval-augmented generati...
2026-02-19 18:36:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.962 | Time: 210ms

[976/1822] Comparing the performance of few-shot in-context learning an...
2026-02-19 18:36:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[977/1822] How to combine slot attention with pre-trained diffusion mod...
2026-02-19 18:36:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 195ms

[978/1822] Improving compositional image generation and object discover...
2026-02-19 18:36:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 183ms

[979/1822] How effective is fine-tuning large language models for the a...
2026-02-19 18:36:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 272ms

[980/1822] Investigating the relationship between reasoning complexity ...
2026-02-19 18:36:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 230ms

[981/1822] How can multi-listwise preference optimization be used to im...
2026-02-19 18:36:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[982/1822] Finetuning protein large language models for multi-attribute...
2026-02-19 18:36:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 192ms

[983/1822] evaluating the safety and vulnerability of large audio langu...
2026-02-19 18:36:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 221ms

[984/1822] benchmarking large audio language models for safety alignmen...
2026-02-19 18:36:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[985/1822] learning-based multi-turn jailbreak attack framework for lar...
2026-02-19 18:36:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.827 | Time: 180ms

[986/1822] How can multi-turn red-teaming strategies using turn-level L...
2026-02-19 18:36:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 268ms

[987/1822] What are the primary technical challenges in developing agen...
2026-02-19 18:36:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 199ms

[988/1822] How to integrate large language models with neural graph dat...
2026-02-19 18:36:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.901 | Time: 174ms

[989/1822] structured prompt engineering frameworks for developing task...
2026-02-19 18:36:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 225ms

[990/1822] using natural language specifications to design conversation...
2026-02-19 18:36:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 223ms

[991/1822] How to use hybrid attention mechanisms and large language mo...
2026-02-19 18:36:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 176ms

[992/1822] Academic papers on combining textual statistical features wi...
2026-02-19 18:36:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 205ms

[993/1822] How to improve large language model performance by aggregati...
2026-02-19 18:36:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 215ms

[994/1822] Fine-tuning techniques for large language models to synthesi...
2026-02-19 18:36:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 255ms

[995/1822] Can Looped Transformers perform neural algorithmic reasoning...
2026-02-19 18:36:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[996/1822] Extending the neural algorithmic reasoning capabilities of L...
2026-02-19 18:36:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 171ms

[997/1822] How to improve text-to-video generation models using text em...
2026-02-19 18:36:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[998/1822] Enhancing text-to-video synthesis by identifying optimal tex...
2026-02-19 18:36:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 214ms

[999/1822] multilingual dataset for hate speech and abusive language de...
2026-02-19 18:36:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 192ms

[1000/1822] benchmarking machine learning models for hate speech classif...
2026-02-19 18:36:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.964 | Time: 198ms

[1001/1822] Bridging the gap between instruction tuning and pre-training...
2026-02-19 18:36:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.907 | Time: 170ms

[1002/1822] Using adaptive data selection and controlled rewriting of pr...
2026-02-19 18:36:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 173ms

[1003/1822] How do padding tokens in text encoders affect the image gene...
2026-02-19 18:36:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.845 | Time: 199ms

[1004/1822] Causal analysis of padding token representations in text-to-...
2026-02-19 18:36:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 215ms

[1005/1822] How does adding random punctuation to mathematical prompts a...
2026-02-19 18:36:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 276ms

[1006/1822] Evaluating the vulnerability of math-specialized large langu...
2026-02-19 18:36:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 181ms

[1007/1822] benchmarking multimodal large language models for complex ge...
2026-02-19 18:36:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 220ms

[1008/1822] agent based framework for improving MLLM performance on geol...
2026-02-19 18:36:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.544 | Time: 180ms

[1009/1822] How can adaptive projector fusion driven by user instruction...
2026-02-19 18:36:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 194ms

[1010/1822] Video large language models using instruction-based dynamic ...
2026-02-19 18:36:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 177ms

[1011/1822] how does inter-model response agreement and focal loss impro...
2026-02-19 18:36:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 275ms

[1012/1822] improving large language model calibration using auxiliary m...
2026-02-19 18:36:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 213ms

[1013/1822] How to implement black-box watermarking for retrieval augmen...
2026-02-19 18:36:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[1014/1822] Robust knowledge-based watermarking methods for LLM retrieva...
2026-02-19 18:36:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[1015/1822] How effective is ChatGPT-4o at generating WCAG compliant web...
2026-02-19 18:36:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 254ms

[1016/1822] Evaluating the utility of large language models in identifyi...
2026-02-19 18:36:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 340ms

[1017/1822] How can goal-conditioned reinforcement learning policies tra...
2026-02-19 18:36:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 185ms

[1018/1822] Achieving horizon generalization in RL through planning inva...
2026-02-19 18:36:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 204ms

[1019/1822] How can segment-level reward models improve reinforcement le...
2026-02-19 18:36:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 187ms

[1020/1822] Using dynamic text segmentation and location-aware normalize...
2026-02-19 18:36:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 205ms

[1021/1822] how to integrate AI-driven intrusion detection systems with ...
2026-02-19 18:36:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 214ms

[1022/1822] multilevel defense strategies combining artificial intellige...
2026-02-19 18:36:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 180ms

[1023/1822] how to add early exit branches to pre-trained deep neural ne...
2026-02-19 18:36:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 206ms

[1024/1822] optimizing the speed accuracy tradeoff in deep learning usin...
2026-02-19 18:36:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 181ms

[1025/1822] how to improve the robustness of large language models again...
2026-02-19 18:36:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 184ms

[1026/1822] techniques for maintaining faithful integrity in LLMs to pre...
2026-02-19 18:36:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[1027/1822] How can we evaluate the ability of large language models to ...
2026-02-19 18:36:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 193ms

[1028/1822] Benchmarks and datasets for assessing and improving the mark...
2026-02-19 18:36:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 184ms

[1029/1822] how to improve retrieval augmented generation for scientific...
2026-02-19 18:36:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 193ms

[1030/1822] utilizing contextualized graph representations and dense-spa...
2026-02-19 18:36:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 176ms

[1031/1822] How can 2D Gaussian splatting be integrated with vector quan...
2026-02-19 18:36:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 205ms

[1032/1822] Using flexible 2D Gaussian features and splatting operations...
2026-02-19 18:36:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 458ms

[1033/1822] How can overlapping messages in text-based human-AI interact...
2026-02-19 18:36:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 234ms

[1034/1822] Designing conversational AI systems that support concurrent ...
2026-02-19 18:36:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 187ms

[1035/1822] How can neural language models be used to prioritize configu...
2026-02-19 18:36:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 205ms

[1036/1822] Techniques for accelerating configuration performance bug te...
2026-02-19 18:36:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 224ms

[1037/1822] synthetic benchmarks for evaluating deductive reasoning in l...
2026-02-19 18:36:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.842 | Time: 181ms

[1038/1822] how do state of the art reasoning models perform compared to...
2026-02-19 18:36:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.931 | Time: 179ms

[1039/1822] flexible modular framework for knowledge graph retrieval aug...
2026-02-19 18:36:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[1040/1822] improving knowledge graph retrieval augmented generation by ...
2026-02-19 18:36:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 180ms

[1041/1822] evaluating theory of mind in large language models using ver...
2026-02-19 18:36:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 224ms

[1042/1822] a multi-choice question answering benchmark for assessing fi...
2026-02-19 18:36:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.885 | Time: 221ms

[1043/1822] interpretable multiple instance learning for whole slide ima...
2026-02-19 18:36:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 268ms

[1044/1822] how to use clinical concepts and vision language models for ...
2026-02-19 18:36:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 202ms

[1045/1822] how to map multimodal llm hidden states to interpretable vis...
2026-02-19 18:36:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 428ms

[1046/1822] training free method for steering multimodal large language ...
2026-02-19 18:36:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 221ms

[1047/1822] How to predict the accuracy of black-box language models by ...
2026-02-19 18:36:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 221ms

[1048/1822] Using follow-up query responses to identify misrepresented m...
2026-02-19 18:36:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 191ms

[1049/1822] How to adaptively select semantically similar translation de...
2026-02-19 18:36:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 193ms

[1050/1822] Improving neural machine translation in large language model...
2026-02-19 18:36:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 184ms

[1051/1822] How to improve Transformer attention mechanisms by incorpora...
2026-02-19 18:36:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[1052/1822] Efficient fine-tuning of foundation models using sparse GIN-...
2026-02-19 18:36:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 206ms

[1053/1822] How can transformer architectures be optimized for generaliz...
2026-02-19 18:36:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.778 | Time: 172ms

[1054/1822] Improving the out-of-distribution generalization of transfor...
2026-02-19 18:36:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 196ms

[1055/1822] How can multimodal large language models be used to help end...
2026-02-19 18:36:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 205ms

[1056/1822] Interactive systems for authoring custom AI vision sensors u...
2026-02-19 18:36:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 214ms

[1057/1822] How can multi-agent large language model frameworks be used ...
2026-02-19 18:36:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 190ms

[1058/1822] Framework for implementing collaborative LLM agents with spe...
2026-02-19 18:36:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 170ms

[1059/1822] How do language and text-to-image models exhibit religious b...
2026-02-19 18:36:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 198ms

[1060/1822] Measuring religious stereotypes in generative AI using natur...
2026-02-19 18:36:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 187ms

[1061/1822] benchmarking generative AI models versus traditional ion exc...
2026-02-19 18:36:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 278ms

[1062/1822] how to improve generative materials discovery using post-gen...
2026-02-19 18:36:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.973 | Time: 225ms

[1063/1822] large time series models with billion scale parameters incor...
2026-02-19 18:36:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[1064/1822] how to use patch convolutional embedding and human feedback ...
2026-02-19 18:36:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 190ms

[1065/1822] Hierarchical tree-structured recommendation system using ret...
2026-02-19 18:36:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 203ms

[1066/1822] How can RAG-enhanced hierarchical models improve the accurac...
2026-02-19 18:36:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 198ms

[1067/1822] How to use self-questioning techniques to reduce hallucinati...
2026-02-19 18:36:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 225ms

[1068/1822] Multi-round training framework for MLLMs that uses heuristic...
2026-02-19 18:36:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 179ms

[1069/1822] Analyzing phase transitions and emergent capabilities in lar...
2026-02-19 18:36:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 176ms

[1070/1822] How to estimate the internal dimension and parameter suffici...
2026-02-19 18:36:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 183ms

[1071/1822] How can digital twins and ray tracing be used together with ...
2026-02-19 18:36:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 240ms

[1072/1822] Using a multi-step tuning process with AI to bridge the gap ...
2026-02-19 18:36:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 241ms

[1073/1822] How can large language models be used to automatically gener...
2026-02-19 18:36:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 227ms

[1074/1822] Automating the transformation of REST APIs into AI compatibl...
2026-02-19 18:36:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 176ms

[1075/1822] robust nonlinear subspace clustering using data-driven kerne...
2026-02-19 18:36:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: -637ms

[1076/1822] how to improve kernel-based subspace clustering by learning ...
2026-02-19 18:36:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[1077/1822] Evaluation of fine-tuned GPT-4o-mini for cost-effective and ...
2026-02-19 18:36:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 258ms

[1078/1822] How can fine-tuned large language models improve de-identifi...
2026-02-19 18:36:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 190ms

[1079/1822] How to improve document retrieval accuracy using zero-shot r...
2026-02-19 18:36:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 226ms

[1080/1822] Techniques for zero-shot document re-ranking that use pre-tr...
2026-02-19 18:36:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 182ms

[1081/1822] How can natural language inference models be improved to rec...
2026-02-19 18:36:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 208ms

[1082/1822] Evaluating the performance of large language models on impli...
2026-02-19 18:36:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 332ms

[1083/1822] Research comparing the emotional variance and sentiment posi...
2026-02-19 18:36:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 261ms

[1084/1822] How does the EmoXpt framework analyze differences in sentime...
2026-02-19 18:36:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 216ms

[1085/1822] current research roadmap and challenges for advancing large ...
2026-02-19 18:36:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.910 | Time: 182ms

[1086/1822] six-layer vision framework for analyzing orchestration and v...
2026-02-19 18:36:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[1087/1822] graph-based framework for generating stealthy jailbreak prom...
2026-02-19 18:36:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.792 | Time: 203ms

[1088/1822] how can interconnected graph structures with pruning improve...
2026-02-19 18:36:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[1089/1822] How can zero-shot large language models and prompt engineeri...
2026-02-19 18:36:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.907 | Time: 223ms

[1090/1822] Effective frameworks for using LLMs to evaluate student comp...
2026-02-19 18:36:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[1091/1822] How does the AIN bilingual multimodal model achieve state-of...
2026-02-19 18:36:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[1092/1822] Development of a large multimodal model for Arabic and Engli...
2026-02-19 18:36:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 224ms

[1093/1822] mathematical framework that unifies preference optimization ...
2026-02-19 18:36:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 186ms

[1094/1822] impact of optimization objectives and explicit reward models...
2026-02-19 18:36:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 178ms

[1095/1822] how to reduce activation memory during transformer fine-tuni...
2026-02-19 18:36:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 201ms

[1096/1822] efficient fine-tuning methods for large language models that...
2026-02-19 18:36:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 167ms

[1097/1822] Bayes-optimal generalisation error for shallow two-layer neu...
2026-02-19 18:36:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 320ms

[1098/1822] Phase transition from universal to specialisation learning i...
2026-02-19 18:36:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 172ms

[1099/1822] How can RAG and chain-of-thought reasoning be used with larg...
2026-02-19 18:36:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 199ms

[1100/1822] Using RAG-based agentic LLMs and chain-of-thought prompting ...
2026-02-19 18:36:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 187ms

[1101/1822] evaluating large language models for longitudinal clinical s...
2026-02-19 18:36:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 168ms

[1102/1822] performance of retrieval augmented generation and chain of t...
2026-02-19 18:36:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 216ms

[1103/1822] How to improve large language model reasoning accuracy by co...
2026-02-19 18:36:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 189ms

[1104/1822] Research on formalizing natural language reasoning tasks for...
2026-02-19 18:36:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 185ms

[1105/1822] How to improve resource efficiency in compound AI systems us...
2026-02-19 18:36:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.913 | Time: 197ms

[1106/1822] Architectural designs for decoupling orchestration and resou...
2026-02-19 18:36:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 171ms

[1107/1822] Using pre-trained foundational vision transformers like DINO...
2026-02-19 18:36:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 251ms

[1108/1822] Machine learning of material properties from microstructures...
2026-02-19 18:36:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 214ms

[1109/1822] how to perform knowledge distillation from large transformer...
2026-02-19 18:36:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 231ms

[1110/1822] distilling large scale transformer language models into rwkv...
2026-02-19 18:36:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 218ms

[1111/1822] how to trigger hallucinations in multimodal large language m...
2026-02-19 18:36:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.910 | Time: 201ms

[1112/1822] transferable visual adversarial attacks against multimodal m...
2026-02-19 18:36:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 279ms

[1113/1822] how to combine large language models with symbolic solvers u...
2026-02-19 18:36:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.951 | Time: 194ms

[1114/1822] neurosymbolic framework for improving large language model p...
2026-02-19 18:36:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 206ms

[1115/1822] survey of AI researchers' opinions on existential risk and t...
2026-02-19 18:36:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 218ms

[1116/1822] empirical study on why AI experts disagree about catastrophi...
2026-02-19 18:36:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 178ms

[1117/1822] preference datasets and benchmarks for evaluating reward mod...
2026-02-19 18:36:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 212ms

[1118/1822] how to develop reward models for trustworthy long-context ge...
2026-02-19 18:36:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[1119/1822] How can legal concept generation and Determinantal Point Pro...
2026-02-19 18:36:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 221ms

[1120/1822] Improving legal document retrieval by augmenting query facts...
2026-02-19 18:36:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 226ms

[1121/1822] AI storytelling system using character symbol manipulation a...
2026-02-19 18:36:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 183ms

[1122/1822] How can symbolic motions from toy-playing be used to guide l...
2026-02-19 18:36:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 203ms

[1123/1822] how to optimize large language model serving for multiple se...
2026-02-19 18:36:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 174ms

[1124/1822] efficient LLM inference systems that utilize hardware-aware ...
2026-02-19 18:36:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 206ms

[1125/1822] how to use agentic frameworks and character knowledge graphs...
2026-02-19 18:36:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 270ms

[1126/1822] improving the factual accuracy of large language model summa...
2026-02-19 18:36:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 201ms

[1127/1822] How to optimize confidence thresholds in large language mode...
2026-02-19 18:36:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 224ms

[1128/1822] Probabilistic modeling of joint error distributions to tune ...
2026-02-19 18:36:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.956 | Time: 194ms

[1129/1822] Efficient LLM serving systems that co-locate latency-sensiti...
2026-02-19 18:36:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 177ms

[1130/1822] How can interference-aware scheduling and latency prediction...
2026-02-19 18:36:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[1131/1822] How to reduce communication overhead in distributed large la...
2026-02-19 18:36:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 180ms

[1132/1822] Techniques for overlapping communication with computation in...
2026-02-19 18:36:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 195ms

[1133/1822] instruction tuning for video facial expression captioning an...
2026-02-19 18:36:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 206ms

[1134/1822] how can multimodal large language models be trained to provi...
2026-02-19 18:36:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[1135/1822] How can multi-neuromodulatory systems like dopamine and nora...
2026-02-19 18:36:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 235ms

[1136/1822] Bio-inspired learning rules using multi-scale neuromodulatio...
2026-02-19 18:36:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[1137/1822] How to effectively integrate multiple vision encoders in mul...
2026-02-19 18:36:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.799 | Time: 177ms

[1138/1822] Advanced fusion strategies for hybrid multimodal large langu...
2026-02-19 18:36:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.977 | Time: 196ms

[1139/1822] What are the current state of the art techniques for early e...
2026-02-19 18:36:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.400 | NDCG@10: 0.620 | Time: 226ms

[1140/1822] Comprehensive review of methodologies using intermediate lay...
2026-02-19 18:36:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 212ms

[1141/1822] how to implement remote and automatic cognitive impairment s...
2026-02-19 18:36:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 232ms

[1142/1822] performance of large language models like DistilBERT in clas...
2026-02-19 18:36:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[1143/1822] How do large language models reflect and amplify stereotypes...
2026-02-19 18:36:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 210ms

[1144/1822] Measuring representational harms and social bias against non...
2026-02-19 18:36:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.945 | Time: 174ms

[1145/1822] how to exploit inter-iteration and intra-iteration output sp...
2026-02-19 18:36:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 196ms

[1146/1822] software-hardware co-design for diffusion models using ffn r...
2026-02-19 18:36:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 168ms

[1147/1822] how to achieve low-bit quantization of text-to-image diffusi...
2026-02-19 18:36:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 219ms

[1148/1822] addressing activation outliers and cross-attention distribut...
2026-02-19 18:36:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[1149/1822] How to combine spatial layout information and semantic text ...
2026-02-19 18:36:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 245ms

[1150/1822] A hybrid method for document segmentation using bounding box...
2026-02-19 18:36:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.979 | Time: 250ms

[1151/1822] improving visual reasoning in large vision-language models u...
2026-02-19 18:36:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 174ms

[1152/1822] DrivingVQA dataset for complex visual question answering in ...
2026-02-19 18:36:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 220ms

[1153/1822] Using explainable AI and machine learning to distinguish bet...
2026-02-19 18:36:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.818 | Time: 180ms

[1154/1822] Differentiating between multiple LLM sources using deep lear...
2026-02-19 18:36:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 201ms

[1155/1822] How can large language models be used for document-level tex...
2026-02-19 18:36:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 216ms

[1156/1822] Improving document simplification performance in large langu...
2026-02-19 18:36:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 195ms

[1157/1822] how can we evaluate the clinical appropriateness of conversa...
2026-02-19 18:36:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 248ms

[1158/1822] benchmarking large language model based psychiatric agents t...
2026-02-19 18:36:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.936 | Time: 237ms

[1159/1822] How can transformer-based models like mT5 and BanglaT5 be ap...
2026-02-19 18:36:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 216ms

[1160/1822] Fine-tuning pre-trained language models for solving Bengali ...
2026-02-19 18:36:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 293ms

[1161/1822] How to improve text-image alignment in diffusion-based style...
2026-02-19 18:36:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 173ms

[1162/1822] Recent methods for balancing textual semantics and stylistic...
2026-02-19 18:36:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.936 | Time: 178ms

[1163/1822] adversarial attacks on LLM routing systems to manipulate mod...
2026-02-19 18:36:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 206ms

[1164/1822] how can an adversary use confounder gadgets to compromise th...
2026-02-19 18:36:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 295ms

[1165/1822] A large-scale dataset of cybersecurity-specific prompts for ...
2026-02-19 18:36:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.982 | Time: 231ms

[1166/1822] Evaluating large language model security using close-ended p...
2026-02-19 18:36:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.414 | Time: 238ms

[1167/1822] How can Large Language Models be used to perform interpretab...
2026-02-19 18:36:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 195ms

[1168/1822] Using adaptive retrieval-augmented generation to bridge soci...
2026-02-19 18:36:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 169ms

[1169/1822] How can retrieval augmented generation and in-context learni...
2026-02-19 18:36:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 231ms

[1170/1822] A framework for dynamic retrieval of regional cultural value...
2026-02-19 18:36:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 229ms

[1171/1822] How does information bottleneck theory explain the internal ...
2026-02-19 18:36:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 240ms

[1172/1822] Improving large language model reasoning and inference effic...
2026-02-19 18:36:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 186ms

[1173/1822] How does data contamination in pre-training sets affect the ...
2026-02-19 18:36:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 197ms

[1174/1822] Controlled study measuring the impact of source and target d...
2026-02-19 18:36:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 172ms

[1175/1822] Why do adaptive optimizers like Adam outperform SGD in trans...
2026-02-19 18:36:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 221ms

[1176/1822] Analysis of how layer normalization placement affects gradie...
2026-02-19 18:36:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 408ms

[1177/1822] How can Generative Adversarial Networks be used to predict s...
2026-02-19 18:36:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 286ms

[1178/1822] Detecting smart grid instability and adversarial attacks usi...
2026-02-19 18:36:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 235ms

[1179/1822] how to erase NSFW concepts from text to image diffusion mode...
2026-02-19 18:36:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 184ms

[1180/1822] efficient concept erasure for diffusion models using a seman...
2026-02-19 18:36:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 179ms

[1181/1822] benchmark for evaluating the effectiveness of large language...
2026-02-19 18:36:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 275ms

[1182/1822] comparing advanced reasoning models and classical LLMs on th...
2026-02-19 18:36:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 424ms

[1183/1822] generative multimodal large language models for explicit sem...
2026-02-19 18:36:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 187ms

[1184/1822] large scale dataset and framework for training event-based v...
2026-02-19 18:36:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 193ms

[1185/1822] How can we automatically generate high-quality dialogue benc...
2026-02-19 18:36:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 219ms

[1186/1822] Using query-based subgraph retrieval and multi-stage LLM pip...
2026-02-19 18:36:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 181ms

[1187/1822] How effective are vision-language models like GPT and Gemini...
2026-02-19 18:36:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 225ms

[1188/1822] A study on using VLMs for the automated assessment of AR sce...
2026-02-19 18:36:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 195ms

[1189/1822] How to use ensemble models and committee voting strategies t...
2026-02-19 18:36:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 308ms

[1190/1822] Improving dataset distillation performance by leveraging col...
2026-02-19 18:36:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 177ms

[1191/1822] How to use ChatGPT and tag-based data analysis to generate p...
2026-02-19 18:36:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[1192/1822] A method for converting student learning behavior data into ...
2026-02-19 18:36:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 198ms

[1193/1822] retrosynthesis prediction using dual graph representations f...
2026-02-19 18:36:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 213ms

[1194/1822] how to improve retrosynthesis prediction by combining dual g...
2026-02-19 18:36:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 187ms

[1195/1822] How can reconfigurable optical circuit switching improve com...
2026-02-19 18:36:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 210ms

[1196/1822] Improving training cost efficiency of MoE models using a reg...
2026-02-19 18:36:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[1197/1822] how to use gpt models and hierarchical summarization for the...
2026-02-19 18:36:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[1198/1822] leveraging large language models and prompt engineering for ...
2026-02-19 18:36:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 204ms

[1199/1822] automated neural architecture search and compression techniq...
2026-02-19 18:36:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 185ms

[1200/1822] how to design low latency neural networks for bragg peak fin...
2026-02-19 18:36:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.951 | Time: 213ms

[1201/1822] How can large language models be used as automated simulator...
2026-02-19 18:36:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[1202/1822] Evaluating concept-based explanation methods through automat...
2026-02-19 18:36:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 210ms

[1203/1822] How to reduce hallucinations in multimodal large language mo...
2026-02-19 18:36:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.933 | Time: 184ms

[1204/1822] A post-pretraining method for improving visual representatio...
2026-02-19 18:36:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 204ms

[1205/1822] How to use multiple large language models to generate high q...
2026-02-19 18:36:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 293ms

[1206/1822] Improving scientific figure captioning by using multimodal L...
2026-02-19 18:36:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.973 | Time: 207ms

[1207/1822] How can we identify and neutralize backdoor trigger tokens i...
2026-02-19 18:36:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 222ms

[1208/1822] Defending against backdoor attacks in natural language proce...
2026-02-19 18:36:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 207ms

[1209/1822] How do large language models simplify or omit representation...
2026-02-19 18:36:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 191ms

[1210/1822] Evaluating cultural representational gaps and power inequiti...
2026-02-19 18:36:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 214ms

[1211/1822] How can semantic graphs and uncertainty propagation between ...
2026-02-19 18:36:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 229ms

[1212/1822] Improving uncertainty-based hallucination detection by model...
2026-02-19 18:36:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 222ms

[1213/1822] how to generate counterfactual examples for natural language...
2026-02-19 18:36:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 397ms

[1214/1822] framework for automatic counterfactual generation using labe...
2026-02-19 18:36:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 251ms

[1215/1822] autonomous driving framework using dual-process decision mak...
2026-02-19 18:36:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 225ms

[1216/1822] improving autonomous vehicle navigation through cognitive pe...
2026-02-19 18:36:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 176ms

[1217/1822] how to improve video large language models by integrating mu...
2026-02-19 18:36:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 219ms

[1218/1822] leveraging multiple frozen vision backbones to create unifie...
2026-02-19 18:36:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 201ms

[1219/1822] lightweight and explainable intrusion detection systems for ...
2026-02-19 18:36:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 256ms

[1220/1822] how to improve transparency and computational efficiency in ...
2026-02-19 18:36:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 188ms

[1221/1822] applying compositional diffusion models for 6 degree-of-free...
2026-02-19 18:36:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 280ms

[1222/1822] how can generative diffusion policies be used for few-shot a...
2026-02-19 18:36:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 213ms

[1223/1822] How can block-wise mixed format quantization using FP4 diale...
2026-02-19 18:36:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 183ms

[1224/1822] Fine-grained block-level quantization techniques for LLMs th...
2026-02-19 18:36:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 179ms

[1225/1822] evaluating multi-step tool use reasoning in large language m...
2026-02-19 18:36:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 185ms

[1226/1822] comparison of process supervised reward models and outcome s...
2026-02-19 18:36:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 212ms

[1227/1822] how to use hadamard rotations to mitigate activation and wei...
2026-02-19 18:36:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: -689ms

[1228/1822] low-precision fine-tuning of transformers using hadamard-ass...
2026-02-19 18:36:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 167ms

[1229/1822] How to implement efficient context pruning using sequence la...
2026-02-19 18:36:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 198ms

[1230/1822] A robust approach for combining context reranking and sequen...
2026-02-19 18:36:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.907 | Time: 182ms

[1231/1822] How can large language models and knowledge graphs be used t...
2026-02-19 18:36:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.975 | Time: 178ms

[1232/1822] Multi-branched reaction pathway search algorithm for identif...
2026-02-19 18:36:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 167ms

[1233/1822] datasets for training foundation models on linked business t...
2026-02-19 18:36:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 344ms

[1234/1822] what datasets are available for research in multi-table repr...
2026-02-19 18:36:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 225ms

[1235/1822] hybrid retrieval-augmented generation framework for universi...
2026-02-19 18:36:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 168ms

[1236/1822] effective strategies for implementing unified RAG systems in...
2026-02-19 18:36:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.822 | Time: 219ms

[1237/1822] How to achieve zero-shot vision to speech generalization in ...
2026-02-19 18:36:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 195ms

[1238/1822] Open-source omnimodal models for real-time emotional speech ...
2026-02-19 18:36:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 200ms

[1239/1822] How can large language models improve their own critique and...
2026-02-19 18:36:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 184ms

[1240/1822] Enhancing LLM self-critique capabilities through self-genera...
2026-02-19 18:36:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.705 | Time: 174ms

[1241/1822] How can large language models improve long-term memory in vo...
2026-02-19 18:36:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 188ms

[1242/1822] Enhancing in-car voice assistant memory by using LLMs to ext...
2026-02-19 18:36:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 196ms

[1243/1822] How to use Conditional Value at Risk in reinforcement learni...
2026-02-19 18:36:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 312ms

[1244/1822] Risk-averse fine-tuning methods for large language models to...
2026-02-19 18:36:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 181ms

[1245/1822] How to use large language models for lexicon-based text embe...
2026-02-19 18:36:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.913 | Time: 188ms

[1246/1822] Improving sparse or lexicon-based embedding performance on t...
2026-02-19 18:36:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 193ms

[1247/1822] How can domain-adversarial fine-tuning improve the generaliz...
2026-02-19 18:36:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 217ms

[1248/1822] Enhancing small language model chain of thought performance ...
2026-02-19 18:36:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 168ms

[1249/1822] using large language models for automated regression test ge...
2026-02-19 18:36:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 203ms

[1250/1822] feedback directed zero shot LLM approach for generating repr...
2026-02-19 18:36:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 196ms

[1251/1822] How to address influence score bias in data selection for in...
2026-02-19 18:36:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 227ms

[1252/1822] Balanced and influential data selection techniques for instr...
2026-02-19 18:36:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 232ms

[1253/1822] how to use large language models for knowledge graph complet...
2026-02-19 18:36:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 265ms

[1254/1822] automated curriculum modeling and topic extraction from lect...
2026-02-19 18:36:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 193ms

[1255/1822] applying large language models to few-shot multivariate time...
2026-02-19 18:36:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.852 | Time: 198ms

[1256/1822] how to leverage pre-trained large language models for few-sh...
2026-02-19 18:36:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.962 | Time: 190ms

[1257/1822] Evaluating the effectiveness of open-source and commercial l...
2026-02-19 18:36:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.951 | Time: 242ms

[1258/1822] How do large language models compare to human teaching assis...
2026-02-19 18:36:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 171ms

[1259/1822] evaluating the fidelity and constraint satisfaction of large...
2026-02-19 18:37:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.556 | Time: 431ms

[1260/1822] systematic framework and error-correction mechanisms for ass...
2026-02-19 18:37:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 217ms

[1261/1822] how can large language models perform zero-shot image and au...
2026-02-19 18:37:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 194ms

[1262/1822] a training-free approach using gradient-free optimization an...
2026-02-19 18:37:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[1263/1822] methods for separating motion concepts from appearance in te...
2026-02-19 18:37:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.414 | Time: 180ms

[1264/1822] using temporal attention purification and appearance highway...
2026-02-19 18:37:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[1265/1822] How can chain-of-thought reasoning be applied to improve lar...
2026-02-19 18:37:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 180ms

[1266/1822] Using proactive intent analysis and reasoning steps to enhan...
2026-02-19 18:37:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 224ms

[1267/1822] fine-tuning large language models using ensembles of LoRA ex...
2026-02-19 18:37:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 175ms

[1268/1822] how can clustering training data by gradient directions and ...
2026-02-19 18:37:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 217ms

[1269/1822] How can trie-based prefix tree structures be used to optimiz...
2026-02-19 18:37:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 311ms

[1270/1822] Efficient beam search algorithms for LLMs that use shared pr...
2026-02-19 18:37:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 169ms

[1271/1822] how effective are large language models at extracting specie...
2026-02-19 18:37:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[1272/1822] using large language models for automated extraction of spec...
2026-02-19 18:37:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 182ms

[1273/1822] limitations of current large language model cybersecurity ev...
2026-02-19 18:37:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 244ms

[1274/1822] comprehensive risk assessment framework for LLM cyber capabi...
2026-02-19 18:37:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 207ms

[1275/1822] How do one-layer attention-only transformers develop interna...
2026-02-19 18:37:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 202ms

[1276/1822] Understanding how training data features shape the internal ...
2026-02-19 18:37:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 216ms

[1277/1822] How to use preference optimization to intrinsically reduce h...
2026-02-19 18:37:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 197ms

[1278/1822] Improving the reliability of LLM machine translation by fine...
2026-02-19 18:37:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 203ms

[1279/1822] comparing the performance of LSTM deep learning models and A...
2026-02-19 18:37:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 186ms

[1280/1822] how do Long Short-Term Memory networks compare to traditiona...
2026-02-19 18:37:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 167ms

[1281/1822] How does the integration of generative AI tools and experien...
2026-02-19 18:37:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 249ms

[1282/1822] Research on human-AI collaboration in business education foc...
2026-02-19 18:37:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 214ms

[1283/1822] How can multimodal large language models perform GUI groundi...
2026-02-19 18:37:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 233ms

[1284/1822] State of the art methods for visual GUI agent grounding and ...
2026-02-19 18:37:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 199ms

[1285/1822] How to improve the safety of large language model reward mod...
2026-02-19 18:37:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 218ms

[1286/1822] Dynamic selection of the most important safety rules to maxi...
2026-02-19 18:37:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.911 | Time: 276ms

[1287/1822] How to optimize the order of in-context learning examples fo...
2026-02-19 18:37:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 210ms

[1288/1822] Dataset-free methods for finding the best sequence of few-sh...
2026-02-19 18:37:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 210ms

[1289/1822] systematic hyperparameter tuning for LLM-as-a-judge using mu...
2026-02-19 18:37:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 214ms

[1290/1822] how to find cost-efficient open-weight LLM judges through mu...
2026-02-19 18:37:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[1291/1822] how to integrate domain specific large language models into ...
2026-02-19 18:37:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.885 | Time: 201ms

[1292/1822] leveraging a suite of general and specialized LLMs within an...
2026-02-19 18:37:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 205ms

[1293/1822] Evaluating large language models and artificial intelligence...
2026-02-19 18:37:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 251ms

[1294/1822] Implications of AI personhood and moral status for the ethic...
2026-02-19 18:37:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 215ms

[1295/1822] how to improve large language model reasoning and explainabi...
2026-02-19 18:37:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 197ms

[1296/1822] using selective tree exploration and supervised fine tuning ...
2026-02-19 18:37:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.727 | Time: 170ms

[1297/1822] how to perform open-set test-time adaptation for multimodal ...
2026-02-19 18:37:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 235ms

[1298/1822] robust multimodal test-time adaptation methods for identifyi...
2026-02-19 18:37:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 234ms

[1299/1822] evaluating the correlation between cross-attention weights i...
2026-02-19 18:37:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 303ms

[1300/1822] investigating the plausibility of using cross-attention weig...
2026-02-19 18:37:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 225ms

[1301/1822] How can specific attention heads in LLMs be used for trainin...
2026-02-19 18:37:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.941 | Time: 212ms

[1302/1822] Techniques for improving large language model performance on...
2026-02-19 18:37:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 260ms

[1303/1822] How can Mixture-of-Experts models be improved by removing ro...
2026-02-19 18:37:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 237ms

[1304/1822] Research on expert self-selection mechanisms in language mod...
2026-02-19 18:37:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 229ms

[1305/1822] How to evaluate fact-conflicting hallucinations in small lan...
2026-02-19 18:37:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 225ms

[1306/1822] Benchmarking the factual analysis and context reasoning capa...
2026-02-19 18:37:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.726 | Time: 189ms

[1307/1822] using direct preference optimization to generate multiple ch...
2026-02-19 18:37:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 221ms

[1308/1822] training a model to predict student choices for generating m...
2026-02-19 18:37:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 247ms

[1309/1822] How can large language models be jailbroken using positive s...
2026-02-19 18:37:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.739 | Time: 244ms

[1310/1822] Effective jailbreak attack methods for large language models...
2026-02-19 18:37:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.711 | Time: 195ms

[1311/1822] benchmark for text-driven image editing evaluation with huma...
2026-02-19 18:37:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 201ms

[1312/1822] multi-modality source-aware quality assessment metric for ev...
2026-02-19 18:37:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 259ms

[1313/1822] How can Pointwise V-Information based fine-tuning improve th...
2026-02-19 18:37:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 179ms

[1314/1822] A comprehensive dataset and evaluation framework for trainin...
2026-02-19 18:37:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 240ms

[1315/1822] Scalable signature-based algorithm for path-dependent hedgin...
2026-02-19 18:37:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 263ms

[1316/1822] Mathematical framework for deep hedging alternatives using u...
2026-02-19 18:37:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[1317/1822] alternative memory architectures for AI inference that optim...
2026-02-19 18:37:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 212ms

[1318/1822] how does managed-retention memory improve energy efficiency ...
2026-02-19 18:37:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[1319/1822] How to improve short text classification using graph learnin...
2026-02-19 18:37:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 184ms

[1320/1822] Novel methods for addressing semantic sparsity in short text...
2026-02-19 18:37:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 234ms

[1321/1822] How to use class-specific visual prompts to improve the inte...
2026-02-19 18:37:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 245ms

[1322/1822] Improving saliency maps in Vision Transformers for fine-grai...
2026-02-19 18:37:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 200ms

[1323/1822] How can an autonomous multi-agent framework using dynamic ro...
2026-02-19 18:37:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 193ms

[1324/1822] Large language model based multi-agent systems for psycholog...
2026-02-19 18:37:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.200 | P@5: 0.200 | NDCG@10: 0.387 | Time: 220ms

[1325/1822] iterative reinforced fine tuning using Monte Carlo Tree Sear...
2026-02-19 18:37:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 251ms

[1326/1822] addressing fragment deficiency and parameter errors in LLM t...
2026-02-19 18:37:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 184ms

[1327/1822] how to use large language models and graph embedding techniq...
2026-02-19 18:37:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 208ms

[1328/1822] fine-tuning pre-trained large language models with graph and...
2026-02-19 18:37:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 220ms

[1329/1822] architectural methods for overlapping communication and comp...
2026-02-19 18:37:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.973 | Time: 183ms

[1330/1822] improving distributed transformer inference speed by decoupl...
2026-02-19 18:37:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 226ms

[1331/1822] how to improve large language model performance on complex c...
2026-02-19 18:37:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 167ms

[1332/1822] effective agentic frameworks for overcoming reasoning and lo...
2026-02-19 18:37:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 243ms

[1333/1822] How do large language models perform in detecting smart cont...
2026-02-19 18:37:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 189ms

[1334/1822] Reducing false positive rates in LLM-based smart contract se...
2026-02-19 18:37:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.832 | Time: 193ms

[1335/1822] How to use model weight averaging during sequential fine-tun...
2026-02-19 18:37:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 185ms

[1336/1822] Mitigating forgetting in diverse domain continual learning b...
2026-02-19 18:37:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 231ms

[1337/1822] How can influence functions be used to identify labeler bias...
2026-02-19 18:37:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 198ms

[1338/1822] Efficient methods for measuring the impact of specific human...
2026-02-19 18:37:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 193ms

[1339/1822] comparing the performance of large language models and encod...
2026-02-19 18:37:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 225ms

[1340/1822] how well do large language models perform at segment-level q...
2026-02-19 18:37:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 227ms

[1341/1822] comprehensive evaluation framework for assessing large langu...
2026-02-19 18:37:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 207ms

[1342/1822] how to benchmark financial large language models on professi...
2026-02-19 18:37:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.949 | Time: 173ms

[1343/1822] systematic mapping study on ethical risks and mitigation str...
2026-02-19 18:37:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 240ms

[1344/1822] current challenges and frameworks for mitigating ethical con...
2026-02-19 18:37:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 184ms

[1345/1822] Open source Python library for evaluating bias and fairness ...
2026-02-19 18:37:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 231ms

[1346/1822] How can I assess algorithmic bias in LLM responses using an ...
2026-02-19 18:37:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[1347/1822] How to use visual Hopfield networks and associative memory t...
2026-02-19 18:37:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 221ms

[1348/1822] Improving LLM-based medical report generation by mining dise...
2026-02-19 18:37:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 176ms

[1349/1822] Using zero-shot prompting with open-source large language mo...
2026-02-19 18:37:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.932 | Time: 242ms

[1350/1822] Evaluation of large language models for scoring transcribed ...
2026-02-19 18:37:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.943 | Time: 194ms

[1351/1822] How to improve the inference efficiency of LLM-based recomme...
2026-02-19 18:37:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 176ms

[1352/1822] Optimizing accuracy and latency in large language model reco...
2026-02-19 18:37:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 213ms

[1353/1822] How can explicit visual prompts like markers and pointers be...
2026-02-19 18:37:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 236ms

[1354/1822] Framework for integrating medical entity extraction and visu...
2026-02-19 18:37:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 671ms

[1355/1822] How can model predictive control frameworks improve the plan...
2026-02-19 18:37:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 233ms

[1356/1822] Using large language models as implicit cost function minimi...
2026-02-19 18:37:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[1357/1822] How does transfer learning work in variational quantum circu...
2026-02-19 18:37:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 200ms

[1358/1822] Analytical fine-tuning methods for adapting pretrained varia...
2026-02-19 18:37:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[1359/1822] benchmarks for evaluating multimodal large language models o...
2026-02-19 18:37:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 191ms

[1360/1822] large scale dataset for cross-event and within-event reasoni...
2026-02-19 18:37:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 192ms

[1361/1822] How can image-based multimodal large language models be used...
2026-02-19 18:37:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 287ms

[1362/1822] Techniques for improving the black-box transferability of ad...
2026-02-19 18:37:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 213ms

[1363/1822] comprehensive review of methodologies for applying large lan...
2026-02-19 18:37:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 200ms

[1364/1822] what are the differences between graph2text and graph2token ...
2026-02-19 18:37:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[1365/1822] How can token-level uncertainty and attention mechanisms be ...
2026-02-19 18:37:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 182ms

[1366/1822] Lightweight decoding strategies for improving faithfulness t...
2026-02-19 18:37:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.423 | Time: 239ms

[1367/1822] impact of explicit symmetry breaking and geometric reference...
2026-02-19 18:37:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 368ms

[1368/1822] benchmarking trade-offs between group equivariance and symme...
2026-02-19 18:37:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.901 | Time: 274ms

[1369/1822] How can the IDADP framework help large language models detec...
2026-02-19 18:37:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 190ms

[1370/1822] Effective prompting strategies for zero-shot irony comprehen...
2026-02-19 18:37:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 288ms

[1371/1822] How do large language models compare to humans in open-ended...
2026-02-19 18:37:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 257ms

[1372/1822] Analyzing the limitations of LLM exploration through sparse ...
2026-02-19 18:37:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 225ms

[1373/1822] Systematization of knowledge on security vulnerabilities and...
2026-02-19 18:37:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 185ms

[1374/1822] A comprehensive study on the risks of code hallucinations an...
2026-02-19 18:37:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 228ms

[1375/1822] evaluating the fairness and robustness of commercial AI safe...
2026-02-19 18:37:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 209ms

[1376/1822] how do safety moderation classifiers used as LLM guardrails ...
2026-02-19 18:37:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.601 | Time: -753ms

[1377/1822] how to improve multimodal language models for mental health ...
2026-02-19 18:37:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 236ms

[1378/1822] multimodal framework for fine grained anxiety symptom detect...
2026-02-19 18:37:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 224ms

[1379/1822] enhancing retrieval augmented generation for complex reasoni...
2026-02-19 18:37:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 260ms

[1380/1822] how to improve agentic RAG performance with self-consistency...
2026-02-19 18:37:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.491 | Time: 302ms

[1381/1822] how can multimodal large language models use image represent...
2026-02-19 18:37:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 190ms

[1382/1822] using spatial intelligence of multimodal LLMs and visual gra...
2026-02-19 18:37:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 184ms

[1383/1822] how to use large language models and retrieval augmented gen...
2026-02-19 18:37:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 186ms

[1384/1822] enhancing automated short answer grading reliability with RA...
2026-02-19 18:37:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 210ms

[1385/1822] How to improve membership inference attack accuracy in large...
2026-02-19 18:37:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 258ms

[1386/1822] Detecting pretraining data leakage in large language models ...
2026-02-19 18:37:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[1387/1822] How can lightweight models be used for evidence extraction t...
2026-02-19 18:37:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 165ms

[1388/1822] Comparison of quote-first-then-answer strategies versus full...
2026-02-19 18:37:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 299ms

[1389/1822] meta-learning for parameter initialization in variational qu...
2026-02-19 18:37:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 190ms

[1390/1822] how to apply MAML-based classical neural networks for findin...
2026-02-19 18:37:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 209ms

[1391/1822] LLM-based framework for automated scientific research using ...
2026-02-19 18:37:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 201ms

[1392/1822] How to automate the scientific research process through iter...
2026-02-19 18:37:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[1393/1822] CharToM benchmark for evaluating theory of mind in large lan...
2026-02-19 18:37:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.946 | Time: 203ms

[1394/1822] Comparing human and AI theory of mind performance when reaso...
2026-02-19 18:37:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 256ms

[1395/1822] Theoretical analysis of diffusion models for nonparametric d...
2026-02-19 18:37:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 192ms

[1396/1822] How do sparse weight-sharing neural network architectures in...
2026-02-19 18:37:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 203ms

[1397/1822] Improving large language model reasoning performance through...
2026-02-19 18:37:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.944 | Time: 212ms

[1398/1822] How can recursive sub-task breakdown and advanced scoring me...
2026-02-19 18:37:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[1399/1822] Implementation of Retrieval-Augmented Generation using BGE-M...
2026-02-19 18:37:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 209ms

[1400/1822] Evaluating localized RAG systems for data privacy and perfor...
2026-02-19 18:37:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 217ms

[1401/1822] iterative pruning methods for diffusion models using gradien...
2026-02-19 18:37:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 177ms

[1402/1822] how to apply progressive soft pruning and gradient flow crit...
2026-02-19 18:37:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 209ms

[1403/1822] large scale dataset and benchmark for fake news detection in...
2026-02-19 18:37:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 269ms

[1404/1822] performance of large language models with quantized low rank...
2026-02-19 18:37:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 210ms

[1405/1822] evaluating large language models on aerospace manufacturing ...
2026-02-19 18:37:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 192ms

[1406/1822] assessing the accuracy and hallucination risks of GPT-4 and ...
2026-02-19 18:37:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[1407/1822] Improving multimodal hierarchical classification accuracy by...
2026-02-19 18:37:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 213ms

[1408/1822] How to integrate hierarchical class relationships into multi...
2026-02-19 18:37:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 210ms

[1409/1822] evaluating the performance of large language models for ment...
2026-02-19 18:37:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.710 | Time: 189ms

[1410/1822] benchmarking multilingual and bilingual LLMs on Arabic menta...
2026-02-19 18:37:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 193ms

[1411/1822] How to use a proximal operator and local correlation regular...
2026-02-19 18:37:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 334ms

[1412/1822] Efficient methods for inducing structured 2:4 sparsity in ne...
2026-02-19 18:37:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 253ms

[1413/1822] How do task-in-prompt adversarial attacks use sequence-to-se...
2026-02-19 18:37:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 222ms

[1414/1822] Evaluating LLM jailbreak vulnerabilities using the PHRYGE be...
2026-02-19 18:37:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.525 | Time: 174ms

[1415/1822] consensus-based optimization methods for derivative-free non...
2026-02-19 18:37:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 217ms

[1416/1822] how to apply mirror maps and bregman distances to particle-b...
2026-02-19 18:37:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 196ms

[1417/1822] How to use open-source large language models for automatic d...
2026-02-19 18:37:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 204ms

[1418/1822] Improving zero-shot classification performance of open-sourc...
2026-02-19 18:37:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.989 | Time: 192ms

[1419/1822] How can privacy guardrails like OneShield be used to detect ...
2026-02-19 18:37:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 209ms

[1420/1822] Effective frameworks for mitigating privacy risks in enterpr...
2026-02-19 18:37:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 211ms

[1421/1822] How can specialized datasets like PlanGTG with reordering an...
2026-02-19 18:37:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 225ms

[1422/1822] Improving graph to text generation in large language models ...
2026-02-19 18:37:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.978 | Time: 197ms

[1423/1822] How to use Z-order curves and dimensionality reduction to en...
2026-02-19 18:37:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 335ms

[1424/1822] Efficient top-k attention mechanisms for causal transformers...
2026-02-19 18:37:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 303ms

[1425/1822] How can semantic analysis and natural language processing be...
2026-02-19 18:37:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 191ms

[1426/1822] A computational architecture for linking systematic analytic...
2026-02-19 18:37:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 231ms

[1427/1822] Scaling in-context reinforcement learning for cross-domain a...
2026-02-19 18:37:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 218ms

[1428/1822] How does algorithm distillation compare to expert distillati...
2026-02-19 18:37:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 217ms

[1429/1822] generating realistic limit order book simulations using tran...
2026-02-19 18:37:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 212ms

[1430/1822] how to evaluate the realism of synthetic market data generat...
2026-02-19 18:37:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 265ms

[1431/1822] How to use embedding-based data perturbation and Tsetlin Mac...
2026-02-19 18:37:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 311ms

[1432/1822] Improving adversarial attacks on text detectors by combining...
2026-02-19 18:37:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.891 | Time: 233ms

[1433/1822] convergence rate of probability flow ODE samplers in diffusi...
2026-02-19 18:37:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[1434/1822] how do probability flow ODEs in score-based generative model...
2026-02-19 18:37:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 193ms

[1435/1822] Automated black-box safety testing of large language models ...
2026-02-19 18:37:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.839 | Time: 241ms

[1436/1822] How can LLMs be used as test oracles to identify harmful res...
2026-02-19 18:37:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.618 | Time: 227ms

[1437/1822] How can hierarchical contrastive learning and concept memori...
2026-02-19 18:37:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 210ms

[1438/1822] Improving the accuracy of generative language models in pred...
2026-02-19 18:37:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.977 | Time: 203ms

[1439/1822] information theoretic framework for multi-bit watermarking i...
2026-02-19 18:37:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 218ms

[1440/1822] optimal schemes for distributional information embedding to ...
2026-02-19 18:37:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 200ms

[1441/1822] Can fine-tuned large language models effectively control spa...
2026-02-19 18:37:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.971 | Time: 187ms

[1442/1822] Comparison of fine-tuned foundation models and traditional d...
2026-02-19 18:37:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 220ms

[1443/1822] hybrid machine learning and biophysical models for predictin...
2026-02-19 18:37:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 199ms

[1444/1822] combining neural networks with physiological models to impro...
2026-02-19 18:37:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 199ms

[1445/1822] How to finetune large language models using a diffusion fram...
2026-02-19 18:37:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 198ms

[1446/1822] Integrating diffusion processes into autoregressive models f...
2026-02-19 18:37:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 195ms

[1447/1822] datasets and benchmarks for fine grained span level chinese ...
2026-02-19 18:37:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 217ms

[1448/1822] evaluating the performance of large language models on ident...
2026-02-19 18:37:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 202ms

[1449/1822] how to apply negative feedback mechanisms from control theor...
2026-02-19 18:37:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 235ms

[1450/1822] efficient weight-only quantization for large language models...
2026-02-19 18:37:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 188ms

[1451/1822] comprehensive survey of deep learning architectures and thei...
2026-02-19 18:37:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 218ms

[1452/1822] how neural networks like CNNs and autoencoders are used for ...
2026-02-19 18:37:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 182ms

[1453/1822] machine learning based qubit readout workflow using QICK and...
2026-02-19 18:37:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 191ms

[1454/1822] how to implement hardware efficient neural networks for sing...
2026-02-19 18:37:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 169ms

[1455/1822] Causal pre-processing methods for resolving the fairness-acc...
2026-02-19 18:37:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 227ms

[1456/1822] How can approximating a fictitious and normatively desired w...
2026-02-19 18:37:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 284ms

[1457/1822] Improving process reward modeling for mathematical reasoning...
2026-02-19 18:37:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 218ms

[1458/1822] How does hierarchical refinement and step merging affect the...
2026-02-19 18:37:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 267ms

[1459/1822] efficient methods for mitigating catastrophic forgetting in ...
2026-02-19 18:37:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.926 | Time: 207ms

[1460/1822] how to balance domain adaptation and general knowledge prese...
2026-02-19 18:37:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 203ms

[1461/1822] Does professional artistic expertise improve the quality of ...
2026-02-19 18:37:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 205ms

[1462/1822] Experimental study on the transfer of traditional artistic s...
2026-02-19 18:37:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 194ms

[1463/1822] How can generative large language models be used for ranking...
2026-02-19 18:37:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.958 | Time: 186ms

[1464/1822] Unified framework for multi-modal question answering that co...
2026-02-19 18:37:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 200ms

[1465/1822] evaluating large language models mathematical reasoning capa...
2026-02-19 18:37:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 212ms

[1466/1822] how does randomizing variables in math word problems help de...
2026-02-19 18:37:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 212ms

[1467/1822] evaluation of large language models on their ability to tran...
2026-02-19 18:37:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 245ms

[1468/1822] do current automatic evaluation metrics like BLEU and COMET ...
2026-02-19 18:37:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.965 | Time: 172ms

[1469/1822] Multi-agent deep reinforcement learning for target localizat...
2026-02-19 18:37:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 205ms

[1470/1822] Collaborative multi-agent system for radioactive source loca...
2026-02-19 18:37:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 252ms

[1471/1822] How do professional software developers perceive the readabi...
2026-02-19 18:37:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 204ms

[1472/1822] Investigating the impact of LLM-based software development a...
2026-02-19 18:37:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.798 | Time: 256ms

[1473/1822] multimodal masked autoencoder framework for denoising modula...
2026-02-19 18:37:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 173ms

[1474/1822] self-supervised pretraining methods for automatic modulation...
2026-02-19 18:37:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 197ms

[1475/1822] How can reinforcement learning and dynamic early exit strate...
2026-02-19 18:37:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[1476/1822] Frameworks for optimizing the trade-off between energy consu...
2026-02-19 18:37:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 469ms

[1477/1822] How to perform domain adaptation of Llama 3.1 for e-commerce...
2026-02-19 18:37:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 230ms

[1478/1822] Techniques for adapting large language models to the e-comme...
2026-02-19 18:37:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[1479/1822] analysis of the Gaussian distribution and statistical proper...
2026-02-19 18:37:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 171ms

[1480/1822] understanding why large foundation model weights follow Gaus...
2026-02-19 18:37:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[1481/1822] How can prompt-based Monte Carlo Tree Search with dynamic ex...
2026-02-19 18:37:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.928 | Time: 189ms

[1482/1822] Improving large model reasoning on SciEval datasets using ad...
2026-02-19 18:37:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 245ms

[1483/1822] supervised contrastive knowledge distillation for few-shot c...
2026-02-19 18:37:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 224ms

[1484/1822] how to alleviate catastrophic forgetting in class-incrementa...
2026-02-19 18:37:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 228ms

[1485/1822] How can text-driven adaptation and modality alignment be use...
2026-02-19 18:37:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 184ms

[1486/1822] Methodology for aligning image and text embeddings to perfor...
2026-02-19 18:37:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 216ms

[1487/1822] How to use weighted maximum likelihood estimation in RLHF to...
2026-02-19 18:37:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 243ms

[1488/1822] Improving reinforcement learning from human feedback by addr...
2026-02-19 18:37:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 163ms

[1489/1822] How can multimodal AI agents in augmented reality proactivel...
2026-02-19 18:37:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 290ms

[1490/1822] Determining optimal intervention timing for proactive AI ass...
2026-02-19 18:37:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 165ms

[1491/1822] benchmarking multilingual gender neutral translation capabil...
2026-02-19 18:37:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 199ms

[1492/1822] systematic evaluation of inclusive translation and gender ne...
2026-02-19 18:37:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 203ms

[1493/1822] How do large language models perform on patient data extract...
2026-02-19 18:37:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 175ms

[1494/1822] Benchmarking Llama2 and Meditron for structured clinical dat...
2026-02-19 18:37:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.990 | Time: 205ms

[1495/1822] How to improve the ability of large language models to follo...
2026-02-19 18:37:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 176ms

[1496/1822] Techniques for training large language models to satisfy mul...
2026-02-19 18:37:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[1497/1822] How can weight recompute and computational graph rearrangeme...
2026-02-19 18:37:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 242ms

[1498/1822] Efficient fine-tuning methods for sparse large language mode...
2026-02-19 18:37:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 211ms

[1499/1822] How can multimodal large language models be used for zero-sh...
2026-02-19 18:37:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 196ms

[1500/1822] Improving the alignment of multimodal LLMs with human aesthe...
2026-02-19 18:37:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 182ms

[1501/1822] How to improve text to speech for research papers with compl...
2026-02-19 18:37:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 231ms

[1502/1822] Text-to-speech systems for visually impaired researchers to ...
2026-02-19 18:37:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 188ms

[1503/1822] How to detect machine-generated academic essays in English a...
2026-02-19 18:37:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 267ms

[1504/1822] Advanced techniques for identifying AI-written academic pape...
2026-02-19 18:37:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 174ms

[1505/1822] How to improve the efficiency of tree search in large langua...
2026-02-19 18:37:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 167ms

[1506/1822] Reducing computational costs and redundancy in LLM multi-ste...
2026-02-19 18:37:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 211ms

[1507/1822] Applying two-fold curriculum learning and proximal policy op...
2026-02-19 18:37:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.986 | Time: 243ms

[1508/1822] How can curriculum learning and variational autoencoders be ...
2026-02-19 18:37:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.982 | Time: 192ms

[1509/1822] How to use video-grounded entailment tree reasoning to impro...
2026-02-19 18:37:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 228ms

[1510/1822] Frameworks for integrating entailment tree construction and ...
2026-02-19 18:37:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 179ms

[1511/1822] How can stochastic distribution embeddings and Wasserstein s...
2026-02-19 18:37:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 200ms

[1512/1822] Deep learning models for knowledge tracing that incorporate ...
2026-02-19 18:37:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 174ms

[1513/1822] How to evaluate social bias in large language models specifi...
2026-02-19 18:37:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 185ms

[1514/1822] Benchmarks and metrics for assessing fairness and social bia...
2026-02-19 18:37:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[1515/1822] How can self-knowledge distillation and logit standardizatio...
2026-02-19 18:37:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: 188ms

[1516/1822] State of the art generative dataset distillation methods foc...
2026-02-19 18:37:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 201ms

[1517/1822] benchmarking large language models on linguistic reasoning t...
2026-02-19 18:37:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 178ms

[1518/1822] evaluating the performance of LLMs on self-contained linguis...
2026-02-19 18:37:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.100 | P@5: 0.000 | NDCG@10: 0.289 | Time: 210ms

[1519/1822] How to improve large language model chain of thought reasoni...
2026-02-19 18:37:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 178ms

[1520/1822] Techniques for handling implicit or missing information in L...
2026-02-19 18:37:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.790 | Time: 240ms

[1521/1822] How can hierarchical reinforcement learning and biometric fe...
2026-02-19 18:37:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 185ms

[1522/1822] Integrating neurobiological data and multi-agent systems for...
2026-02-19 18:37:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 1228ms

[1523/1822] How to protect the copyright of hardware description languag...
2026-02-19 18:37:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 182ms

[1524/1822] Methods for embedding robust watermarks into Verilog RTL and...
2026-02-19 18:37:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: -683ms

[1525/1822] using curiosity-driven reinforcement learning to automatical...
2026-02-19 18:37:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.920 | Time: 262ms

[1526/1822] automated generation of adversarial prompts to identify bias...
2026-02-19 18:37:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 162ms

[1527/1822] how to improve x-ray prohibited item detection performance w...
2026-02-19 18:37:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 212ms

[1528/1822] data augmentation methods for robust object detection in x-r...
2026-02-19 18:37:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 177ms

[1529/1822] search-based software engineering framework for automated to...
2026-02-19 18:37:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.988 | Time: 208ms

[1530/1822] how can evolutionary algorithms and iterative prompt generat...
2026-02-19 18:37:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.800 | NDCG@10: 0.782 | Time: 175ms

[1531/1822] How to use natural language processing on corporate 10-K fil...
2026-02-19 18:37:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 226ms

[1532/1822] A data-driven methodology for measuring firm-level AI engage...
2026-02-19 18:37:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 178ms

[1533/1822] benchmark dataset for evaluating the linguistic diversity an...
2026-02-19 18:37:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 169ms

[1534/1822] how do current open-vocabulary 3D visual grounding methods p...
2026-02-19 18:37:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 231ms

[1535/1822] dynamic context-aware positional encoding for improving long...
2026-02-19 18:37:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.200 | NDCG@10: 0.631 | Time: 193ms

[1536/1822] improving transformer performance using equivariant position...
2026-02-19 18:37:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 190ms

[1537/1822] zero-shot multi-hop question answering over hybrid sources o...
2026-02-19 18:37:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 210ms

[1538/1822] how to construct a unified hybrid graph from tabular and tex...
2026-02-19 18:37:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.813 | Time: 196ms

[1539/1822] How to use hybrid static and dynamic fingerprinting techniqu...
2026-02-19 18:37:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 381ms

[1540/1822] Identifying large language models in multi-agent systems and...
2026-02-19 18:38:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.200 | NDCG@10: 0.500 | Time: 201ms

[1541/1822] Analyzing the risks of software supply chain attacks through...
2026-02-19 18:38:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 210ms

[1542/1822] Relationship between HumanEval coding benchmarks and the pro...
2026-02-19 18:38:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 214ms

[1543/1822] How to design a flexible LLM re-ranker with configurable dep...
2026-02-19 18:38:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[1544/1822] Efficient large language model re-ranking techniques for pas...
2026-02-19 18:38:00 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.984 | Time: 171ms

[1545/1822] How can projection-free algorithms be used to solve online c...
2026-02-19 18:38:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 200ms

[1546/1822] projection-free online learning policies that utilize linear...
2026-02-19 18:38:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 246ms

[1547/1822] How to detect backdoors in deep neural networks by analyzing...
2026-02-19 18:38:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 218ms

[1548/1822] Trojan scanning methods for deep learning models that work a...
2026-02-19 18:38:01 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 231ms

[1549/1822] How do open-source contributions impact Large Language Model...
2026-02-19 18:38:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 185ms

[1550/1822] A comparative analysis of open-source versus proprietary Lar...
2026-02-19 18:38:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 263ms

[1551/1822] In-context reinforcement learning for few-shot budget alloca...
2026-02-19 18:38:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 198ms

[1552/1822] How to optimize budget allocation across stages in online ad...
2026-02-19 18:38:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 221ms

[1553/1822] How to use GraphRAG and retrieve-divide-solve agent pipeline...
2026-02-19 18:38:02 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 197ms

[1554/1822] Knowledge graph based RAG framework for exploring protein-pr...
2026-02-19 18:38:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.143 | P@5: 0.000 | NDCG@10: 0.333 | Time: 197ms

[1555/1822] How to use unsupervised domain adaptation and graph-based kn...
2026-02-19 18:38:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 197ms

[1556/1822] Improving cross-modal feature representation in text-to-imag...
2026-02-19 18:38:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 257ms

[1557/1822] How to perform zero-shot verification of large language mode...
2026-02-19 18:38:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 191ms

[1558/1822] Methods for evaluating and guiding large language model reas...
2026-02-19 18:38:03 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.933 | Time: 167ms

[1559/1822] How to use diffusion models for efficient neural video compr...
2026-02-19 18:38:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 181ms

[1560/1822] Applying foundational diffusion models to video compression ...
2026-02-19 18:38:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 191ms

[1561/1822] How to use the Fisher information matrix for active learning...
2026-02-19 18:38:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 279ms

[1562/1822] Active learning strategies for sequential tasks that balance...
2026-02-19 18:38:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 179ms

[1563/1822] How can large language models be used to provide intelligent...
2026-02-19 18:38:04 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 171ms

[1564/1822] Improving IR-based bug localization through automated query ...
2026-02-19 18:38:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 173ms

[1565/1822] how to improve multi-modal large language models for expert-...
2026-02-19 18:38:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 185ms

[1566/1822] integrating vision language models with physics simulators f...
2026-02-19 18:38:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 213ms

[1567/1822] How can large language models be used to automate the TinyML...
2026-02-19 18:38:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 267ms

[1568/1822] Framework for leveraging large language models to streamline...
2026-02-19 18:38:05 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.901 | Time: 170ms

[1569/1822] graph contrastive learning for short text classification wit...
2026-02-19 18:38:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 203ms

[1570/1822] how to use multi-view text embeddings from graph learning to...
2026-02-19 18:38:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[1571/1822] How can artificial intelligence be used to identify social b...
2026-02-19 18:38:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.889 | Time: 237ms

[1572/1822] Applications of AI for social inclusion including sign langu...
2026-02-19 18:38:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 215ms

[1573/1822] evaluating the accuracy of large language models for transla...
2026-02-19 18:38:06 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 182ms

[1574/1822] benchmarking llama and gpt-4o for the automated translation ...
2026-02-19 18:38:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 206ms

[1575/1822] How to perform zero-shot cyber threat intelligence informati...
2026-02-19 18:38:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 293ms

[1576/1822] Scalable AI framework for extracting STIX compliant named en...
2026-02-19 18:38:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 191ms

[1577/1822] multi-parallel document-level translation corpus for African...
2026-02-19 18:38:07 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 197ms

[1578/1822] evaluating the performance of large language models versus N...
2026-02-19 18:38:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 177ms

[1579/1822] automated prompt engineering framework using Knowledge-Gradi...
2026-02-19 18:38:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.983 | Time: 198ms

[1580/1822] How to apply sequential optimal learning and mixed-integer o...
2026-02-19 18:38:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.929 | Time: 188ms

[1581/1822] How do different floating-point quantization parameters like...
2026-02-19 18:38:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 237ms

[1582/1822] What is the optimal bit-width and exponent-mantissa ratio fo...
2026-02-19 18:38:08 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 179ms

[1583/1822] How can the concept of applied multiplexity be used to mitig...
2026-02-19 18:38:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 218ms

[1584/1822] Using multi-agent systems to improve cultural inclusivity an...
2026-02-19 18:38:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 199ms

[1585/1822] how to improve graph retrieval augmented generation by recon...
2026-02-19 18:38:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 217ms

[1586/1822] methods for mitigating information loss in knowledge graphs ...
2026-02-19 18:38:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 201ms

[1587/1822] automated evaluation metrics for retrieval augmented generat...
2026-02-19 18:38:09 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.829 | Time: 243ms

[1588/1822] how to measure the conversational faithfulness and context r...
2026-02-19 18:38:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 195ms

[1589/1822] Large language model error patterns in mathematical word pro...
2026-02-19 18:38:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 196ms

[1590/1822] Improving mathematical reasoning in LLMs through error-aware...
2026-02-19 18:38:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 200ms

[1591/1822] impact of real-world spelling mistakes and wikipedia edit hi...
2026-02-19 18:38:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 189ms

[1592/1822] comparing the robustness of mt5 and bloom models against hum...
2026-02-19 18:38:10 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 173ms

[1593/1822] automated network configuration translation between differen...
2026-02-19 18:38:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 192ms

[1594/1822] how can large language models be used to migrate legacy netw...
2026-02-19 18:38:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 232ms

[1595/1822] how to model fine-grained time-dependent user interests from...
2026-02-19 18:38:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 194ms

[1596/1822] using time-gap-aware attention and retrieval mechanisms to c...
2026-02-19 18:38:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 193ms

[1597/1822] How to use logit-based knowledge distillation to optimize de...
2026-02-19 18:38:11 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 211ms

[1598/1822] Research on distillation frameworks for deep spiking neural ...
2026-02-19 18:38:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 281ms

[1599/1822] framework for building domain-specific AI agents using natur...
2026-02-19 18:38:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.996 | Time: 203ms

[1600/1822] improving long-horizon planning in LLM agents by integrating...
2026-02-19 18:38:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.943 | Time: 175ms

[1601/1822] Applying knowledge graph completion and relational graph att...
2026-02-19 18:38:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 185ms

[1602/1822] Knowledge graph based framework for modeling complex relatio...
2026-02-19 18:38:12 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 199ms

[1603/1822] Automated prompt optimization techniques that use task-aware...
2026-02-19 18:38:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 203ms

[1604/1822] How to implement task-referenced adaptation and multi-metric...
2026-02-19 18:38:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 210ms

[1605/1822] benefits of introducing positive friction in conversational ...
2026-02-19 18:38:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 219ms

[1606/1822] how slowing down AI interactions and adding deliberate frict...
2026-02-19 18:38:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 205ms

[1607/1822] how to use llm-based line-level filtering to improve the qua...
2026-02-19 18:38:13 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 216ms

[1608/1822] improving training data quality through fine-grained line-le...
2026-02-19 18:38:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 175ms

[1609/1822] How to improve time series reasoning in multi-modal language...
2026-02-19 18:38:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.992 | Time: 189ms

[1610/1822] Multi-modal language models for complex time series reasonin...
2026-02-19 18:38:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 214ms

[1611/1822] How can heterogeneous graph neural networks be applied to re...
2026-02-19 18:38:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 201ms

[1612/1822] Deep learning approaches for multimodal emotion prediction i...
2026-02-19 18:38:14 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 266ms

[1613/1822] How to improve Quranic question answering using cross-langua...
2026-02-19 18:38:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 200ms

[1614/1822] Cross-language strategies for addressing linguistic disparit...
2026-02-19 18:38:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 190ms

[1615/1822] How can sparse autoencoders be used to perform feature-level...
2026-02-19 18:38:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 251ms

[1616/1822] Improving LLM response consistency for paraphrased inputs us...
2026-02-19 18:38:15 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.975 | Time: 195ms

[1617/1822] How to use Partial Information Decomposition principles to q...
2026-02-19 18:38:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 272ms

[1618/1822] Decomposing causal power using the Möbius function of the re...
2026-02-19 18:38:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 253ms

[1619/1822] evaluating the reliability of membership inference attacks o...
2026-02-19 18:38:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 231ms

[1620/1822] do membership inference attacks on LLMs actually detect trai...
2026-02-19 18:38:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.950 | Time: 184ms

[1621/1822] How can retrieval-augmented generation and evidence-based me...
2026-02-19 18:38:16 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 181ms

[1622/1822] Advanced frameworks for medical LLMs that utilize evidence s...
2026-02-19 18:38:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 192ms

[1623/1822] How can self-learning agents using curriculum learning princ...
2026-02-19 18:38:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 187ms

[1624/1822] Using large language model agents and iterative exploration ...
2026-02-19 18:38:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 196ms

[1625/1822] How do multimodal large language models perform when users p...
2026-02-19 18:38:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 0.818 | Time: 178ms

[1626/1822] Benchmarking the vulnerability of vision-language models to ...
2026-02-19 18:38:17 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 192ms

[1627/1822] How does implementing a retrieval-augmented generation frame...
2026-02-19 18:38:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 216ms

[1628/1822] Evaluating the performance of retriever and generative model...
2026-02-19 18:38:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 219ms

[1629/1822] multilingual end-to-end speech recognition using mixture of ...
2026-02-19 18:38:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 225ms

[1630/1822] how to improve LID-based routers in MoE architectures for mu...
2026-02-19 18:38:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 163ms

[1631/1822] How to prevent explicit content generation in text-to-image ...
2026-02-19 18:38:18 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 216ms

[1632/1822] Research on using embedding space distortion to defend again...
2026-02-19 18:38:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 185ms

[1633/1822] research on federated multimodal instruction tuning framewor...
2026-02-19 18:38:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 200ms

[1634/1822] how to use mixture of adapters and adaptive parameter aggreg...
2026-02-19 18:38:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 186ms

[1635/1822] deep learning approaches for idiom detection in Sorani Kurdi...
2026-02-19 18:38:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 192ms

[1636/1822] evaluating the performance of transformer models for identif...
2026-02-19 18:38:19 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 185ms

[1637/1822] lightweight deep learning models for energy efficient weed d...
2026-02-19 18:38:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 190ms

[1638/1822] how to achieve high accuracy weed detection on low power har...
2026-02-19 18:38:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 174ms

[1639/1822] How can large multimodal language models be used to automati...
2026-02-19 18:38:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 249ms

[1640/1822] Large scale dataset and specialized multimodal vision encode...
2026-02-19 18:38:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 218ms

[1641/1822] how to implement large language models for educational manag...
2026-02-19 18:38:20 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 226ms

[1642/1822] frameworks for applying fine-tuned large language models to ...
2026-02-19 18:38:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 232ms

[1643/1822] How can metric learning with proxy anchor methods and tri-tr...
2026-02-19 18:38:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[1644/1822] Using BLIP-2 pre-trained encoders and cross-modal transforme...
2026-02-19 18:38:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.733 | Time: 211ms

[1645/1822] using low rank adaptation to finetune language models for be...
2026-02-19 18:38:21 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 214ms

[1646/1822] improving the efficiency of sparse autoencoder reconstructio...
2026-02-19 18:38:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 217ms

[1647/1822] how to design fair pricing mechanisms for LLM training data ...
2026-02-19 18:38:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 193ms

[1648/1822] economic frameworks for large language model data markets fo...
2026-02-19 18:38:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 244ms

[1649/1822] Benchmarking AI agents on scientific discovery tasks through...
2026-02-19 18:38:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 272ms

[1650/1822] How can autonomous agents be evaluated on their ability to d...
2026-02-19 18:38:22 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.997 | Time: 205ms

[1651/1822] How to use large language models to automatically synthesize...
2026-02-19 18:38:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 193ms

[1652/1822] Integrating LLM-generated heuristics into search algorithms ...
2026-02-19 18:38:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 169ms

[1653/1822] How does the length of tokenized Java code affect the accura...
2026-02-19 18:38:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 203ms

[1654/1822] Evaluating the impact of input context window size on the pe...
2026-02-19 18:38:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 214ms

[1655/1822] how to use large language models and retrieval augmented gen...
2026-02-19 18:38:23 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 181ms

[1656/1822] automated g-code generation for cnc machines using self-corr...
2026-02-19 18:38:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 218ms

[1657/1822] How can Large Language Models be used for semantic consisten...
2026-02-19 18:38:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 211ms

[1658/1822] Semi-supervised sentiment classification using entity extrac...
2026-02-19 18:38:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 220ms

[1659/1822] Combining width and depth pruning strategies for efficient s...
2026-02-19 18:38:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.922 | Time: 213ms

[1660/1822] How to perform two-stage structured pruning on LLMs by remov...
2026-02-19 18:38:24 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 212ms

[1661/1822] how to improve large language model logical reasoning using ...
2026-02-19 18:38:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 179ms

[1662/1822] enhancing llm problem solving through town hall style debate...
2026-02-19 18:38:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 187ms

[1663/1822] How can participatory design methods be used to create large...
2026-02-19 18:38:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 169ms

[1664/1822] Challenges and opportunities of developing a journalist-cont...
2026-02-19 18:38:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 186ms

[1665/1822] How can large language models be used as reference-aware cri...
2026-02-19 18:38:25 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.962 | Time: 234ms

[1666/1822] Benchmarking execution-free evaluation methods for code agen...
2026-02-19 18:38:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 182ms

[1667/1822] Performance comparison of YOLOv7 and Faster R-CNN for vision...
2026-02-19 18:38:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.990 | Time: 240ms

[1668/1822] How can deep learning models like YOLOv7 be applied to autom...
2026-02-19 18:38:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 191ms

[1669/1822] How can few-shot optimization and iterative prompt engineeri...
2026-02-19 18:38:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 181ms

[1670/1822] Fine-tuning Mistral-7B-Instruct-v0.3 for robust hallucinatio...
2026-02-19 18:38:26 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.984 | Time: 230ms

[1671/1822] using in-context learning with transformer models to detect ...
2026-02-19 18:38:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 288ms

[1672/1822] how can large language model architectures and in-context le...
2026-02-19 18:38:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[1673/1822] using large language models and retrieval augmented generati...
2026-02-19 18:38:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 205ms

[1674/1822] evaluating how interactive browser extensions and chat inter...
2026-02-19 18:38:27 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 218ms

[1675/1822] methods for 2-bit KV cache quantization in vision-language m...
2026-02-19 18:38:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 279ms

[1676/1822] how to optimize vision-language model memory consumption usi...
2026-02-19 18:38:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.949 | Time: 191ms

[1677/1822] Evaluating random forest classifiers for the detection of Na...
2026-02-19 18:38:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 189ms

[1678/1822] How can researchers develop accurate language identification...
2026-02-19 18:38:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 202ms

[1679/1822] How can analytical decomposition of first-layer attention we...
2026-02-19 18:38:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.980 | Time: 197ms

[1680/1822] Weight-based methods for analyzing how transformer models ma...
2026-02-19 18:38:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.877 | Time: -737ms

[1681/1822] Adaptive reward function exploration for action-level backdo...
2026-02-19 18:38:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 210ms

[1682/1822] How to perform stealthy backdoor attacks in continuous reinf...
2026-02-19 18:38:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 220ms

[1683/1822] How can fine-tuned large language models like LLaMA improve ...
2026-02-19 18:38:28 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 188ms

[1684/1822] Comparative evaluation of neural machine translation and fin...
2026-02-19 18:38:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[1685/1822] Limitations and biases of automated factuality metrics in ev...
2026-02-19 18:38:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.167 | P@5: 0.000 | NDCG@10: 0.356 | Time: 282ms

[1686/1822] How reliable are automated factuality evaluators at measurin...
2026-02-19 18:38:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 288ms

[1687/1822] how to optimize kv cache storage in large language models us...
2026-02-19 18:38:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.985 | Time: 199ms

[1688/1822] adaptive kv cache pruning techniques that distinguish betwee...
2026-02-19 18:38:29 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 176ms

[1689/1822] methods for converting dense large language models into mixt...
2026-02-19 18:38:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 195ms

[1690/1822] how to apply differentiable dynamic pruning to transform MLP...
2026-02-19 18:38:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 176ms

[1691/1822] How can diversity-based adaptive random testing using string...
2026-02-19 18:38:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 199ms

[1692/1822] Effective test selection and prioritization strategies for L...
2026-02-19 18:38:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 172ms

[1693/1822] How to improve retrieval-augmented medical question answerin...
2026-02-19 18:38:30 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.957 | Time: 229ms

[1694/1822] Methods for improving the accuracy of medical QA systems usi...
2026-02-19 18:38:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.880 | Time: 205ms

[1695/1822] A unified framework for general mobility trajectory modeling...
2026-02-19 18:38:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 214ms

[1696/1822] How can masked conditional diffusion models with contextual ...
2026-02-19 18:38:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 166ms

[1697/1822] How can large language models be used for few-shot harmful c...
2026-02-19 18:38:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.952 | Time: 214ms

[1698/1822] Evaluating the effectiveness of multimodal in-context learni...
2026-02-19 18:38:31 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 186ms

[1699/1822] How to improve perceptual consistency and visual sharpness i...
2026-02-19 18:38:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 338ms

[1700/1822] Perception-inspired loss functions for neural edge detection...
2026-02-19 18:38:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 183ms

[1701/1822] empirical study evaluating the performance of deep learning ...
2026-02-19 18:38:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 192ms

[1702/1822] challenges and performance degradation of deep learning appr...
2026-02-19 18:38:32 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 201ms

[1703/1822] How can large language models be used to develop autonomous ...
2026-02-19 18:38:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 206ms

[1704/1822] Large language model based proactive dialogue systems for pr...
2026-02-19 18:38:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 184ms

[1705/1822] relationship between Ehrenfeucht-Haussler rank of Boolean fu...
2026-02-19 18:38:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[1706/1822] how many Chain of Thought steps are required for a single-la...
2026-02-19 18:38:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 240ms

[1707/1822] A comprehensive analysis of statistical methodology errors i...
2026-02-19 18:38:33 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 218ms

[1708/1822] Investigating the prevalence of inadequate data analysis tec...
2026-02-19 18:38:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 191ms

[1709/1822] How can selective boosting of attention weights for local an...
2026-02-19 18:38:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 222ms

[1710/1822] Mitigating hallucinations in LVLMs by intervening in self-at...
2026-02-19 18:38:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 200ms

[1711/1822] new optimization algorithms for large language model pre-tra...
2026-02-19 18:38:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.841 | Time: 215ms

[1712/1822] improving Signum optimizer stability for GPT-2 training by i...
2026-02-19 18:38:34 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.927 | Time: 177ms

[1713/1822] How to implement adaptive PII mitigation and policy-driven m...
2026-02-19 18:38:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 223ms

[1714/1822] Advanced NLP techniques for context-aware analysis and anony...
2026-02-19 18:38:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 160ms

[1715/1822] How can node influence maximization and decoupled influence ...
2026-02-19 18:38:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 211ms

[1716/1822] A scalable graph unlearning framework that uses influence fu...
2026-02-19 18:38:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 170ms

[1717/1822] understanding the sequential learning of skills in neural ne...
2026-02-19 18:38:35 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 179ms

[1718/1822] research on the domino effect in deep learning training and ...
2026-02-19 18:38:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.994 | Time: 171ms

[1719/1822] How can large vision-language models be used to identify inc...
2026-02-19 18:38:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 192ms

[1720/1822] Using large vision-language models and reference images to v...
2026-02-19 18:38:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 175ms

[1721/1822] How to use retrieval-augmented generation and large language...
2026-02-19 18:38:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 190ms

[1722/1822] Open source RAG-based systems for automatic extraction of pr...
2026-02-19 18:38:36 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 283ms

[1723/1822] Benchmark datasets for Norwegian question answering evaluati...
2026-02-19 18:38:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 239ms

[1724/1822] Performance evaluation of language models on new Norwegian q...
2026-02-19 18:38:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 197ms

[1725/1822] How well does GPT-4o understand the ironic use of emojis com...
2026-02-19 18:38:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 227ms

[1726/1822] Comparative study of large language models and human percept...
2026-02-19 18:38:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 165ms

[1727/1822] improving direct preference optimization for large language ...
2026-02-19 18:38:37 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.941 | Time: 261ms

[1728/1822] alternative DPO methods that down-weight misranked samples a...
2026-02-19 18:38:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.600 | NDCG@10: 0.757 | Time: 213ms

[1729/1822] How does pyramid-descent visual position encoding enhance mu...
2026-02-19 18:38:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 190ms

[1730/1822] Improving vision-language model performance by using periphe...
2026-02-19 18:38:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.971 | Time: 200ms

[1731/1822] Empirical study analyzing gender and ethnicity bias in Stabl...
2026-02-19 18:38:38 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 275ms

[1732/1822] How do text-to-image generative models like Stable Diffusion...
2026-02-19 18:38:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 213ms

[1733/1822] How can 4-bit quantization reduce memory storage and speed u...
2026-02-19 18:38:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.880 | Time: 187ms

[1734/1822] Using low-precision 4-bit integer quantization to optimize t...
2026-02-19 18:38:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 193ms

[1735/1822] Effective multi-stage training strategies for bilingual neur...
2026-02-19 18:38:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.995 | Time: 165ms

[1736/1822] Developing a lightweight bilingual Islamic LLM for document ...
2026-02-19 18:38:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 191ms

[1737/1822] What are the results of a thematic analysis regarding the ad...
2026-02-19 18:38:39 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.624 | Time: 251ms

[1738/1822] Qualitative research on the ethical integration of generativ...
2026-02-19 18:38:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 180ms

[1739/1822] efficient structured pruning techniques for large language m...
2026-02-19 18:38:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 221ms

[1740/1822] how to prune large language models quickly using structured ...
2026-02-19 18:38:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 183ms

[1741/1822] How can generative AI and quantum computing be integrated in...
2026-02-19 18:38:40 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.974 | Time: 214ms

[1742/1822] Prototyping platform and dataset for 6G semantic communicati...
2026-02-19 18:38:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 187ms

[1743/1822] how to improve document-level machine translation consistenc...
2026-02-19 18:38:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 193ms

[1744/1822] using doc-guided memory and agent-based approaches to ensure...
2026-02-19 18:38:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 188ms

[1745/1822] How does instruction tuning affect the fundamental task capa...
2026-02-19 18:38:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.910 | Time: 174ms

[1746/1822] Correlation between instruction-tuned performance and base m...
2026-02-19 18:38:41 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.867 | Time: 455ms

[1747/1822] multi-modal large language models for single-cell rna sequen...
2026-02-19 18:38:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.993 | Time: 191ms

[1748/1822] ai models for single-cell analysis capable of cell type anno...
2026-02-19 18:38:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.967 | Time: 221ms

[1749/1822] natural language interface for optimization models using lar...
2026-02-19 18:38:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.967 | Time: 207ms

[1750/1822] how can large language models be used to provide counterfact...
2026-02-19 18:38:42 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 265ms

[1751/1822] How to use Large Language Models as an action selection filt...
2026-02-19 18:38:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[1752/1822] A hybrid approach combining large language model decision ma...
2026-02-19 18:38:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 200ms

[1753/1822] Integrating large language models into hierarchical planning...
2026-02-19 18:38:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.947 | Time: 221ms

[1754/1822] Standardized benchmarks and datasets for evaluating the perf...
2026-02-19 18:38:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 215ms

[1755/1822] How to improve the reasoning capabilities of mixture of expe...
2026-02-19 18:38:43 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.976 | Time: 191ms

[1756/1822] Enhancing cognitive depth in language models by facilitating...
2026-02-19 18:38:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 216ms

[1757/1822] Norwegian abstractive summarization dataset for benchmarking...
2026-02-19 18:38:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 376ms

[1758/1822] How to evaluate the performance of Norwegian large language ...
2026-02-19 18:38:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 184ms

[1759/1822] How to apply denoise diffusion models and stochastic differe...
2026-02-19 18:38:44 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.333 | P@5: 0.600 | NDCG@10: 0.644 | Time: 186ms

[1760/1822] Diffusion transformer based signal detection methods for red...
2026-02-19 18:38:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 175ms

[1761/1822] efficient elastic quantization framework for deploying large...
2026-02-19 18:38:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 173ms

[1762/1822] how to improve memory elasticity and transition granularity ...
2026-02-19 18:38:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 174ms

[1763/1822] How does the magnitude of enriched categories of texts relat...
2026-02-19 18:38:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 191ms

[1764/1822] Mathematical framework for computing magnitude homology and ...
2026-02-19 18:38:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 200ms

[1765/1822] memory efficient on-FPGA training of transformer models usin...
2026-02-19 18:38:45 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 194ms

[1766/1822] hardware accelerator design for end-to-end transformer train...
2026-02-19 18:38:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 178ms

[1767/1822] How can large language models be used to generate synthetic ...
2026-02-19 18:38:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 0.983 | Time: 190ms

[1768/1822] Effective methods for reducing gender bias in pre-trained mo...
2026-02-19 18:38:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.500 | P@5: 0.400 | NDCG@10: 0.639 | Time: 190ms

[1769/1822] Orchestration framework for large language model training us...
2026-02-19 18:38:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.250 | P@5: 0.200 | NDCG@10: 0.431 | Time: 214ms

[1770/1822] Implementing joint mining mechanisms and bilateral value sha...
2026-02-19 18:38:46 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.920 | Time: 201ms

[1771/1822] Attention-based deep learning framework for interpretable en...
2026-02-19 18:38:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 204ms

[1772/1822] How can Transformer architectures be used to predict multipl...
2026-02-19 18:38:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[1773/1822] How to combine BERT sentence embeddings and TF-IDF features ...
2026-02-19 18:38:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 281ms

[1774/1822] Effective methods for Marathi plagiarism detection using a w...
2026-02-19 18:38:47 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 203ms

[1775/1822] L2 convergence rates and stability of linear Q-learning with...
2026-02-19 18:38:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 321ms

[1776/1822] stochastic approximation analysis of linear Q-learning diver...
2026-02-19 18:38:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 183ms

[1777/1822] How can large language models be used for reference-free and...
2026-02-19 18:38:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 213ms

[1778/1822] Automated metrics for counterspeech generation evaluation us...
2026-02-19 18:38:48 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 227ms

[1779/1822] How to use large language models for active knowledge retrie...
2026-02-19 18:38:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 179ms

[1780/1822] LLM-enhanced knowledge augmentation methods that integrate e...
2026-02-19 18:38:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 224ms

[1781/1822] How do cognitive forcing functions and different explanation...
2026-02-19 18:38:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 181ms

[1782/1822] Comparing the impact of visual explanations and cognitive fo...
2026-02-19 18:38:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 224ms

[1783/1822] neurosymbolic knowledge base for environmental social and go...
2026-02-19 18:38:49 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 326ms

[1784/1822] how to extract actionable sustainability information from co...
2026-02-19 18:38:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 277ms

[1785/1822] How can large language models be used as proxies to reduce t...
2026-02-19 18:38:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.800 | NDCG@10: 1.000 | Time: 229ms

[1786/1822] Using LLM-based pipelines and DNF proper learning to acceler...
2026-02-19 18:38:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 215ms

[1787/1822] improving best-of-n sampling in large language models using ...
2026-02-19 18:38:50 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 0.991 | Time: 255ms

[1788/1822] how to use pairwise comparison and chain of thought reasonin...
2026-02-19 18:38:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.125 | P@5: 0.000 | NDCG@10: 0.315 | Time: 221ms

[1789/1822] How can masking high perplexity tokens in training data help...
2026-02-19 18:38:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 183ms

[1790/1822] The effect of LLM-generated synthetic data on reducing perfo...
2026-02-19 18:38:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.842 | Time: 439ms

[1791/1822] How to identify both latent driving forces and direct causal...
2026-02-19 18:38:51 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 197ms

[1792/1822] Causal discovery and representation learning framework for u...
2026-02-19 18:38:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 194ms

[1793/1822] How can debate and scalable oversight be used to improve wea...
2026-02-19 18:38:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 180ms

[1794/1822] Can a weak model extract trustworthy information from a stro...
2026-02-19 18:38:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 235ms

[1795/1822] How can zero-knowledge proofs be used to verify the effectiv...
2026-02-19 18:38:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 221ms

[1796/1822] Privacy-preserving verification techniques for low-rank adap...
2026-02-19 18:38:52 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 209ms

[1797/1822] How do binary decision biases and sampling methods in large ...
2026-02-19 18:38:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 238ms

[1798/1822] Investigating randomness and decision-making biases in GPT m...
2026-02-19 18:38:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 1.000 | Time: 188ms

[1799/1822] How to measure conditional feature importance using adversar...
2026-02-19 18:38:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 190ms

[1800/1822] Explainable AI methods for estimating on-manifold conditiona...
2026-02-19 18:38:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 171ms

[1801/1822] benchmarking vision language model safety using a dataset of...
2026-02-19 18:38:53 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 184ms

[1802/1822] multilingual evaluation of multimodal safety in vision langu...
2026-02-19 18:38:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 190ms

[1803/1822] How to optimize large scale machine learning training pipeli...
2026-02-19 18:38:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 314ms

[1804/1822] Techniques for improving embedding table lookups and handlin...
2026-02-19 18:38:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 209ms

[1805/1822] How does language-adaptive fine-tuning with the AfriBERTa mo...
2026-02-19 18:38:54 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.850 | Time: 347ms

[1806/1822] Evaluating the effectiveness of language-adaptive fine-tunin...
2026-02-19 18:38:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 223ms

[1807/1822] Methods for adapting decoder-only large language models to p...
2026-02-19 18:38:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 210ms

[1808/1822] How can combining bidirectional and causal attention in a ge...
2026-02-19 18:38:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 186ms

[1809/1822] Evaluating how large language models like Llama and Claude m...
2026-02-19 18:38:55 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 217ms

[1810/1822] Comparison of emotional intensity and semantic coherence bet...
2026-02-19 18:38:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 182ms

[1811/1822] How do large language models exhibit systematic provider bia...
2026-02-19 18:38:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.906 | Time: 190ms

[1812/1822] Empirical study and dataset for evaluating service provider ...
2026-02-19 18:38:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 1.000 | Time: 190ms

[1813/1822] benchmarking vision language models for error detection and ...
2026-02-19 18:38:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 198ms

[1814/1822] how well do current vision language models perform at evalua...
2026-02-19 18:38:56 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.600 | NDCG@10: 0.955 | Time: 202ms

[1815/1822] How do large language models perform on recalling notable gl...
2026-02-19 18:38:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.400 | NDCG@10: 0.899 | Time: 170ms

[1816/1822] Evaluating geographic disparities and socioeconomic correlat...
2026-02-19 18:38:57 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 950ms

[1817/1822] Automated pipeline for converting general Word documents int...
2026-02-19 18:38:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 0.200 | NDCG@10: 1.000 | Time: 197ms

[1818/1822] How to evaluate the quality of AI-generated presentations us...
2026-02-19 18:38:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 0.000 | P@5: 0.000 | NDCG@10: 0.000 | Time: 191ms

[1819/1822] trends in the use of causal inference methods and their impa...
2026-02-19 18:38:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 217ms

[1820/1822] how does causal narrative complexity and novelty in economic...
2026-02-19 18:38:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 208ms

[1821/1822] optimizing Mixture-of-Experts model inference on serverless ...
2026-02-19 18:38:58 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 190ms

[1822/1822] distributed deployment of MoE models in serverless computing...
2026-02-19 18:38:59 | INFO     | arxiv-rag.openai_embedder | OpenAI client initialized
  MRR: 1.000 | P@5: 1.000 | NDCG@10: 1.000 | Time: 196ms

OPENAI Summary:
  Avg MRR:      0.757
  Avg NDCG@5:   0.724
  Avg NDCG@10:  0.756
  Avg P@5:      0.533
  Avg P@10:     0.413
  Avg Time:     225ms

============================================================
COMPARISON SUMMARY
============================================================
Mode            MRR    NDCG@10     P@10   Time(ms)
------------------------------------------------------------
dense         0.433      0.434    0.140        440
sparse        0.772      0.762    0.436       2599
hybrid        0.614      0.681    0.370       3578
openai        0.757      0.756    0.413        225

Results saved to: results/benchmark_20260219_151112.json
