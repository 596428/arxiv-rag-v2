[
  {
    "query": "enhancing large language model reasoning capabilities through pure reinforcement learning without using human annotated chain of thought data",
    "relevant_papers": [
      "2501.12948v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using reinforcement learning to develop emergent self reflection and verification behaviors in models for mathematics and coding competitions",
    "relevant_papers": [
      "2501.12948v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement test-time scaling in large language models using budget forcing and small high-quality reasoning datasets like s1K",
    "relevant_papers": [
      "2501.19393v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving language model reasoning performance on competition math problems through test-time compute scaling and budget forcing techniques",
    "relevant_papers": [
      "2501.19393v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Scaling reinforcement learning for large language models using simplified policy optimization and long context to achieve competitive reasoning performance on math and coding benchmarks.",
    "relevant_papers": [
      "2501.12599v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to use long chain of thought training techniques to improve the performance of short chain of thought models in multimodal language tasks.",
    "relevant_papers": [
      "2501.12599v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "open-source world foundation models for physical AI development featuring video curation pipelines and pre-trained video tokenizers for digital twin simulation",
    "relevant_papers": [
      "2501.03575v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to use general-purpose world models for training physical AI policies through customized downstream application fine-tuning and digital twin environments",
    "relevant_papers": [
      "2501.03575v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can agentic search workflows and reason-in-documents modules improve the performance of large reasoning models on complex open-domain question answering tasks?",
    "relevant_papers": [
      "2501.05366v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Improving the trustworthiness of large reasoning models using dynamic retrieval-augmented generation and document refinement techniques for long-stepwise reasoning chains.",
    "relevant_papers": [
      "2501.05366v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "End-to-end native GUI agents that use vision-only screenshot perception to perform human-like interactions across different operating systems.",
    "relevant_papers": [
      "2501.12326v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How does system-2 reasoning and iterative training with reflective online traces improve the performance of autonomous GUI interaction models?",
    "relevant_papers": [
      "2501.12326v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "challenging multi-modal benchmark for evaluating large language models on expert level academic knowledge across diverse fields like mathematics and science",
    "relevant_papers": [
      "2501.14249v9"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "dataset for testing state of the art models on advanced academic questions that are difficult to answer using simple internet retrieval",
    "relevant_papers": [
      "2501.14249v9"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "survey of collaboration mechanisms and coordination protocols for large language model based multi agent systems and their framework dimensions",
    "relevant_papers": [
      "2501.06322v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "what are the key dimensions and strategies for organizing large language model agents to solve complex tasks in multi-agent systems",
    "relevant_papers": [
      "2501.06322v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to effectively use llm-as-a-judge and consensus filtering to improve data annotation for process reward models in mathematical reasoning",
    "relevant_papers": [
      "2501.07301v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "identifying and mitigating biases in best-of-n evaluation strategies for process supervision models in large language model reasoning",
    "relevant_papers": [
      "2501.07301v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving small language model math reasoning using monte carlo tree search and self-evolved process reward models without distillation",
    "relevant_papers": [
      "2501.04519v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to scale mathematical reasoning in small models through iterative self-evolution and code-augmented chain of thought data synthesis",
    "relevant_papers": [
      "2501.04519v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "autonomous LLM agent framework for end-to-end scientific discovery including literature review experimentation and automated report generation with human feedback",
    "relevant_papers": [
      "2501.04227v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "how to use large language model agents as research assistants to automate machine learning experiments and reduce the cost of scientific research",
    "relevant_papers": [
      "2501.04227v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "how to reduce the inference overhead and reasoning length of O1-like large language models without losing accuracy",
    "relevant_papers": [
      "2501.12570v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "RL-style fine-tuning for pruning reasoning steps and optimizing token budgets in complex mathematical problem-solving tasks",
    "relevant_papers": [
      "2501.12570v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive survey on reinforcement learning methods for training large language models to master complex reasoning through automated data construction and trial-and-error search",
    "relevant_papers": [
      "2501.09686v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "the impact of test-time scaling and reinforced reasoning on the development of large reasoning models similar to OpenAI o1 for complex problem solving",
    "relevant_papers": [
      "2501.09686v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "survey of autonomous AI agents in retrieval augmented generation systems for complex reasoning and dynamic multi-step tasks",
    "relevant_papers": [
      "2501.09136v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how do agentic design patterns like reflection and planning improve traditional RAG workflows for large language model applications",
    "relevant_papers": [
      "2501.09136v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to address the optimization trade-off between reconstruction quality and generation performance in high-dimensional latent diffusion models using foundation model alignment?",
    "relevant_papers": [
      "2501.01423v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Training efficient Diffusion Transformers using VA-VAE and vision foundation model alignment to achieve state-of-the-art ImageNet generation with faster convergence.",
    "relevant_papers": [
      "2501.01423v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to address underthinking and frequent thought switching in o1-like large language models to improve reasoning depth on complex mathematical tasks?",
    "relevant_papers": [
      "2501.18585v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving the performance of reasoning models by using a decoding strategy with thought switching penalty to discourage premature transitions between thoughts.",
    "relevant_papers": [
      "2501.18585v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can multimodal large language models use visual reasoning traces and image generation to improve performance on complex spatial reasoning tasks?",
    "relevant_papers": [
      "2501.07542v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving spatial reasoning in multimodal models by generating intermediate visual representations through token discrepancy loss and visualization of thought.",
    "relevant_papers": [
      "2501.07542v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to apply direct preference optimization to rectified flow models for improving motion smoothness and prompt alignment in generated videos.",
    "relevant_papers": [
      "2501.13918v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Multi-dimensional video reward models for aligning flow-based video generation with human preferences using training-time and inference-time guidance strategies.",
    "relevant_papers": [
      "2501.13918v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "benchmark for evaluating large multimodal models on expert knowledge acquisition and adaptation from multi-discipline professional videos across different cognitive stages",
    "relevant_papers": [
      "2501.13826v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "measuring knowledge gain in multimodal learning through perception comprehension and adaptation tasks with a dataset of human-annotated expert video questions",
    "relevant_papers": [
      "2501.13826v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How does the Qwen2.5-1M model achieve a one million token context window using progressive pre-training and specialized long data synthesis techniques?",
    "relevant_papers": [
      "2501.15383v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Inference optimization methods for one million token context windows including sparse attention and chunked prefill strategies in the Qwen2.5 open source framework.",
    "relevant_papers": [
      "2501.15383v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking different large language model steering methods and concept detection techniques comparing sparse autoencoders with prompting and supervised baselines",
    "relevant_papers": [
      "2501.17148v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "performance of sparse autoencoders versus rank-1 representation finetuning and prompting for fine-grained control and interpretability of transformer language model outputs",
    "relevant_papers": [
      "2501.17148v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "inference-time steering of diffusion models using Feynman-Kac framework and interacting particle resampling for reward maximization",
    "relevant_papers": [
      "2501.06848v5"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to control diffusion model generation with reward functions without fine-tuning using particle filtering and potential scores",
    "relevant_papers": [
      "2501.06848v5"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large multimodal models maintain performance while compressing visual information into a single vision token for efficient image and video understanding?",
    "relevant_papers": [
      "2501.03895v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient large multimodal model architecture that uses modality pre-fusion to reduce vision tokens and process long video sequences with low computational overhead.",
    "relevant_papers": [
      "2501.03895v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Applying chain of thought reasoning and step by step verification to improve autoregressive image generation performance using reward models",
    "relevant_papers": [
      "2501.13926v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Using the potential assessment reward model and direct preference optimization to reinforce and self correct image generation in autoregressive systems",
    "relevant_papers": [
      "2501.13926v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How does the lightning attention mechanism combined with mixture of experts enable foundation models to handle extremely long context windows of up to four million tokens?",
    "relevant_papers": [
      "2501.08313v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Scaling large vision-language and text models using efficient lightning attention and optimized parallel strategies for training on context lengths spanning millions of tokens.",
    "relevant_papers": [
      "2501.08313v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking expert-level medical reasoning and multimodal understanding in large language models with diverse clinical images and patient records",
    "relevant_papers": [
      "2501.18362v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "high difficulty medical question answering dataset with clinical reasoning and multimodal data to evaluate advanced models beyond standard benchmarks",
    "relevant_papers": [
      "2501.18362v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to protect large language models from universal jailbreak attacks using constitutional classifiers trained on synthetic data and natural language rules",
    "relevant_papers": [
      "2501.18837v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the effectiveness of constitutional classifiers against extensive red teaming and universal jailbreaks while minimizing false refusal rates in production",
    "relevant_papers": [
      "2501.18837v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating large language models on their ability to navigate deep website structures and subpages for systematic information retrieval in complex question answering",
    "relevant_papers": [
      "2501.07572v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "a multi-agent framework using an explore-critic paradigm to mimic human-like web traversal for improving retrieval-augmented generation with high-quality data",
    "relevant_papers": [
      "2501.07572v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use 3D tracking videos as control signals in diffusion models for versatile video generation tasks like camera control and motion transfer",
    "relevant_papers": [
      "2501.03847v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "A unified video diffusion framework that leverages 3D control signals to achieve temporal consistency and 3D-aware object manipulation in generated videos",
    "relevant_papers": [
      "2501.03847v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement System 2 reasoning in large language models using Meta Chain-of-Thought and process supervision to improve complex problem solving",
    "relevant_papers": [
      "2501.04682v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Training language models to perform meta-reasoning about their internal thought processes through synthetic data generation and linearized search trace fine-tuning",
    "relevant_papers": [
      "2501.04682v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "hierarchical multi-agent framework for mobile task automation using large multimodal models with self-evolving memory and long-term planning capabilities",
    "relevant_papers": [
      "2501.11733v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how can mobile agents learn from past experiences using tips and shortcuts to improve performance on complex multi-app long-horizon tasks",
    "relevant_papers": [
      "2501.11733v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmark for evaluating multi-turn conversation capabilities of frontier large language models focusing on instruction following and context allocation challenges",
    "relevant_papers": [
      "2501.17399v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "realistic multi-turn evaluation for large language models using llm-as-judge with instance-level rubrics to identify performance gaps in frontier models",
    "relevant_papers": [
      "2501.17399v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "transformer-based diffusion models for closed-loop autonomous driving planning and multi-modal behavior modeling with flexible classifier guidance for safety",
    "relevant_papers": [
      "2501.15564v2"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "achieving human-like driving behaviors and joint prediction-planning through score-based diffusion methods evaluated on large-scale datasets like nuPlan",
    "relevant_papers": [
      "2501.15564v2"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking large language model performance on competitive programming tasks using human-comparable Elo ratings and CodeForces contest problems",
    "relevant_papers": [
      "2501.01257v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to measure the reasoning and coding abilities of LLMs through a standardized benchmark based on official CodeForces platform submissions",
    "relevant_papers": [
      "2501.01257v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does graph-based retrieval-augmented generation solve the limitations of traditional flat text retrieval for specialized large language model applications?",
    "relevant_papers": [
      "2501.13958v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Systematic review of GraphRAG technical foundations including graph-structured knowledge representation and multihop reasoning for domain-specific language model customization.",
    "relevant_papers": [
      "2501.13958v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarks for evaluating expert level reasoning and domain specific knowledge in video understanding across science healthcare and engineering disciplines",
    "relevant_papers": [
      "2501.12380v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "multimodal foundation model evaluation datasets with human expert annotated reasoning rationales for complex multi discipline video understanding tasks",
    "relevant_papers": [
      "2501.12380v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to use process reward models with speculative decoding to improve the efficiency and accuracy of large language model reasoning",
    "relevant_papers": [
      "2501.19324v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "efficient large language model inference using a draft model and reward-guided biased speculative decoding for complex mathematical tasks",
    "relevant_papers": [
      "2501.19324v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking vision language models on physical world understanding across object properties, relationships, and physics-based dynamics with video data",
    "relevant_papers": [
      "2501.16411v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve physical reasoning in vision language models using specialized vision models for embodied AI and task planning",
    "relevant_papers": [
      "2501.16411v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Technical details of omni-modal models using multi-stage training strategies for integrated vision, audio, and text understanding and generation",
    "relevant_papers": [
      "2501.15368v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does the Baichuan-Audio-Tokenizer capture semantic and acoustic information for seamless integration in multimodal large language models?",
    "relevant_papers": [
      "2501.15368v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement a temporally-aware knowledge graph system for improving AI agent memory and long-term context retrieval in enterprise applications.",
    "relevant_papers": [
      "2501.13956v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarking Zep against MemGPT using the Deep Memory Retrieval and LongMemEval metrics for evaluating agentic memory performance.",
    "relevant_papers": [
      "2501.13956v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "human annotated datasets for large language model safety alignment and comprehensive hazard taxonomies for commercial guardrail development",
    "relevant_papers": [
      "2501.09004v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to train lightweight LLM safety guardrails using a hybrid human-AI jury system and diverse risk datasets",
    "relevant_papers": [
      "2501.09004v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Multimodal large language models for real-time full-duplex voice interaction with low latency and multi-stage alignment training techniques",
    "relevant_papers": [
      "2501.06282v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can multimodal LLMs achieve seamless two-way voice communication using multi-stage speech-to-text and text-to-speech alignment training?",
    "relevant_papers": [
      "2501.06282v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to synthesize agent interaction data using backward construction from environment documentation for improving LLM performance in realistic tasks?",
    "relevant_papers": [
      "2501.10893v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Data-centric framework for adapting autonomous agents to digital environments like WebArena and OSWorld without manual human annotations.",
    "relevant_papers": [
      "2501.10893v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how can reinforcement learning and oversampling for increased sampling diversity improve complex mathematical reasoning and exploration in large language models",
    "relevant_papers": [
      "2501.11651v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "strategies for enabling inference scaling in large language models to improve performance on reasoning benchmarks by increasing computation budgets without external verifiers",
    "relevant_papers": [
      "2501.11651v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can multiagent systems and independent model specialization help large language models overcome diminishing returns in autonomous self-improvement through diverse reasoning chains?",
    "relevant_papers": [
      "2501.05707v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Finetuning language models on synthetic data from multiagent interactions to improve reasoning performance and maintain model diversity across multiple rounds of training.",
    "relevant_papers": [
      "2501.05707v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "preference optimization algorithm for Thinking-LLM-as-a-Judge models using explicit evaluation plans and self-training to improve RewardBench scores",
    "relevant_papers": [
      "2501.18099v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how does separating evaluation planning from execution in chain-of-thought reasoning improve the performance of generative reward models and LLM judges",
    "relevant_papers": [
      "2501.18099v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can reasoning-based supervised fine-tuning and hard sample direct preference optimization improve the safety and explainability of LLM guardrails?",
    "relevant_papers": [
      "2501.18492v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "A large-scale dataset with detailed reasoning steps for training guard models to perform complex safety moderation across multiple safety-critical benchmarks.",
    "relevant_papers": [
      "2501.18492v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Can large language models articulate their own learned behaviors and fine-tuned tendencies like generating insecure code without specific training on self-description?",
    "relevant_papers": [
      "2501.11120v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Research on behavioral self-awareness in LLMs and whether models can detect their own backdoors or identify problematic behaviors they were trained to exhibit.",
    "relevant_papers": [
      "2501.11120v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How are large language models being used across the different stages of the scientific research cycle like hypothesis generation, experiment planning, and peer review?",
    "relevant_papers": [
      "2501.04306v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive review of the roles and methodologies of large language models in automating scientific discovery, manuscript preparation, and the academic peer review process.",
    "relevant_papers": [
      "2501.04306v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use open-source large language models for efficient GitHub issue resolution using coarse-to-fine file retrieval and code patch generation",
    "relevant_papers": [
      "2501.05040v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "State of the art open-source LLM framework for SWE-bench Lite and Verified benchmarks using a two-module approach for file retrieval and editing",
    "relevant_papers": [
      "2501.05040v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "reproducing o1 style slow-thinking in multimodal models by fine-tuning on textual long-form reasoning data",
    "relevant_papers": [
      "2501.01904v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "research on transferring long-form reasoning capabilities from language models to multimodal large language models for complex semantic tasks",
    "relevant_papers": [
      "2501.01904v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How does pre-training data influence the emergence of Chain of Thought and Plan of Thought reasoning abilities in large transformer-based language models?",
    "relevant_papers": [
      "2501.04040v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Survey of large language model architectures and scaling mechanisms with a focus on emergent reasoning, LLM-modulo frameworks, and domain-specific applications in healthcare and finance.",
    "relevant_papers": [
      "2501.04040v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does increasing inference-time compute in reasoning models like OpenAI o1 affect their robustness against different types of adversarial attacks?",
    "relevant_papers": [
      "2501.18841v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the impact of test-time compute scaling on the adversarial robustness and reliability of large language models during inference.",
    "relevant_papers": [
      "2501.18841v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve multimodal GUI agents using a two-stage fine-tuning pipeline with native reasoning and reflection capabilities?",
    "relevant_papers": [
      "2501.04575v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Developing generalist GUI agents with hierarchical reasoning and expectation-reflection skills for task automation on computers and mobile devices.",
    "relevant_papers": [
      "2501.04575v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "open source multi-modal reward models for aligning large vision language models with human preferences across text image and video inputs",
    "relevant_papers": [
      "2501.12368v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "using reward models for reinforcement learning and test-time scaling to improve instruction following in multi-modal large language models",
    "relevant_papers": [
      "2501.12368v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Recent advancements in large vision-language models for detailed video description using fine-grained temporal alignment and Direct Preference Optimization training.",
    "relevant_papers": [
      "2501.07888v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Large multi-modal models that outperform GPT-4o and Gemini 1.5 Pro in video understanding and description benchmarks through scaled pre-training.",
    "relevant_papers": [
      "2501.07888v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can Monte Carlo Tree Search be integrated with large language models to improve automatic heuristic design for complex optimization problems?",
    "relevant_papers": [
      "2501.08603v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Improving the exploration of LLM-based heuristic generation using MCTS to avoid local optima in combinatorial optimization tasks.",
    "relevant_papers": [
      "2501.08603v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "statistical methods for justifying the replacement of human annotators with large language models in research studies and data labeling",
    "relevant_papers": [
      "2501.10970v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to evaluate if an LLM is a reliable substitute for human judges using the alternative annotator test and statistical validation",
    "relevant_papers": [
      "2501.10970v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language model agents be designed to support lifelong learning and continuous adaptation while preventing catastrophic forgetting in dynamic environments?",
    "relevant_papers": [
      "2501.07278v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive survey of perception, memory, and action modules for implementing incremental and continual learning capabilities in autonomous agents based on LLMs.",
    "relevant_papers": [
      "2501.07278v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Multimodal foundation models for computational pathology pre-trained on large-scale datasets of paired H&E stained whole-slide images and molecular genomic profiles.",
    "relevant_papers": [
      "2501.16652v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How does incorporating transcriptomic data into the pre-training of pathology foundation models improve performance in predicting treatment response and rare clinical events?",
    "relevant_papers": [
      "2501.16652v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "training-free test-time alignment of diffusion models using sequential monte carlo sampling to prevent reward over-optimization and maintain sample diversity",
    "relevant_papers": [
      "2501.05803v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to optimize diffusion models for multiple reward objectives at inference time without fine-tuning or performance degradation from reward hacking",
    "relevant_papers": [
      "2501.05803v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Inference-time alignment techniques for diffusion models using Sequential Monte Carlo and value functions to maximize reward without fine-tuning",
    "relevant_papers": [
      "2501.09685v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Guide to reward-guided generation and inference-time guidance for biological protein design applications using pre-trained diffusion models",
    "relevant_papers": [
      "2501.09685v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to use multimodal large language models to convert chart images into executable code while maintaining high visual detail and accuracy?",
    "relevant_papers": [
      "2501.06598v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Multimodal LLM trained on Chart2Code-160k dataset for generating Python code from charts using Snippet-of-Thought prompting techniques.",
    "relevant_papers": [
      "2501.06598v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to use dynamic trend representation transformers and cross spatial-temporal attention for improving traffic flow prediction accuracy in urban networks",
    "relevant_papers": [
      "2501.10796v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "fusing dynamic trends and static graph attributes for traffic forecasting through spatial-temporal transformers and representation graph compression techniques",
    "relevant_papers": [
      "2501.10796v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can learnable orthogonal and scaling transformations improve data distribution fitting and bit precision during post-training quantization for large language models?",
    "relevant_papers": [
      "2501.13987v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Post-training quantization methods using Quantization Space Utilization Rate and KL-Top loss to reduce performance loss in W4A4KV4 LLaMA model compression.",
    "relevant_papers": [
      "2501.13987v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Scalable parallel transformer architecture for video diffusion models with improved text alignment and temporal coherence for high-fidelity video generation",
    "relevant_papers": [
      "2501.08453v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Memory-efficient training frameworks using hybrid parallelism for scaling text-to-video generation models on large distributed systems with high-quality datasets",
    "relevant_papers": [
      "2501.08453v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can evolutionary search strategies be used to scale inference time compute for large language models in complex natural language planning tasks?",
    "relevant_papers": [
      "2501.09891v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Using language models to generate and recombine candidate responses for improved reasoning without formal solvers on TravelPlanner and Natural Plan benchmarks.",
    "relevant_papers": [
      "2501.09891v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve low-level spatial understanding in vision-language-action models for robots using unified multi-modal understanding and future prediction training objectives",
    "relevant_papers": [
      "2501.18867v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Recent advancements in training VLA models that integrate semantic reasoning with physical dynamics prediction to enhance performance on the Calvin benchmark for embodied agents",
    "relevant_papers": [
      "2501.18867v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can self-play between a conjecturer and a prover LLM improve performance on formal theorem proving benchmarks like LeanWorkbook and miniF2F?",
    "relevant_papers": [
      "2502.00212v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Training large language models for formal mathematical verification through iterative conjecturing and expert iteration to overcome the scarcity of high-quality proof data.",
    "relevant_papers": [
      "2502.00212v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "self-supervised music representation learning models that utilize mel residual vector quantization for improved performance in downstream music informatics tasks",
    "relevant_papers": [
      "2501.01108v2"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "how to train a joint music-text embedding model using contrastive learning and predictive tokens for zero-shot music tagging applications",
    "relevant_papers": [
      "2501.01108v2"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve large language model reasoning through test-time scaling by combining parallel sampling with self-verification and self-correction without fine-tuning.",
    "relevant_papers": [
      "2501.19306v5"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient test-time computation methods for LLMs that leverage internal self-improvement capabilities to overcome saturation in repeated sampling and sequential refinement.",
    "relevant_papers": [
      "2501.19306v5"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How does scaling long chain of thought data to one million samples improve the performance of slow-thinking reasoning models on complex math and geometry benchmarks?",
    "relevant_papers": [
      "2501.11284v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the impact of long-CoT dataset scaling and reinforcement learning on the reasoning performance and sample efficiency of slow-thinking large language models.",
    "relevant_papers": [
      "2501.11284v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can chain-of-thought reasoning and spatial coordinate alignment be used to improve vision-language model performance on complex embodied tasks?",
    "relevant_papers": [
      "2501.10074v3"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "Advanced spatial reasoning in vision-language models using bi-directional coordinate alignment and grounding for robot navigation and manipulation planning.",
    "relevant_papers": [
      "2501.10074v3"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve language model reasoning by training models to critique noisy responses instead of using standard supervised fine-tuning?",
    "relevant_papers": [
      "2501.17703v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparison between critique fine-tuning and imitation learning for enhancing mathematical problem solving and instruction following in large language models.",
    "relevant_papers": [
      "2501.17703v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "What are the primary challenges and open problems in using machine unlearning to remove dual-use dangerous knowledge for AI safety?",
    "relevant_papers": [
      "2501.04952v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the limitations of machine unlearning for cybersecurity and CBRN safety risks including side effects and tensions with existing safeguards.",
    "relevant_papers": [
      "2501.04952v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can multimodal large language models use hidden latent space representations to achieve more efficient chain of thought reasoning?",
    "relevant_papers": [
      "2501.19201v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Compressing textual reasoning chains into compact thinking tokens to reduce inference costs in multimodal models while maintaining interpretability.",
    "relevant_papers": [
      "2501.19201v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "best practices and data-centric strategies for post-training vision-language models to achieve state-of-the-art results with smaller parameter counts",
    "relevant_papers": [
      "2501.14818v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "detailed implementation of post-training data strategies and training recipes for building high-performance open-source multimodal models similar to Eagle2",
    "relevant_papers": [
      "2501.14818v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarks for evaluating temporal awareness and real-time reasoning in online video large language models across different timestamps",
    "relevant_papers": [
      "2501.05510v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to evaluate the ability of video LLMs to handle incremental video streams through backward tracing and forward active responding",
    "relevant_papers": [
      "2501.05510v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language model agents be trained with reinforcement learning to perform autonomous academic paper search and reference selection?",
    "relevant_papers": [
      "2501.10120v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "LLM-based autonomous agents for comprehensive literature retrieval and paper search using synthetic datasets and reinforcement learning methods.",
    "relevant_papers": [
      "2501.10120v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "Are chain of thought explanations in reasoning models like DeepSeek R1 more faithful than those in traditional large language models?",
    "relevant_papers": [
      "2501.08156v5"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Measuring if large language models can accurately describe how misleading prompt cues influence their reasoning and final answers.",
    "relevant_papers": [
      "2501.08156v5"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking negation understanding in vision language models using NegBench with applications in image retrieval and medical datasets",
    "relevant_papers": [
      "2501.09425v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "improving CLIP model performance on negated text queries through fine-tuning with large scale synthetic datasets of negative captions",
    "relevant_papers": [
      "2501.09425v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "multimodal dataset for building damage assessment combining high resolution optical and SAR imagery for all weather disaster response applications",
    "relevant_papers": [
      "2501.06019v4"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "globally distributed dataset for training AI models in building damage assessment with very high resolution satellite images from multiple disaster types",
    "relevant_papers": [
      "2501.06019v4"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive survey of parameter-efficient fine-tuning techniques for large language and multimodal foundation models",
    "relevant_papers": [
      "2501.13787v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "recent developments and systematic review of PEFT methods across language vision and generative foundation models",
    "relevant_papers": [
      "2501.13787v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluation framework for assessing the functional correctness and security vulnerabilities of code generated by large language models using outcome-driven test oracles",
    "relevant_papers": [
      "2501.08200v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "multilingual benchmarks for measuring whether AI-generated code is both functionally correct and free from common security flaws or weaknesses",
    "relevant_papers": [
      "2501.08200v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models improve cold-start recommendation performance compared to traditional methods like graph relations or content features?",
    "relevant_papers": [
      "2501.01945v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "Survey of recent advances and future research directions in cold-start recommendation systems leveraging the power of large language models.",
    "relevant_papers": [
      "2501.01945v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How to use Monte Carlo Tree Search and iterative self-training to help language model agents recover from errors in interactive environments?",
    "relevant_papers": [
      "2501.11425v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Training language model agents to perform self-reflection and error recovery using MCTS-guided critique construction and iterative path splicing.",
    "relevant_papers": [
      "2501.11425v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to maintain safety alignment and prevent jailbreaking risks in large language models when fine-tuning with low-rank adaptation methods like LoRA",
    "relevant_papers": [
      "2501.01765v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Parameter efficient fine-tuning methods that preserve safety guardrails in LLMs using fixed safety modules and specialized initialization for trainable low-rank adapters",
    "relevant_papers": [
      "2501.01765v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the stability of features extracted by sparse autoencoders from large language models when using different random seeds for training.",
    "relevant_papers": [
      "2501.16615v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Do TopK sparse autoencoders identify consistent features across different training runs compared to standard ReLU SAE architectures?",
    "relevant_papers": [
      "2501.16615v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Scaling vision language model pretraining with massive high quality recaption data and evaluating performance gains for 2B parameter models",
    "relevant_papers": [
      "2501.05952v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How does progressively scaling SFT data quantity and complexity improve the benchmark performance of vision language models compared to standard one-stage training",
    "relevant_papers": [
      "2501.05952v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive survey of large language models used for automated peer review of scientific manuscripts including methods datasets and performance issues",
    "relevant_papers": [
      "2501.10326v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "recent advancements and challenges in using large language models for automated scholarly paper review and their impact on academic publication systems",
    "relevant_papers": [
      "2501.10326v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Interpretable machine unlearning in diffusion models using sparse autoencoders to identify and remove specific visual concepts or styles",
    "relevant_papers": [
      "2501.18052v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Applying sparse autoencoders to diffusion model activations for targeted feature intervention and robust removal of harmful content like nudity",
    "relevant_papers": [
      "2501.18052v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "interactive benchmark for evaluating multimodal large language models as embodied agents in diverse 3D simulation environments",
    "relevant_papers": [
      "2501.11858v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How do state of the art multimodal large language models perform on interactive embodied tasks like navigation and object manipulation in 3D scenes?",
    "relevant_papers": [
      "2501.11858v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive architectural framework and modular blueprint for designing large reasoning models using reinforcement learning, search heuristics, and test-time compute",
    "relevant_papers": [
      "2501.11223v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement reasoning language models using process-based supervision and modular components like x1 for rapid prototyping and experimentation",
    "relevant_papers": [
      "2501.11223v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "training-free methods for consistent character identity in text-to-image generation for visual storytelling across multiple frames or images",
    "relevant_papers": [
      "2501.13554v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to achieve consistent identity in diffusion models using a single prompt approach without additional fine-tuning or training datasets",
    "relevant_papers": [
      "2501.13554v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Recent survey on the performance and training techniques of small language models within the range of one to eight billion parameters",
    "relevant_papers": [
      "2501.05465v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparing the efficiency and scalability of task-specific small language models against massive foundation models for cost-effective AI development",
    "relevant_papers": [
      "2501.05465v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to align large language model outputs with human preferences during inference using iterative textual critiques and feedback without retraining the model parameters?",
    "relevant_papers": [
      "2501.12895v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for test-time preference optimization that use natural language reward signals and iterative refinement to improve large language model performance on the fly.",
    "relevant_papers": [
      "2501.12895v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "large scale multimodal benchmark for evaluating cultural bias in vision language models and fine-tuning techniques to improve global cultural awareness",
    "relevant_papers": [
      "2501.01282v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "fine-tuning vision language models on the CultureVerse dataset to improve recognition of cultural symbols, gestures, and artifacts from non-Western regions",
    "relevant_papers": [
      "2501.01282v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "speculative decoding techniques that use a judge model to accept high quality candidate tokens even when they do not align with target model probabilities",
    "relevant_papers": [
      "2501.19309v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to increase the inference speed of large language models like Llama 3.1 by using judge-based verification instead of standard speculative sampling alignment",
    "relevant_papers": [
      "2501.19309v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can tensor product decomposition be used to reduce KV cache memory overhead in transformers for long sequence modeling tasks?",
    "relevant_papers": [
      "2501.06425v7"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient attention mechanisms using contextual low-rank representations and tensor factorizations to improve memory scaling and performance in language models.",
    "relevant_papers": [
      "2501.06425v7"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparing the impact of conversational XAI interfaces versus dashboards on user understanding, trust, and overreliance in human-AI collaborative decision making",
    "relevant_papers": [
      "2501.17546v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How does integrating large language model agents into conversational explainable AI systems affect human overreliance and the illusion of explanatory depth",
    "relevant_papers": [
      "2501.17546v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How are large language models being applied to automate and optimize the various stages of the electronic design automation workflow according to recent surveys?",
    "relevant_papers": [
      "2501.09655v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "A review of current research on using large language models for complex dataset analysis and architectural optimizations in the field of electronic design automation.",
    "relevant_papers": [
      "2501.09655v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "rehearsal-free class incremental learning using decoupled low-rank adaptation for magnitude and direction to improve scalability in foundation models",
    "relevant_papers": [
      "2501.13198v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to achieve a stable stability-plasticity trade-off in continual learning by decoupling LoRA components without retaining samples of previous tasks",
    "relevant_papers": [
      "2501.13198v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to train large language models using FP4 quantization while maintaining accuracy comparable to BF16 or FP8 formats?",
    "relevant_papers": [
      "2501.17116v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for ultra-low precision training of LLMs using differentiable quantization estimators and outlier clamping for 4-bit floating point operations.",
    "relevant_papers": [
      "2501.17116v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can instruction tuning with environment-based self-refinement improve the generalization and robustness of large language model agents on unseen tasks?",
    "relevant_papers": [
      "2501.01702v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Training LLM agents to correct their own mistakes using environmental feedback to enhance performance on held-out benchmarks and reduce trajectory errors.",
    "relevant_papers": [
      "2501.01702v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How does inference-time scaling and extended reasoning chains impact the performance of large language models on complex medical diagnostic tasks?",
    "relevant_papers": [
      "2501.06458v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Applying journey learning and systematic clinical reasoning to improve large language model performance on benchmarks like MedQA and JAMA Clinical Challenges.",
    "relevant_papers": [
      "2501.06458v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive benchmark for evaluating large language model tool usage in multi-turn dialogues and scenarios with ambiguous instructions",
    "relevant_papers": [
      "2501.12851v8"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to assess tool-augmented large language models using categories like normal basic scenarios special incomplete instructions and multi-agent interactions",
    "relevant_papers": [
      "2501.12851v8"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how can we align time series data with linguistic logic and structure to activate deep understanding in pre-trained large language models",
    "relevant_papers": [
      "2501.03747v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "using dual scale context alignment graph neural networks to improve large language model performance on zero-shot and few-shot time series forecasting",
    "relevant_papers": [
      "2501.03747v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking large language models for factual grounding and response accuracy when provided with long-form context documents up to thirty-two thousand tokens",
    "relevant_papers": [
      "2501.03200v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "automated evaluation methods and leaderboards for testing whether language model responses are fully grounded in provided input documents of various lengths",
    "relevant_papers": [
      "2501.03200v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement retrieval augmented generation over a large video corpus using large video language models for multimodal information retrieval and response generation",
    "relevant_papers": [
      "2501.05874v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "framework for dynamic video retrieval and informative frame selection to improve the factual accuracy of large language model responses using video evidence",
    "relevant_papers": [
      "2501.05874v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking multi-turn retrieval augmented generation systems for large language models using human-generated conversational datasets and multi-domain tasks",
    "relevant_papers": [
      "2501.03468v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how do state-of-the-art RAG systems handle multi-turn conversations with non-standalone questions and unanswerable queries in a conversational setting",
    "relevant_papers": [
      "2501.03468v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can process reward models be used to improve multimodal mathematical reasoning and address reward hacking in reinforcement learning?",
    "relevant_papers": [
      "2501.04686v6"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Training multimodal large language models for mathematical reasoning using automated process-level supervision and process-supervised group-relative-policy-optimization algorithms.",
    "relevant_papers": [
      "2501.04686v6"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models use sparse autoencoders to represent shared grammatical concepts like gender and tense across diverse languages?",
    "relevant_papers": [
      "2501.06346v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Investigation into whether multilingual LLMs encode universal morphosyntactic features in shared latent directions using causal interventions and sparse autoencoders.",
    "relevant_papers": [
      "2501.06346v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "methods for scaling serial and parallel test-time compute to improve large language model performance on solving real-world GitHub issues and SWE-bench",
    "relevant_papers": [
      "2501.14723v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "using model-generated test voting and multi-turn selection trajectories to ensemble candidate code edits for software engineering tasks",
    "relevant_papers": [
      "2501.14723v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Is the performance drop in continual learning of large language models caused by actual knowledge loss or just a decline in task alignment?",
    "relevant_papers": [
      "2501.13453v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does freezing bottom layers of large language models help mitigate spurious forgetting and maintain task alignment during continual learning across different tasks?",
    "relevant_papers": [
      "2501.13453v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "comparing the safety and alignment levels of DeepSeek-R1 and OpenAI o3-mini models using the ASTRAL automated safety testing framework",
    "relevant_papers": [
      "2501.18438v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "systematic evaluation of DeepSeek-R1 and o3-mini reasoning models to determine which LLM is more likely to generate unsafe or harmful responses",
    "relevant_papers": [
      "2501.18438v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "techniques for maintaining response diversity in large language models during preference optimization without sacrificing performance on creative generation tasks",
    "relevant_papers": [
      "2501.18101v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to optimize language models to generate more diverse persona attributes and stories by selecting rare but high quality preference pairs",
    "relevant_papers": [
      "2501.18101v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to bridge the multilingual performance gap in mathematical reasoning by using English as an anchor for large language models",
    "relevant_papers": [
      "2501.02448v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving Korean language math reasoning capabilities in LLMs using the HRM8K benchmark and the Understand Solve and Translate method",
    "relevant_papers": [
      "2501.02448v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does scaling the input vocabulary size using multi-gram tokens affect the training loss and performance of transformer based large language models?",
    "relevant_papers": [
      "2501.16975v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the impact of decoupling input and output vocabularies in transformer architectures to improve efficiency and language modeling performance through larger tokenizers.",
    "relevant_papers": [
      "2501.16975v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can hierarchical memory systems be used to enable training-free streaming video understanding and multi-turn dialogue in Large Language Models?",
    "relevant_papers": [
      "2501.13468v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Frameworks for real-time video reasoning that use parallel scheduling and memory compression to support long sequences and interactive conversations.",
    "relevant_papers": [
      "2501.13468v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how does the inconsistency between comprehension and safety in multimodal models allow for jailbreak attacks through shuffled text and image instructions",
    "relevant_papers": [
      "2501.04931v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "using shuffle inconsistency and query-based black-box optimization to bypass safety filters in commercial multimodal large language models like GPT-4o",
    "relevant_papers": [
      "2501.04931v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How does the accuracy of frequent ChatGPT users compare to automated software for detecting AI-generated text from models like GPT-4o and Claude?",
    "relevant_papers": [
      "2501.15654v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Can experienced LLM users identify AI-written articles that use evasion tactics like paraphrasing and what specific text features do they look for?",
    "relevant_papers": [
      "2501.15654v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Theoretical analysis of gradient descent dynamics and loss trajectories for in-context linear regression in linear self-attention models.",
    "relevant_papers": [
      "2501.16265v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does the parametrization of key and query weights affect the acquisition of in-context learning abilities during training?",
    "relevant_papers": [
      "2501.16265v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does the level of sparsity in mixture-of-experts models affect scaling laws and the trade-off between total parameters and training compute?",
    "relevant_papers": [
      "2501.12370v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the optimal balance between computational efficiency and model capacity through sparsity in large-scale mixture-of-experts language models.",
    "relevant_papers": [
      "2501.12370v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating large language models on multi-step and constrained function calling tasks within long-context scenarios using a comprehensive benchmark and automated framework",
    "relevant_papers": [
      "2501.10132v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to measure the performance of LLMs in complex tool use scenarios requiring parameter value reasoning and handling of 128k context windows",
    "relevant_papers": [
      "2501.10132v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language model-based generative agents be used to simulate learner response data and evaluate personalized learning algorithms in intelligent education systems?",
    "relevant_papers": [
      "2501.10332v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "Using LLM-powered learner simulators with memory and reflection modules to bridge the gap between offline metrics and online performance in educational platforms.",
    "relevant_papers": [
      "2501.10332v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "How does global batch load balancing loss improve expert specialization and performance compared to micro-batch implementation in Mixture-of-Experts models?",
    "relevant_papers": [
      "2501.11873v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Training Mixture-of-Experts models with global frequency synchronization for load balancing loss to prevent uniform routing in domain-specific sequences.",
    "relevant_papers": [
      "2501.11873v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Best practices and design considerations for conducting external red teaming on large language models to identify novel safety risks and stress test mitigations.",
    "relevant_papers": [
      "2503.16431v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "How can AI developers implement external red teaming frameworks including team composition, access levels, and integration with automated evaluation systems?",
    "relevant_papers": [
      "2503.16431v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "Unified generative model for speech and singing voice enhancement that utilizes masked modeling, prompt-based guidance for speaker timbre, and an iterative self-critic refinement process.",
    "relevant_papers": [
      "2501.15417v3"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "How can masked generative models be used for zero-shot voice enhancement tasks like dereverberation and target speaker extraction without performing any task-specific fine-tuning?",
    "relevant_papers": [
      "2501.15417v3"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "Scalable deep graph neural networks for crystal property prediction using dense connectivity and hierarchical residual networks to prevent over-smoothing",
    "relevant_papers": [
      "2501.03278v1"
    ],
    "category": "cond-mat.mtrl-sci",
    "original_relevant_count": 1
  },
  {
    "query": "Advanced graph neural network models for molecules and materials incorporating local structure order parameters embedding for high performance on JARVIS-DFT and QM9",
    "relevant_papers": [
      "2501.03278v1"
    ],
    "category": "cond-mat.mtrl-sci",
    "original_relevant_count": 1
  },
  {
    "query": "How to map hidden knowledge of neural networks into the multimodal space of CLIP for automated neuron labeling and mechanistic understanding?",
    "relevant_papers": [
      "2501.05398v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Scalable method for validating AI models and identifying neurons encoding specific concepts or spurious correlations through semantic space projection",
    "relevant_papers": [
      "2501.05398v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Methods for training retrieval augmented generation models that use iterative reasoning and multi-step retrieval chains to solve complex multi-hop question answering benchmarks.",
    "relevant_papers": [
      "2501.14342v3"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "Improving RAG performance through test-time compute scaling and dynamic query reformulation using rejection sampling to generate intermediate retrieval steps for foundation models.",
    "relevant_papers": [
      "2501.14342v3"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive review of the methods and algorithms used to enhance multi-turn dialogue capabilities and context maintenance in current large language models",
    "relevant_papers": [
      "2501.09959v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how are multi-turn interactions in large language models evaluated and what are the core capabilities needed for effective conversational agents and tutoring systems",
    "relevant_papers": [
      "2501.09959v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use tree search algorithms like MCTS to mitigate hallucinations in large language models during the inference phase?",
    "relevant_papers": [
      "2501.01306v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Applying dual process theory and slow thinking generation with self-evaluation reward models to improve factual accuracy in large language models.",
    "relevant_papers": [
      "2501.01306v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "collaborative framework for large language models and small recommendation models using device-cloud architecture for real-time preference updates",
    "relevant_papers": [
      "2501.05647v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "how to integrate cloud-based LLMs with on-device small recommendation models to improve real-time ranking and inference efficiency",
    "relevant_papers": [
      "2501.05647v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "Best practices for optimizing Retrieval-Augmented Generation systems using query expansion and contrastive in-context learning",
    "relevant_papers": [
      "2501.07391v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do factors like document chunk size, retrieval stride, and sentence-level focus mode impact the performance of RAG models?",
    "relevant_papers": [
      "2501.07391v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can self-updating libraries and task decomposition improve the performance of large language models on complex chemical reasoning and calculation tasks?",
    "relevant_papers": [
      "2501.06590v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Frameworks for large language models that use dynamic memory and sub-task decomposition to solve domain-specific chemistry problems on the SciBench benchmark.",
    "relevant_papers": [
      "2501.06590v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking text to image diffusion models for toxicity fairness and privacy risks using large scale manually annotated datasets",
    "relevant_papers": [
      "2501.12612v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive safety assessment framework for evaluating bias and toxic content generation in state of the art image diffusion models",
    "relevant_papers": [
      "2501.12612v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Do different large language models produce similar creative outputs and show less diversity compared to human responses in standardized creativity tests?",
    "relevant_papers": [
      "2501.19361v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "Study comparing the population-level diversity and homogeneity of creative ideas generated by humans versus multiple large language models as writing assistants.",
    "relevant_papers": [
      "2501.19361v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "how effective are LLM-based software repair agents at fixing human and machine-reported bugs in an industrial enterprise environment like Google",
    "relevant_papers": [
      "2501.07531v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "comparison of agentic program repair performance between open-source SWE-Bench datasets and proprietary industrial bug reports from large-scale software companies",
    "relevant_papers": [
      "2501.07531v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "benchmark for evaluating large language model hallucinations across multiple domains using automatic high-precision verifiers that decompose text into atomic units for factual verification",
    "relevant_papers": [
      "2501.08292v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "new taxonomy for large language model hallucinations distinguishing between incorrect training data recollection and fabrication errors during text generation",
    "relevant_papers": [
      "2501.08292v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve automated feature interpretability in large language models using output-centric descriptions and vocabulary unembedding heads",
    "relevant_papers": [
      "2501.08319v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "natural language descriptions for llm features that capture the causal effect of activations on model outputs and steering behavior",
    "relevant_papers": [
      "2501.08319v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve multi-step reasoning in RAG systems using process reward models and natural language explanations for step-level refinement?",
    "relevant_papers": [
      "2501.07861v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Addressing early-step bias in process reward models through temporal-difference look-ahead search and iterative preference optimization for large language models.",
    "relevant_papers": [
      "2501.07861v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do advanced second-order optimizers like Self-Scaled BFGS improve convergence and accuracy in physics-informed machine learning models for solving PDEs?",
    "relevant_papers": [
      "2501.16371v6"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Performance of self-scaled quasi-Newton methods like SSBFGS and SSBroyden for training physics-informed neural networks and Kolmogorov-Arnold networks",
    "relevant_papers": [
      "2501.16371v6"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to use adaptive projective gradient descent and shared subspace projection for data-free multi-task model merging without losing task-specific information?",
    "relevant_papers": [
      "2501.01230v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Constrained optimization methods for multi-task model merging that minimize the performance gap between merged and individual expert models through task vector alignment.",
    "relevant_papers": [
      "2501.01230v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does displaying AI confidence levels influence human self-confidence and the calibration of their own judgment during collaborative decision-making tasks?",
    "relevant_papers": [
      "2501.12868v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Research on the alignment between artificial intelligence confidence and user self-confidence in human-AI collaboration and the impact of real-time correctness feedback.",
    "relevant_papers": [
      "2501.12868v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Performance comparison between large language models and traditional machine learning for multiclass job review classification and binary fake news detection tasks",
    "relevant_papers": [
      "2501.08457v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Analyzing the trade-off between F1-score and inference time for LLMs like Llama3 and GPT-4 compared to deep learning in text classification",
    "relevant_papers": [
      "2501.08457v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive review of techniques for distinguishing between aleatoric and epistemic uncertainty in high-risk artificial intelligence applications like healthcare",
    "relevant_papers": [
      "2501.03282v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "comparison of probabilistic methods and ensemble learning for quantifying uncertainty to improve the reliability and safety of autonomous systems",
    "relevant_papers": [
      "2501.03282v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "discrete diffusion models for multi-task drug discovery using SAFE sequences and fragment remasking strategies",
    "relevant_papers": [
      "2501.06158v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to use non-autoregressive bidirectional parallel decoding and discrete diffusion for goal-directed lead optimization",
    "relevant_papers": [
      "2501.06158v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "semi-supervised split learning frameworks for addressing intermittent connectivity and data labeling scarcity in LEO satellite networks",
    "relevant_papers": [
      "2501.01293v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement split learning for resource-constrained LEO satellites while managing data heterogeneity and limited labeling through semi-supervised techniques",
    "relevant_papers": [
      "2501.01293v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to adapt ConvNeXt architectures for facial emotion recognition using spatial transformer networks and squeeze and excitation blocks to improve classification accuracy?",
    "relevant_papers": [
      "2501.08199v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Deep learning frameworks for facial expression recognition that utilize self-attention regularization and spatial transformers to achieve state-of-the-art results on the FER2013 dataset.",
    "relevant_papers": [
      "2501.08199v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Reducing hallucinations in legal question answering systems using behavior cloning and hard sample-aware iterative direct preference optimization techniques.",
    "relevant_papers": [
      "2501.06521v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarking and evaluating large language model factuality in law through LegalHalBench and metrics like non-hallucinated statute rate.",
    "relevant_papers": [
      "2501.06521v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can modified harmful datasets bypass moderation guardrails during the fine-tuning process of large language models to break safety alignment?",
    "relevant_papers": [
      "2501.17433v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the effectiveness of guardrail moderation in preventing harmful fine-tuning attacks and safety degradation in pre-trained large language models.",
    "relevant_papers": [
      "2501.17433v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How to integrate natural language, algorithmic, and symbolic reasoning paradigms to improve mathematical problem solving performance in large language models?",
    "relevant_papers": [
      "2501.11110v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Progressive paradigm training strategies for unifying multiple reasoning approaches in LLMs to achieve state-of-the-art results on math benchmarks and theorem proving.",
    "relevant_papers": [
      "2501.11110v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "training large language models to adaptively allocate inference budgets and reasoning chain lengths for solving mathematics problems efficiently",
    "relevant_papers": [
      "2501.17974v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Inference Budget-Constrained Policy Optimization for improving large language model reasoning performance under specific computation and token constraints",
    "relevant_papers": [
      "2501.17974v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "What are the common limitations and reliability issues of using large language models for simulating human behavior in social and economic research?",
    "relevant_papers": [
      "2501.08579v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Frameworks and algorithms for improving the alignment and credibility of LLM-based human simulations across psychological and policy domains.",
    "relevant_papers": [
      "2501.08579v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "synthetic benchmarks for evaluating the long-context reasoning capabilities of large language models through context expansion of short-context questions",
    "relevant_papers": [
      "2501.15089v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how do large language models perform on long-context logical inference and mathematical word problems as input length increases in synthetic reasoning benchmarks",
    "relevant_papers": [
      "2501.15089v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can sparse autoencoder features be optimized for precise and interpretable activation steering in large language models like Gemma-2?",
    "relevant_papers": [
      "2501.09929v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Comparison of Feature Guided Activation Additions with Contrastive Activation Addition and SAE-TS for improving the control and interpretability of LLM behaviors.",
    "relevant_papers": [
      "2501.09929v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to automatically convert open ended visual question answering datasets into multiple choice format for more reliable vision language model evaluation",
    "relevant_papers": [
      "2501.03225v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "agent based framework for generating challenging distractors and transforming VQA benchmarks into multiple choice questions for vision language models",
    "relevant_papers": [
      "2501.03225v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to adapt a small auto-regressive model like Qwen2 for multilingual embedding tasks using persona-based synthetic data and ranking consistency filtering?",
    "relevant_papers": [
      "2501.01028v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "What are the most effective training data cleaning and sampling techniques for developing multilingual embedding models with less than one billion parameters?",
    "relevant_papers": [
      "2501.01028v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can Kolmogorov-Arnold Networks be combined with recurrent neural architectures to improve short term load forecasting for diverse residential and industrial energy consumers?",
    "relevant_papers": [
      "2501.06965v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Applying learnable temporal spline functions and edge-based activations in recurrent networks to model non-linear variations in building energy consumption data.",
    "relevant_papers": [
      "2501.06965v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to use causal reward modeling and counterfactual invariance to mitigate spurious correlations and length bias in LLM reinforcement learning from human feedback",
    "relevant_papers": [
      "2501.09620v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "improving the fairness and reliability of large language model alignment by incorporating causal inference into reward models to prevent reward hacking",
    "relevant_papers": [
      "2501.09620v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive review of mitigation strategies for large language model risks including privacy leakage, hallucinations, and jailbreak defense across the entire development lifecycle",
    "relevant_papers": [
      "2501.09431v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "recent advancements in responsible AI for enhancing large language models through unified frameworks covering value alignment, toxicity elimination, and ethical safety measures",
    "relevant_papers": [
      "2501.09431v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can matching user queries against pre-generated questions from document chunks improve retrieval accuracy in RAG systems compared to traditional methods?",
    "relevant_papers": [
      "2501.02702v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for reducing information dilution in RAG systems by generating domain-specific question datasets from document chunks for more precise information retrieval.",
    "relevant_papers": [
      "2501.02702v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "vision transformer framework using foundation models for energy efficient cell segmentation and adaptive cell type classification in digital pathology images",
    "relevant_papers": [
      "2501.05269v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "automated generation of training data from immunofluorescence stainings for zero shot cell segmentation and classification using vision transformers",
    "relevant_papers": [
      "2501.05269v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to prevent attention distribution flattening in long context transformer models using an alternative to standard softmax scaling?",
    "relevant_papers": [
      "2501.19399v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving length generalization and key information retrieval in language models by replacing softmax with a scalable attention mechanism.",
    "relevant_papers": [
      "2501.19399v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can external document knowledge be integrated directly into the feed-forward network parameters of large language models to improve retrieval augmented generation performance?",
    "relevant_papers": [
      "2501.15915v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for parameterizing retrieved knowledge into model weights to overcome the limitations and context length constraints of traditional in-context RAG methods.",
    "relevant_papers": [
      "2501.15915v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "What are the limitations of graph neural networks for solving graph connectivity using circuit complexity bounds?",
    "relevant_papers": [
      "2501.06444v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the computational power of message passing GNNs for graph isomorphism through the lens of TC0 and NC1 complexity classes.",
    "relevant_papers": [
      "2501.06444v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement dynamic workflow adjustment and modular subtask allocation in multi-agent systems using activity on vertex graphs and LLMs?",
    "relevant_papers": [
      "2501.07834v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Research on enhancing multi-agent framework performance through modularized workflow automation and dynamic refinement based on subtask dependency complexity.",
    "relevant_papers": [
      "2501.07834v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can developers build a structured safety case to prove that LLM agents are unable to subvert control measures or exfiltrate data?",
    "relevant_papers": [
      "2501.17315v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating AI control safety by using red teaming and conservative performance extrapolation to prevent deployment risks of capable models.",
    "relevant_papers": [
      "2501.17315v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Comprehensive survey on explainable artificial intelligence techniques ranging from inherently interpretable models to modern methods for interpreting large language models",
    "relevant_papers": [
      "2501.09967v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models and vision-language frameworks be applied to automate and improve the explainability of complex black box machine learning models?",
    "relevant_papers": [
      "2501.09967v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking the performance of constrained decoding frameworks for language models using large scale real world json schemas",
    "relevant_papers": [
      "2501.10868v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how do different constrained decoding tools compare in terms of efficiency and quality for structured output generation in large language models",
    "relevant_papers": [
      "2501.10868v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to merge multiple deep learning models sequentially without retraining using orthogonal weight projections and adaptive scaling",
    "relevant_papers": [
      "2501.09522v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "training-free methods for scalable continual model merging that minimize task interference and maintain constant memory complexity",
    "relevant_papers": [
      "2501.09522v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to use audio large language models for natural language-based speech quality evaluation and Mean Opinion Score prediction using distillation?",
    "relevant_papers": [
      "2501.17202v2"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "Alignment approach with LLM distillation for enhancing audio large language models with descriptive speech quality assessment and A/B test capabilities.",
    "relevant_papers": [
      "2501.17202v2"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking page-level and layout-level retrieval systems for identifying specific charts, figures, and tables within long multimodal documents",
    "relevant_papers": [
      "2501.08828v3"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "evaluation of visual retrievers versus text-based retrievers for fine-grained information retrieval from extensive documents with complex layout structures",
    "relevant_papers": [
      "2501.08828v3"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve multilingual reasoning in large language models by selectively fine-tuning specific transformer layers for better parameter efficiency and reduced training time?",
    "relevant_papers": [
      "2501.03681v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient methods for multilingual reasoning alignment that target lower-level representation layers to avoid catastrophic forgetting and high computational costs in LLMs.",
    "relevant_papers": [
      "2501.03681v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be trained to reason over user personal context to improve the quality of long-form personalized text generation?",
    "relevant_papers": [
      "2501.04167v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Self-training frameworks for personalized LLMs that utilize reasoning paths and expectation maximization to align with individual user styles and preferences.",
    "relevant_papers": [
      "2501.04167v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "methods for reducing hallucinations in multimodal large language models using hierarchical direct preference optimization across token and segment levels",
    "relevant_papers": [
      "2501.16629v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving multimodal LLM alignment through cross-modal preference learning and visual preference optimization to minimize object hallucination errors",
    "relevant_papers": [
      "2501.16629v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does Meta use Large Language Models for mutation-guided test generation to improve code security and privacy?",
    "relevant_papers": [
      "2501.12862v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "LLM-based test generation framework for hardening Android Kotlin code against regressions and privacy leaks in large-scale industrial software.",
    "relevant_papers": [
      "2501.12862v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement a neuro-fuzzy system on an FPGA for real-time driving style recognition and personalized adaptive cruise control?",
    "relevant_papers": [
      "2501.16212v1"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "FPGA-based intelligent sensor for personalizing time headway parameters in adaptive cruise control using SHRP2 naturalistic driving data and neuro-fuzzy logic",
    "relevant_papers": [
      "2501.16212v1"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "impact of AWQ and GPTQ low-bit quantization on the mathematical reasoning and numerical computation performance of Llama-3 models",
    "relevant_papers": [
      "2501.03035v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "effective fine-tuning strategies to restore mathematical reasoning capabilities in quantized large language models using minimal task-specific training data",
    "relevant_papers": [
      "2501.03035v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "systematic review of large language models for natural disaster management including taxonomies of application scenarios and available public datasets",
    "relevant_papers": [
      "2501.06932v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how are generative artificial intelligence and large language models being used to improve resilience against natural disasters across different response phases",
    "relevant_papers": [
      "2501.06932v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Does OpenAI's o3 model represent true artificial general intelligence despite its high score on the ARC-AGI benchmark?",
    "relevant_papers": [
      "2501.07458v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Critique of massive trialling of predefined operations as a method for achieving artificial general intelligence in Large Language Models like o3",
    "relevant_papers": [
      "2501.07458v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating implicit sociodemographic bias in large language models using agent personas and decision-making simulations across different social categories.",
    "relevant_papers": [
      "2501.17420v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Do advanced large language models exhibit greater implicit bias in decision-making tasks compared to their performance on explicit fairness benchmarks?",
    "relevant_papers": [
      "2501.17420v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Systematic literature review of large language models in CHI papers from 2020 to 2024 examining research domains and methodologies",
    "relevant_papers": [
      "2501.12557v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How are large language models being used as research tools and simulated users in human-computer interaction studies published at CHI?",
    "relevant_papers": [
      "2501.12557v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating personalized long-form text generation using large language models with explainable reference-based frameworks focusing on content and writing style alignment",
    "relevant_papers": [
      "2501.14956v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to extract atomic aspects and evidence from LLM generated text to provide interpretable evaluation for personalized writing styles compared to reference texts",
    "relevant_papers": [
      "2501.14956v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "large scale dataset of high quality science problem solution pairs for training large language models in higher education stem domains",
    "relevant_papers": [
      "2501.15587v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "automated extraction pipeline for building scientific reasoning datasets from heterogeneous educational sources for large language model development",
    "relevant_papers": [
      "2501.15587v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive survey of gradient-based multi-objective optimization techniques and algorithms for training deep neural networks with conflicting goals",
    "relevant_papers": [
      "2501.10945v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "recent advancements in learning continuous Pareto sets and finding optimal trade-offs in multi-task learning using deep learning optimization methods",
    "relevant_papers": [
      "2501.10945v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Comprehensive survey of deep reinforcement learning algorithms applied to job scheduling and dynamic resource management in heterogeneous cloud computing environments.",
    "relevant_papers": [
      "2501.01007v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "How does deep reinforcement learning compare to traditional heuristics for managing tasks and resource allocation in modern cloud computing systems?",
    "relevant_papers": [
      "2501.01007v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "Survey of model optimization and system architecture strategies for deploying reasoning-capable large language models on resource-constrained edge devices",
    "relevant_papers": [
      "2501.03265v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can cognitive edge computing frameworks balance latency, energy, and privacy when running autonomous AI agents on hardware with limited memory?",
    "relevant_papers": [
      "2501.03265v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to generate 360 panoramas using multi-view diffusion models by synthesizing cubemap faces instead of equirectangular projections",
    "relevant_papers": [
      "2501.17162v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "diffusion based method for high resolution 360 degree panorama generation from text prompts using six face cubemap synthesis",
    "relevant_papers": [
      "2501.17162v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve concept erasure in diffusion models by dynamically selecting target concepts to prevent side effects on unrelated concepts?",
    "relevant_papers": [
      "2501.18950v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Adaptive Guided Erasure method for selectively removing harmful concepts from diffusion models using local geometric properties of concept space.",
    "relevant_papers": [
      "2501.18950v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does a hybrid attention mechanism combining global and local spans improve long-context performance compared to traditional RoPE-based models?",
    "relevant_papers": [
      "2501.18795v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparison of RoPE, NoPE, and QK-Normalization patterns in transformer models for training efficiency and extended context window capabilities.",
    "relevant_papers": [
      "2501.18795v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to design a multi-agent framework using large language models for personalized goal-oriented learning in intelligent tutoring systems?",
    "relevant_papers": [
      "2501.15749v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "LLM-based approach for mapping learner goals to skills and optimizing personalized learning paths with multi-agent coordination.",
    "relevant_papers": [
      "2501.15749v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how do large language models exhibit conformity bias and groupthink within multi-agent systems during collaborative reasoning tasks",
    "relevant_papers": [
      "2501.13381v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking social influence in AI agents using BenchForm to evaluate factors like majority size and interaction protocols on LLM conformity",
    "relevant_papers": [
      "2501.13381v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking framework for evaluating large language model based agents in autonomous cloud operations and incident management tasks",
    "relevant_papers": [
      "2501.06706v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "holistic platform for testing ai agents using microservice fault injection and telemetry data to enable self healing cloud systems",
    "relevant_papers": [
      "2501.06706v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can multimodal prompts like images videos and humming be used for emotionally controllable symbolic music generation using a projector and composer framework?",
    "relevant_papers": [
      "2501.08809v1"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "A generalized framework for symbolic music generation that utilizes a multi-task learning selector to evaluate quality emotion and genre in MIDI files.",
    "relevant_papers": [
      "2501.08809v1"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement training-free visual token pruning for multimodal large language models using graph-based node connections and information flow",
    "relevant_papers": [
      "2501.02268v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "reducing visual redundancy in MLLMs like LLaVA-NeXT through graph-based pruning methods that preserve essential foreground and background tokens",
    "relevant_papers": [
      "2501.02268v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "distributed training of large language models using asynchronous parameter synchronization and quantization to reduce communication bandwidth across distant compute clusters",
    "relevant_papers": [
      "2501.18512v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve DiLoCo for distributed training by overlapping communication with computation and using streaming parameter updates for low bandwidth links",
    "relevant_papers": [
      "2501.18512v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can count-based exploration and optimistic reward estimates improve the performance of online preference alignment for large language models?",
    "relevant_papers": [
      "2501.12735v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "A practical algorithm for online RLHF that uses a coin-flip counting module to estimate pseudo-counts for better data coverage in preference optimization.",
    "relevant_papers": [
      "2501.12735v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used to identify adversarial participants and generate safety-critical scenarios for testing autonomous driving systems in closed-loop simulations?",
    "relevant_papers": [
      "2501.15850v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Improving the safety and robustness of autonomous vehicles through iterative adversarial scenario generation using LLM-coordinated agents and trajectory optimization techniques.",
    "relevant_papers": [
      "2501.15850v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Observational study on how programming students use generative AI tools like ChatGPT for code authoring and the resulting impact on learning behavior",
    "relevant_papers": [
      "2501.10091v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How do computer science students interact with large language models during coding tasks and what are their common prompting and problem-solving strategies",
    "relevant_papers": [
      "2501.10091v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Video reasoning segmentation using multimodal large language models and hierarchical tokens for capturing complex spatial and inter-frame motion features",
    "relevant_papers": [
      "2501.08549v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Improving video reasoning segmentation accuracy on ReVOS benchmarks with temporal dynamic aggregation and SAM2-based keyframe selection strategies",
    "relevant_papers": [
      "2501.08549v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can concept activation vectors be used to steer large language model generation for toxicity reduction and sentiment control without extensive fine-tuning?",
    "relevant_papers": [
      "2501.05764v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Lightweight framework for granular control of LLM outputs by adjusting activation layers and steering magnitudes with concept activation vectors for various styles.",
    "relevant_papers": [
      "2501.05764v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does GPT-4o perform on multimodal physics concept inventories across different languages compared to undergraduate students?",
    "relevant_papers": [
      "2501.06143v3"
    ],
    "category": "physics.ed-ph",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the multimodal and multilingual capabilities of large language models on diverse physics topics and image-based concept inventory items.",
    "relevant_papers": [
      "2501.06143v3"
    ],
    "category": "physics.ed-ph",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluation of large language models for translating natural language requirements into formal specifications using Coq, Lean4, Dafny, ACSL, and TLA+",
    "relevant_papers": [
      "2501.16207v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Fine-tuning small language models with distilled high-quality formal reasoning data to match the performance of large models like DeepSeek-R1",
    "relevant_papers": [
      "2501.16207v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Fine-grained complexity analysis of visual autoregressive models and SETH lower bounds for efficient image generation frameworks",
    "relevant_papers": [
      "2501.04377v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "What are the computational limits and efficiency criteria for visual autoregressive models using low-rank approximations and theoretical complexity analysis?",
    "relevant_papers": [
      "2501.04377v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "best practices and lessons learned from red teaming generative AI products and large language models based on industry case studies",
    "relevant_papers": [
      "2501.07238v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "what are the key methodologies and threat model ontologies used for red teaming generative AI systems at a large scale",
    "relevant_papers": [
      "2501.07238v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be fine-tuned to improve their robustness against noisy, irrelevant, or misleading information retrieved in RAG systems?",
    "relevant_papers": [
      "2501.18365v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for training large language models to ignore counterfactual or poor quality search results within a retrieval-augmented generation framework.",
    "relevant_papers": [
      "2501.18365v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "relationship between non-smooth convex optimization theory and learning rate schedules like constant with linear cooldown for training large language models",
    "relevant_papers": [
      "2501.18965v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to use optimization theory bounds to transfer optimal learning rates and extend training for Llama-type model architectures",
    "relevant_papers": [
      "2501.18965v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can multi-modal large language models be improved for fine-grained visual recognition tasks involving subordinate-level categories and object-category alignment?",
    "relevant_papers": [
      "2501.15140v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Enhancing fine-grained recognition in MLLMs using contrastive learning on object-attribute pairs and attribute-category pairs with hard negative samples.",
    "relevant_papers": [
      "2501.15140v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing knowledge base question answering through agentic reasoning and monte carlo tree search for logical form generation",
    "relevant_papers": [
      "2501.18922v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving low resource kbqa performance with mcts guided exploration and incremental fine tuning of large language models",
    "relevant_papers": [
      "2501.18922v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "mathematical analysis of transformer layer dynamics using Vlasov equations and mean-field limits for different attention mechanisms",
    "relevant_papers": [
      "2501.18322v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "understanding the evolution of data anisotropy and clustering in deep transformers through the lens of continuous-time PDEs and Gaussian measures",
    "relevant_papers": [
      "2501.18322v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "adaptive retrieval methods for overcoming bounded recall in listwise large language model reranking systems",
    "relevant_papers": [
      "2501.09186v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "improving retrieval recall by using listwise LLM rankers to provide feedback for guiding document search results",
    "relevant_papers": [
      "2501.09186v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language model agents be used for goal-driven and constraint-guided hypothesis generation in materials discovery and design?",
    "relevant_papers": [
      "2501.13299v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluation framework and novel dataset for assessing the quality of hypotheses generated by LLMs for material science applications.",
    "relevant_papers": [
      "2501.13299v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "The relationship between softmax numerical stability and the delayed generalization phenomenon known as grokking in deep learning models",
    "relevant_papers": [
      "2501.04697v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Achieving grokking without regularization by mitigating softmax collapse and using perpendicular gradient components to avoid naive loss minimization",
    "relevant_papers": [
      "2501.04697v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "automated techniques for optimizing prompts to improve test case generation performance across different large language models",
    "relevant_papers": [
      "2501.01329v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "how to automatically generate model-specific prompts for software testing by incorporating domain knowledge and diversity guidance",
    "relevant_papers": [
      "2501.01329v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How can interdisciplinary collaboration between physicists and computer scientists facilitate the development of specialized large scale AI models for scientific research?",
    "relevant_papers": [
      "2501.05382v1"
    ],
    "category": "physics.data-an",
    "original_relevant_count": 1
  },
  {
    "query": "A roadmap for creating large physics models using foundation model architectures and specialized tools for mathematical symbolic reasoning and experimental data analysis",
    "relevant_papers": [
      "2501.05382v1"
    ],
    "category": "physics.data-an",
    "original_relevant_count": 1
  },
  {
    "query": "How can small language models achieve high performance in retrieval augmented generation using heterogeneous graph indexing and lightweight topology-enhanced retrieval?",
    "relevant_papers": [
      "2501.06713v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient retrieval augmented generation framework for resource-constrained scenarios that uses graph-based indexing of text chunks and entities to support small language models.",
    "relevant_papers": [
      "2501.06713v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to reduce error accumulation in online test-time prompt tuning for vision-language models using dynamic selection and buffer management strategies?",
    "relevant_papers": [
      "2501.16404v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for adaptive prompt selection based on prediction entropy and probability difference to improve zero-shot generalization during test-time inference.",
    "relevant_papers": [
      "2501.16404v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Multi-modal sequential recommendation models using hierarchical mixture of experts to filter redundant information and capture explicit temporal signals for dynamic user interests.",
    "relevant_papers": [
      "2501.14269v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How can hierarchical mixture of experts and contrastive learning be combined to improve feature extraction and temporal modeling in multi-modal sequential recommendation?",
    "relevant_papers": [
      "2501.14269v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive review of Mixture of Experts architectures, gating networks, and their applications in solving key technical challenges within big data environments.",
    "relevant_papers": [
      "2501.16352v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does the Mixture of Experts framework improve the performance and scalability of artificial intelligence models when processing massive and diverse datasets?",
    "relevant_papers": [
      "2501.16352v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive guide to pre-training generative models and alignment techniques for students learning about the core architecture of large language models",
    "relevant_papers": [
      "2501.09223v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "academic reference for understanding the fundamental pillars of language modeling including prompting inference strategies and pre-training methodologies for practitioners",
    "relevant_papers": [
      "2501.09223v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "training free inference time methods for mitigating hallucinations in large vision language models by calibrating inter-modality correlations and refining cross modal attention",
    "relevant_papers": [
      "2501.01926v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can contrastive decoding mechanisms and masking significant cross-modal attention value vectors reduce object hallucinations in large vision-language models without additional training?",
    "relevant_papers": [
      "2501.01926v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How does the extended context window of large language models like Gemini 1.5 Pro affect performance and latency in NL2SQL tasks?",
    "relevant_papers": [
      "2501.12372v6"
    ],
    "category": "cs.DB",
    "original_relevant_count": 1
  },
  {
    "query": "Leveraging long context large language models for text to SQL generation using schema documentation and column example values without fine-tuning.",
    "relevant_papers": [
      "2501.12372v6"
    ],
    "category": "cs.DB",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing the performance of LLM input guardrails through chain of thought fine-tuning and alignment for effective malicious prompt detection",
    "relevant_papers": [
      "2501.13080v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how can fine-tuning large language models as judges with chain of thought reasoning improve the security of conversational AI systems",
    "relevant_papers": [
      "2501.13080v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive review of text data augmentation methods using prompt-based and retrieval-based strategies for training large language models",
    "relevant_papers": [
      "2501.18845v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "current challenges and future opportunities in using generative models to synthesize training datasets for natural language processing tasks",
    "relevant_papers": [
      "2501.18845v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can deep learning models combining LSTM and CNN with attention mechanisms improve the accuracy of intrusion detection systems in industrial internet of things?",
    "relevant_papers": [
      "2501.13962v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Using synthetic minority over-sampling technique and hybrid neural networks for detecting and classifying cyber-attacks in the Edge-IIoTset dataset.",
    "relevant_papers": [
      "2501.13962v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "state of the art 8B small language model as a judge for general purpose evaluation and absolute scoring benchmarks",
    "relevant_papers": [
      "2501.17195v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "techniques for training small language models for automated evaluation using direct preference optimization and synthetic critique datasets",
    "relevant_papers": [
      "2501.17195v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can self-reflection frameworks and simulated psychological assessments be used to compare explicit and implicit social biases in large language models?",
    "relevant_papers": [
      "2501.02295v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the inconsistency between explicit and implicit social bias in large language models and the impact of alignment techniques on bias suppression.",
    "relevant_papers": [
      "2501.02295v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does GPU dynamic voltage and frequency scaling impact the energy efficiency and inference latency of large language models like LLaMA and Mistral?",
    "relevant_papers": [
      "2501.08219v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Analyzing the relationship between input sequence characteristics and energy consumption in large language model inference using hardware-level DVFS optimizations",
    "relevant_papers": [
      "2501.08219v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can transformers perform full Bayesian inference using in-context learning for generalized linear models and latent factor models?",
    "relevant_papers": [
      "2501.16825v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Comparing transformer in-context learning with Markov Chain Monte Carlo and variational inference for approximating complex posterior distributions in statistical models.",
    "relevant_papers": [
      "2501.16825v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the effectiveness and limitations of reinforcement learning versus supervised fine-tuning for ensuring harmlessness in DeepSeek-R1 reasoning models",
    "relevant_papers": [
      "2501.17030v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "hybrid training approaches combining RL and SFT to address reward hacking and language mixing in large language model safety alignment",
    "relevant_papers": [
      "2501.17030v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to apply speculative sampling techniques to accelerate inference in continuous diffusion models without training a separate draft model",
    "relevant_papers": [
      "2501.05370v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Speeding up diffusion model generation using speculative decoding strategies to achieve exact samples with fewer function evaluations",
    "relevant_papers": [
      "2501.05370v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to use synthetic persona data from Persona Hub to improve character generalization and role-playing capabilities in large language models",
    "relevant_papers": [
      "2501.15427v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Large-scale synthetic data generation strategies for training customizable role-playing dialogue agents with supervised fine-tuning on Llama-3 models",
    "relevant_papers": [
      "2501.15427v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can temperature sensitivity be used to detect if instruction tuning data was used to train a vision-language model through membership inference?",
    "relevant_papers": [
      "2501.18624v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Assessing the vulnerability of vision-language models to membership inference attacks by analyzing small sets of samples and their response to temperature variations.",
    "relevant_papers": [
      "2501.18624v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How has constructionism evolved as an educational framework from the advent of personal computing to the current era of generative artificial intelligence?",
    "relevant_papers": [
      "2501.07486v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "Applying constructionist principles to smart education models to promote learner autonomy and personalized learning experiences in contemporary digital environments.",
    "relevant_papers": [
      "2501.07486v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "agentic workflows for program synthesis using LLM quality checks to improve code generation accuracy and self-debugging performance",
    "relevant_papers": [
      "2501.17167v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement a dynamic multi-agent system for program synthesis that uses internal quality checkers to validate code against unit tests",
    "relevant_papers": [
      "2501.17167v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How to use hierarchical feature trees and high-level abstractions to generate diverse and complex synthetic data for repository-level code generation tasks?",
    "relevant_papers": [
      "2501.04694v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Iterative feature tree synthesis framework for generating high-quality training data spanning from function-level operations to complex multi-file software engineering scenarios.",
    "relevant_papers": [
      "2501.04694v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Recent surveys on large vision-language model alignment and misalignment issues categorized by object attribute and relational levels through explainability perspectives",
    "relevant_papers": [
      "2501.01346v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "What are the primary causes of multimodal misalignment in vision-language models and how can explainable AI help identify these representation errors",
    "relevant_papers": [
      "2501.01346v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "performance comparison of GPT-4o and Claude 3.5 against traditional OCR systems like TrOCR for transcribing historical handwritten tabular documents",
    "relevant_papers": [
      "2501.11623v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the accuracy of large language models for line-by-line and whole-scan transcription of historical records using character error rate and BLEU scores",
    "relevant_papers": [
      "2501.11623v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking the security and vulnerability of retrieval-augmented generation systems against adversarial attacks like inter-context conflict and white denial of service",
    "relevant_papers": [
      "2501.18636v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "how do external knowledge injections and unverified context impact the safety and service quality of large language model RAG pipelines",
    "relevant_papers": [
      "2501.18636v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "automated extraction of olympiad level math problems from online forums for training and evaluating large language models",
    "relevant_papers": [
      "2501.14275v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking mathematical reasoning in large language models using timestamped problems to prevent data contamination",
    "relevant_papers": [
      "2501.14275v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking the performance of large language model agents on complex clinical tasks within a realistic and FHIR compliant electronic health record environment",
    "relevant_papers": [
      "2501.14654v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "standardized evaluation framework for testing the planning and tool utilization capabilities of medical AI agents in simulated healthcare settings",
    "relevant_papers": [
      "2501.14654v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking framework for evaluating the effectiveness of hate speech detectors on synthetic content generated by various large language models",
    "relevant_papers": [
      "2501.16750v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "how well do modern hate speech detection systems perform against adversarial attacks and LLM-driven hate campaigns designed to evade detection",
    "relevant_papers": [
      "2501.16750v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive survey of generative artificial intelligence techniques for automating keyframe inbetweening and colorization in traditional celluloid animation workflows",
    "relevant_papers": [
      "2501.06250v5"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how are diffusion models and multimodal AI being applied to enhance efficiency and creative expression in the 2D animation production pipeline",
    "relevant_papers": [
      "2501.06250v5"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can diffusion priors be used to balance perceptual quality and performance in downstream vision tasks for unified image restoration?",
    "relevant_papers": [
      "2501.13134v2"
    ],
    "category": "eess.IV",
    "original_relevant_count": 1
  },
  {
    "query": "A unified image restoration model using diffusion priors with task feature adapters to optimize both human perception and machine vision utility.",
    "relevant_papers": [
      "2501.13134v2"
    ],
    "category": "eess.IV",
    "original_relevant_count": 1
  },
  {
    "query": "validated tools and metrics for evaluating the quality and accuracy of clinical document summaries generated by large language models in healthcare settings",
    "relevant_papers": [
      "2501.08977v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to assess the structural and substantive validity of automated medical note summarization instruments using factor analysis and inter-rater reliability",
    "relevant_papers": [
      "2501.08977v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "investigating the emergence and formation of localized task vectors in transformers trained for in-context learning on synthetic datasets",
    "relevant_papers": [
      "2501.09240v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "using task vector prompting loss to enhance task representation robustness and generalization in transformer models performing in-context learning",
    "relevant_papers": [
      "2501.09240v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models achieve higher knowledge density and depth in long-form writing using iterative expansion and slow-thinking retrieval processes?",
    "relevant_papers": [
      "2501.09751v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Machine writing frameworks that simulate human-like cognitive reflection and iterative knowledge expansion to improve the quality of retrieval-augmented generation outputs.",
    "relevant_papers": [
      "2501.09751v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can modified Algorithm-of-Thoughts techniques like AoT+ help large language models achieve state-of-the-art performance in autonomous long-horizon planning tasks?",
    "relevant_papers": [
      "2501.13545v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the use of enhanced Algorithm-of-Thoughts frameworks to improve the autonomous planning capabilities of LLMs on standard benchmarks like Blocksworld.",
    "relevant_papers": [
      "2501.13545v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can multi-agent systems and agentic AI frameworks like OVON be used to detect and mitigate hallucinations in large language models?",
    "relevant_papers": [
      "2501.13946v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluation of hallucination mitigation strategies using hierarchical specialized agents and natural language based communication protocols between AI agents.",
    "relevant_papers": [
      "2501.13946v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can knowledge distillation and conditional variational autoencoders be combined to improve the diversity and accuracy of equation generation in math word problem solvers?",
    "relevant_papers": [
      "2501.03670v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Adaptive diversity distillation techniques for math word problems to help student models learn multiple valid solution equations from a teacher model and improve generalization.",
    "relevant_papers": [
      "2501.03670v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used to generate functional interior design layouts by extracting object lists and spatial constraints for optimization?",
    "relevant_papers": [
      "2501.04648v2"
    ],
    "category": "cs.GR",
    "original_relevant_count": 1
  },
  {
    "query": "Leveraging LLMs and design layout graphs to automate the creation of diverse high-quality virtual scenes using constrained optimization techniques.",
    "relevant_papers": [
      "2501.04648v2"
    ],
    "category": "cs.GR",
    "original_relevant_count": 1
  },
  {
    "query": "How well do large language models perform on Allen's interval relations and temporal reasoning tasks using benchmarks like Wikidata?",
    "relevant_papers": [
      "2501.03040v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating large language model capabilities in temporal understanding and arithmetic through event interval relationships and real-world temporal data benchmarks.",
    "relevant_papers": [
      "2501.03040v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "segment-level direct preference optimization for improving multi-turn social dialogue performance and reducing training noise in large language models",
    "relevant_papers": [
      "2501.01821v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to optimize social agents using segment-based direct preference optimization to outperform session-level alignment methods on the SOTOPIA benchmark",
    "relevant_papers": [
      "2501.01821v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve RAG systems for industrial applications using specialized knowledge extraction and iterative rationale construction for complex logical reasoning tasks?",
    "relevant_papers": [
      "2501.11551v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for knowledge atomizing and knowledge-aware task decomposition to enhance large language model performance in domain-specific retrieval-augmented generation.",
    "relevant_papers": [
      "2501.11551v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be integrated into an analytics framework to interpret and explain multimodal user behaviors across different extended reality environments?",
    "relevant_papers": [
      "2501.13778v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "A framework for analyzing multi-user XR sessions using a platform-agnostic recording schema and LLM-assisted visual interfaces for understanding complex user actions.",
    "relevant_papers": [
      "2501.13778v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "generating diverse and customizable synthetic Q&A pairs for evaluating retrieval-augmented generation systems in domain-specific contexts",
    "relevant_papers": [
      "2501.12789v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "a two-stage framework for producing lexically and semantically diverse synthetic benchmarks to improve the evaluation of RAG applications",
    "relevant_papers": [
      "2501.12789v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models like fine-tuned BART and BERT be applied to proactively predict and identify cyber threats in IoT network traffic?",
    "relevant_papers": [
      "2501.01664v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Proactive intrusion prediction framework for IoT security using transformer-based models to forecast network traffic and detect malicious packets before they occur.",
    "relevant_papers": [
      "2501.01664v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the performance of machine-generated text detectors across multiple domains and Large Language Models using the RAID benchmark dataset",
    "relevant_papers": [
      "2501.08913v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Can machine learning models robustly detect AI-generated content from diverse domains and LLMs when those sources are present in the training data?",
    "relevant_papers": [
      "2501.08913v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve safety visual reasoning in large vision-language models using multi-image datasets and safety chain-of-thought fine-tuning logic?",
    "relevant_papers": [
      "2501.18533v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Reducing attack success rate in vision-language models by addressing the safety reasoning gap with multi-image safety instruction following datasets.",
    "relevant_papers": [
      "2501.18533v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to achieve temporally consistent video relighting using diffusion models with text or environment map conditions without requiring intrinsic decomposition?",
    "relevant_papers": [
      "2501.16330v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Frameworks for video relighting that preserve illumination priors and ensure temporal stability through diffusion models trained on in-the-wild and rendered datasets.",
    "relevant_papers": [
      "2501.16330v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve Sharpness-Aware Minimization performance by explicitly regularizing the top Hessian eigenvalue and aligning perturbation vectors with leading eigenvectors?",
    "relevant_papers": [
      "2501.12666v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Analysis of SAM training dynamics using third-order stochastic differential equations and the impact of eigenvalue regularization on model generalization and sharpness.",
    "relevant_papers": [
      "2501.12666v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating uncertainty estimation versus self-knowledge for adaptive retrieval in RAG systems to improve question answering efficiency and performance",
    "relevant_papers": [
      "2501.12835v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive analysis of uncertainty estimation techniques as a more efficient alternative to self-knowledge pipelines for triggering retrieval in large language models",
    "relevant_papers": [
      "2501.12835v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving mathematical reasoning in large language models using step-aligned in-context learning and retrieved reference steps for fine-grained problem solving",
    "relevant_papers": [
      "2501.03226v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing LLM math capabilities with a first-try strategy and step-level retrieval to improve chain-of-thought and tree search reasoning performance",
    "relevant_papers": [
      "2501.03226v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarks for evaluating long-context language models on complex procedural generation tasks and long-form structured output synthesis beyond simple information retrieval",
    "relevant_papers": [
      "2501.05414v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how do current large language models perform on tasks requiring the synthesis of highly dispersed information into long-form structured documents up to 8k tokens",
    "relevant_papers": [
      "2501.05414v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can domain prompts and semantic prototypes be used in a diffusion model to generate high-quality time series data across multiple domains?",
    "relevant_papers": [
      "2501.05403v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Diffusion-based time series generation using prototype assignment modules for few-shot adaptation to new domains and improved data augmentation performance.",
    "relevant_papers": [
      "2501.05403v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can we effectively detect AI-generated text that has been paraphrased by humanizer tools to evade standard machine learning detection software?",
    "relevant_papers": [
      "2501.03437v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A data-centric augmentation approach for building robust models that detect adversarially modified or humanized AI text across different paraphrasing platforms.",
    "relevant_papers": [
      "2501.03437v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does the presence of citations in large language model responses influence user trust and verification behavior in question answering systems?",
    "relevant_papers": [
      "2501.01303v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Experimental research examining if users trust LLM generated content more when citations are present regardless of whether the sources are relevant or random.",
    "relevant_papers": [
      "2501.01303v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can retrieval-augmented dynamic prompt tuning improve the performance of multimodal transformers when dealing with missing or incomplete modalities?",
    "relevant_papers": [
      "2501.01120v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "A framework for incomplete multimodal learning using retrieval strategies and context-aware dynamic prompts to recover missing information.",
    "relevant_papers": [
      "2501.01120v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can model editing and attention head analysis be used to improve the semantic consistency of large language models for equivalent prompts?",
    "relevant_papers": [
      "2501.11041v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving large language model robustness by identifying key attention heads and injecting biases along semantic-consistency activation directions through model editing.",
    "relevant_papers": [
      "2501.11041v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating large language models on everyday moral dilemmas using the Am I the Asshole Reddit dataset to compare AI and human judgments",
    "relevant_papers": [
      "2501.18081v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how do large language models perform on complex social ethical dilemmas compared to human evaluations from crowdsourced online communities",
    "relevant_papers": [
      "2501.18081v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Effective post-training quantization methods for Mamba architectures to address outlier distributions in gate projections and parallel scan operations.",
    "relevant_papers": [
      "2501.13484v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to use variance aligned rotation and Karhunen-Loeve Transformation for low-bit quantization of state space models like Mamba.",
    "relevant_papers": [
      "2501.13484v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can black-box adversarial attacks disrupt decision-making reasoning chains in vision language models used for autonomous driving systems?",
    "relevant_papers": [
      "2501.13563v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the robustness of vision-language models in autonomous driving against cascading adversarial disruption and risky scene induction attacks.",
    "relevant_papers": [
      "2501.13563v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to use optimal transport and Monge-Kantorovich vector ranks for multivariate conformal prediction to create flexible non-convex prediction regions?",
    "relevant_papers": [
      "2501.18991v2"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "Improving conformal prediction for multi-output regression and classification using optimal transport based vector quantiles for adaptive uncertainty quantification.",
    "relevant_papers": [
      "2501.18991v2"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "How can direct preference optimization be used to personalize text-to-image diffusion models based on a small set of individual user preference examples?",
    "relevant_papers": [
      "2501.06655v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Aligning diffusion models with multiple individual user rewards using vision-language model embeddings and few-shot fine-tuning for personalized image generation.",
    "relevant_papers": [
      "2501.06655v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Survey of recent research on how embodiment, symbol grounding, causality, and memory can help large language models achieve human-level general intelligence.",
    "relevant_papers": [
      "2501.03151v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How are researchers addressing the limitations of large language models in causality and symbol grounding to move toward artificial general intelligence?",
    "relevant_papers": [
      "2501.03151v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "academic benchmark for evaluating multi-hop tool use in large language models with interdependent functions and query-driven data construction methods",
    "relevant_papers": [
      "2501.02506v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluation dataset for testing complex function calling and multi-step reasoning capabilities in state-of-the-art language models using local executable tools",
    "relevant_papers": [
      "2501.02506v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking cultural bias towards Western entities in Arabic language models using the CAMeL-2 dataset of parallel Arabic and English cultural entities",
    "relevant_papers": [
      "2501.04662v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "impact of pre-training data frequency and subword tokenization on entity-related cultural biases in multilingual language models for Arabic and non-Western cultures",
    "relevant_papers": [
      "2501.04662v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can internal activation steering improve the safety and robustness of vision language models against jailbreaking attacks without retraining parameters?",
    "relevant_papers": [
      "2501.16378v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Methods for revising multimodal model activations during generation to align visual inputs with the safety guardrails of the backbone large language model.",
    "relevant_papers": [
      "2501.16378v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can multi-agent large language model frameworks improve the accuracy and scalability of complex remote sensing workflows compared to single-agent systems?",
    "relevant_papers": [
      "2501.16254v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Implementing multi-agent orchestration for geospatial tasks using AutoGen to coordinate specialized sub-agents for urban monitoring, climate analysis, and agricultural studies.",
    "relevant_papers": [
      "2501.16254v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can we design just-in-time and human-verifiable security policies for generalist AI agents operating across diverse task contexts?",
    "relevant_papers": [
      "2501.17070v3"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "A framework for generating contextual security policies for autonomous agents to replace static permissions and manual user confirmation systems.",
    "relevant_papers": [
      "2501.17070v3"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "External safety evaluation results and red teaming findings for OpenAI o3-mini model using the ASTRAL automated testing tool",
    "relevant_papers": [
      "2501.17749v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Systematic generation of unsafe test inputs to assess the pre-deployment safety and ethical risks of OpenAI's latest o3-mini model",
    "relevant_papers": [
      "2501.17749v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "flexible and scalable training system for sparse mixture of experts models using adaptive gradient partitioning and communication computation co-scheduling",
    "relevant_papers": [
      "2501.10714v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "optimizing task scheduling and communication overhead in large scale MoE models compared to DeepSpeed-MoE and Tutel systems",
    "relevant_papers": [
      "2501.10714v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can retrieval augmented generation and large language models be used for real-time fraud detection and policy verification during phone conversations?",
    "relevant_papers": [
      "2501.15290v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Implementing RAG-based systems for real-time phone call fraud detection and user impersonation checks to improve security without retraining entire models.",
    "relevant_papers": [
      "2501.15290v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking the performance of multimodal large language models on temporal reasoning tasks and visual time-lapse estimation",
    "relevant_papers": [
      "2501.10674v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the ability of MLLMs to determine chronological event sequences and estimate time differences between images",
    "relevant_papers": [
      "2501.10674v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Improving quantum machine learning performance by training the measurement phase alongside the variational quantum circuit parameters",
    "relevant_papers": [
      "2501.05663v1"
    ],
    "category": "quant-ph",
    "original_relevant_count": 1
  },
  {
    "query": "End-to-end differentiable framework for learning parameterized observables and Hermitian matrices in quantum neural network models",
    "relevant_papers": [
      "2501.05663v1"
    ],
    "category": "quant-ph",
    "original_relevant_count": 1
  },
  {
    "query": "Best practices and training recipes for domain-adaptive post-training of large language models in the financial sector using preference data distillation",
    "relevant_papers": [
      "2501.04961v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does combining continual pre-training with instruction-following and process-based reward models improve the performance of financial LLMs?",
    "relevant_papers": [
      "2501.04961v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to model long-range dependencies in brain networks using graph transformers for accurate neurological disease diagnosis on ABIDE and ADNI",
    "relevant_papers": [
      "2501.01100v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "using biased random walks in brain graph transformers to model long-range communication and multi-level connectivity between brain regions of interest",
    "relevant_papers": [
      "2501.01100v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "adaptive perturbation methods for mitigating harmful fine-tuning in large language models without sacrificing performance on downstream tasks",
    "relevant_papers": [
      "2501.18100v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to recover large language model safety alignment after harmful fine-tuning using post-fine-tuning weight perturbations and layer safety affinity",
    "relevant_papers": [
      "2501.18100v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive benchmark for evaluating undergraduate level mathematical reasoning in large language models with a focus on mitigating test set contamination",
    "relevant_papers": [
      "2501.13766v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating large reasoning models using metrics like effective accuracy and reasoning gap across diverse university level mathematics topics",
    "relevant_papers": [
      "2501.13766v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve table understanding in language models using adaptive reasoning between textual and symbolic methods for tabular data tasks",
    "relevant_papers": [
      "2501.19378v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A framework for enhancing table reasoning in LLMs by extracting relevant content and verbalizing table semantics with flexible symbolic reasoning strategies",
    "relevant_papers": [
      "2501.19378v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive review of foundation models in computational pathology covering self-supervised learning methods, dataset adaptation strategies, and clinical evaluation benchmarks",
    "relevant_papers": [
      "2501.15724v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "what are the current challenges and methodologies for building multi-modal foundation models using unlabeled whole-slide images in digital pathology research",
    "relevant_papers": [
      "2501.15724v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "adaptive world model reinforcement learning for autonomous driving to mitigate performance degradation during online finetuning in CARLA simulator",
    "relevant_papers": [
      "2501.13072v2"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "how to address distribution shift in world model based planning by selectively updating policy and dynamics model during the finetuning process",
    "relevant_papers": [
      "2501.13072v2"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive review of recent deep learning architectures and foundation models used for deterministic and probabilistic weather forecasting and climate data analysis.",
    "relevant_papers": [
      "2501.06907v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How do modern deep learning foundation models categorize their training paradigms between deterministic predictive learning and probabilistic generative learning for atmospheric science applications?",
    "relevant_papers": [
      "2501.06907v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "evaluation of explainability methods for encoder based language models using metrics like human reasoning agreement robustness and consistency",
    "relevant_papers": [
      "2501.15374v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comparative analysis of lime shap and lrp techniques for transformer models like bert and deberta on sentiment analysis tasks",
    "relevant_papers": [
      "2501.15374v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do attention modules in different transformer layers contribute to memorization and generalization in large language models using attribution techniques?",
    "relevant_papers": [
      "2501.05078v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Analyzing the role of early versus late transformer layers in LLM memorization through systematic intervention and model attribution methods.",
    "relevant_papers": [
      "2501.05078v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the reliability of large language models as judges for assessing thematic alignment in AI generated summaries of open ended survey data",
    "relevant_papers": [
      "2501.08167v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comparing human rater performance with LLM as judge models using Cohen's kappa and Krippendorff's alpha for validating thematic summaries of unstructured text",
    "relevant_papers": [
      "2501.08167v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to generate task-specific LoRA weights using Conditional Variational Autoencoders and in-context meta-learning for efficient multi-task large language model adaptation.",
    "relevant_papers": [
      "2501.17635v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using meta-learning and CVAE generators to produce task-aware low-rank adaptation parameters for multiple downstream tasks without additional fine-tuning steps.",
    "relevant_papers": [
      "2501.17635v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use a world knowledge tree and self-reflection refinement for generating high quality synthetic data to align large language models",
    "relevant_papers": [
      "2501.12273v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "framework for scaling supervised fine-tuning data through knowledge driven synthesis and iterative self improvement for large scale language models",
    "relevant_papers": [
      "2501.12273v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Challenges and future research directions for integrating multimodal large language models and task-agnostic foundation models into cyber-physical systems and internet of things",
    "relevant_papers": [
      "2501.16368v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "What are the essential requirements and desiderata for developing domain-specific foundation models to bridge the gap in complex CPS and IoT sensor data analysis",
    "relevant_papers": [
      "2501.16368v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to use prioritized depth-first search and large language models for efficient unsupervised ontology matching in biomedical domains",
    "relevant_papers": [
      "2501.11441v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "combining embedding based retrieval with search heuristics to minimize large language model requests in ontology alignment tasks",
    "relevant_papers": [
      "2501.11441v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "taxonomy of interaction types between software developers and artificial intelligence tools for coding and system development",
    "relevant_papers": [
      "2501.08774v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "how do developers interact with generative AI and large language models throughout the software development lifecycle",
    "relevant_papers": [
      "2501.08774v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive analysis and taxonomy of common error types in large language models performing text-to-SQL tasks through in-context learning",
    "relevant_papers": [
      "2501.09310v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "New methods for efficient error detection and automated repair of SQL queries generated by large language models using in-context learning techniques",
    "relevant_papers": [
      "2501.09310v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive survey of large language models in bioinformatics covering training methodologies datasets and applications in drug discovery",
    "relevant_papers": [
      "2501.06271v1"
    ],
    "category": "q-bio.QM",
    "original_relevant_count": 1
  },
  {
    "query": "what are the current challenges and future directions for using biological language models in clinical research and vaccine development",
    "relevant_papers": [
      "2501.06271v1"
    ],
    "category": "q-bio.QM",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive review of test-time compute scaling methods for improving reasoning and robustness in system 1 and system 2 models",
    "relevant_papers": [
      "2501.02497v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "survey on inference time computation techniques such as self-correction and tree search to enhance the reasoning capabilities of large language models",
    "relevant_papers": [
      "2501.02497v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "integrating large language model multi-agent frameworks with Kafka and publish-subscribe systems for complex event processing in the Internet of Multimedia Things",
    "relevant_papers": [
      "2501.00906v2"
    ],
    "category": "cs.MA",
    "original_relevant_count": 1
  },
  {
    "query": "performance evaluation of Autogen based multi-agent systems for autonomous video query processing pipelines and complex event detection in multimedia data streams",
    "relevant_papers": [
      "2501.00906v2"
    ],
    "category": "cs.MA",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models respond to malicious jailbreaking prompts that use academic and scientific language to justify harmful social biases?",
    "relevant_papers": [
      "2501.14073v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the impact of using fabricated scientific arguments and misinterpreted psychological studies to bypass safety guardrails in models like GPT-4 and Llama3.",
    "relevant_papers": [
      "2501.14073v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "factors influencing high school students' acceptance of generative artificial intelligence in mathematics education using the technology acceptance model and structural equation modeling",
    "relevant_papers": [
      "2501.14779v2"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "research on the role of perceived enjoyment and compatibility in student adoption of generative AI tools for secondary school mathematics in Finland",
    "relevant_papers": [
      "2501.14779v2"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "dataset for evaluating vision language models on handwritten student math responses with teacher annotations and pedagogical reasoning",
    "relevant_papers": [
      "2501.14877v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how well do vision language models perform on reasoning tasks involving hand-drawn math diagrams and student problem solving strategies",
    "relevant_papers": [
      "2501.14877v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "adaptive interpolation methods for knowledge distillation to address the capacity gap between large teacher and small student language models",
    "relevant_papers": [
      "2501.16937v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to prevent mode collapse and mode averaging in language model distillation using temporally shifting intermediate probability distributions",
    "relevant_papers": [
      "2501.16937v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "automated methods for optimizing large language model pretraining data mixtures using token-count heuristics and LLM-estimated data utility",
    "relevant_papers": [
      "2501.11747v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve LLM training efficiency with UtiliMax and MEDU for compute-efficient data mixture selection across different training regimes",
    "relevant_papers": [
      "2501.11747v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Diffusion transformer models for joint image and video virtual try-on using temporal concatenation of garment and person features.",
    "relevant_papers": [
      "2501.11325v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to maintain temporal consistency in long video virtual try-on with overlapping clip-based inference and adaptive clip normalization.",
    "relevant_papers": [
      "2501.11325v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to mitigate systematic misalignment in reinforcement learning from human feedback using hindsight simulation of downstream outcomes",
    "relevant_papers": [
      "2501.08617v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "the impact of providing evaluators with simulated future consequences on reducing prediction-based reward hacking in language model alignment",
    "relevant_papers": [
      "2501.08617v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to perform precise free-form grounding across multiple images in multimodal large language models using the MGrounding dataset and MIG-Bench",
    "relevant_papers": [
      "2501.05767v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving multi-image grounding capabilities in MLLMs through end-to-end training and large-scale instruction-following datasets for complex visual scenes",
    "relevant_papers": [
      "2501.05767v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be integrated with high-order message passing on hypergraphs to improve missing value imputation in mixed-type tabular datasets?",
    "relevant_papers": [
      "2501.02191v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Framework for mixed-type data imputation using bidirectional message passing on cell-oriented hypergraphs combined with large language model adapters and progressive masking techniques.",
    "relevant_papers": [
      "2501.02191v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models and CTGANs be used to generate synthetic tabular student data for privacy-preserving learning analytics research?",
    "relevant_papers": [
      "2501.01793v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the performance of GPT-based models and CTGAN in creating synthetic educational datasets that maintain statistical and predictive utility for research.",
    "relevant_papers": [
      "2501.01793v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can optimized soft prompts in the textual embedding space be used to mitigate NSFW content generation in text-to-image models?",
    "relevant_papers": [
      "2501.03544v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Effective safety alignment for diffusion models using category-specific soft prompts to prevent sexually explicit or violent image generation without reducing inference speed.",
    "relevant_papers": [
      "2501.03544v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "using reinforcement learning and representation space guidance to optimize black-box jailbreak attacks against large language models like GPT-4o",
    "relevant_papers": [
      "2501.16727v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "interpretable reinforcement learning methods for LLM jailbreaking through embedding proximity analysis between benign and malicious prompt representations",
    "relevant_papers": [
      "2501.16727v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve diversity in mixture of experts for low-rank adaptation by applying orthogonal constraints to prevent expert collapse",
    "relevant_papers": [
      "2501.10062v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "using the Gram-Schmidt process and Stiefel manifold to enhance expert representation diversity in parameter-efficient fine-tuning with MoE-LoRA models",
    "relevant_papers": [
      "2501.10062v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can hierarchical autoregressive transformers combine character-level and word-level processing to improve language model robustness against spelling errors without using subword tokenizers?",
    "relevant_papers": [
      "2501.10322v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Large language models using character-to-word hierarchical architectures for faster domain adaptation and better performance on out-of-domain languages during continued pretraining.",
    "relevant_papers": [
      "2501.10322v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does the increase in energy loss in the final layer of large language models during reinforcement learning relate to reward hacking and reduced contextual relevance?",
    "relevant_papers": [
      "2501.19358v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Effective methods for mitigating reward hacking in RLHF by penalizing energy loss using an energy loss-aware PPO algorithm and its connection to entropy regularization.",
    "relevant_papers": [
      "2501.19358v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "certified robustness of large language models using randomized smoothing and knapsack solvers for worst-case analysis",
    "relevant_papers": [
      "2501.19040v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to calculate tight lower bounds for the worst-case robustness of stochastic defenses in large language models against adversarial attacks",
    "relevant_papers": [
      "2501.19040v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How do scaling laws for large language models change when incorporating differential privacy to optimize the compute, privacy, and utility tradeoff?",
    "relevant_papers": [
      "2501.18914v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Empirical analysis of scaling laws for training differentially private language models and predicting optimal hyper-parameters for various privacy budgets and compute scales.",
    "relevant_papers": [
      "2501.18914v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How consistent are large language models like GPT-4 and Claude 3.5 when answering complex legal questions based on real case facts?",
    "relevant_papers": [
      "2502.05196v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Measuring the instability of LLM outputs for legal decision making and case analysis when temperature settings are set to zero.",
    "relevant_papers": [
      "2502.05196v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Assessing the ability of large language models to trace execution paths and understand complex structural control flow in programming tasks",
    "relevant_papers": [
      "2501.16456v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Measuring the gap between code generation performance and structural reasoning in LLMs using execution traces and the CoCoNUT benchmark",
    "relevant_papers": [
      "2501.16456v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "impact of using problem-solving data versus general mathematical corpora during continued pre-training for large language models",
    "relevant_papers": [
      "2501.14002v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comparing mathematical reasoning performance of LLMs trained with tutorship amplification synthesis in the CPT and SFT stages",
    "relevant_papers": [
      "2501.14002v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to modify Chinchilla scaling laws to include inference latency costs and optimize model architecture for better accuracy and speed tradeoffs",
    "relevant_papers": [
      "2501.18107v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Training language models for inference efficiency by co-optimizing parameter count, training tokens, and model shape to achieve high performance with lower latency",
    "relevant_papers": [
      "2501.18107v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to use a hierarchical mixture-of-experts framework to model complex interactions between text and images for multimodal fake news detection.",
    "relevant_papers": [
      "2501.12431v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Advanced multimodal fake news detection methods focusing on interaction gating mechanisms and semantic alignment between different media types in social media posts.",
    "relevant_papers": [
      "2501.12431v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating instruction-following large language models for zero-shot and few-shot claim matching in the context of automated fact-checking systems",
    "relevant_papers": [
      "2501.10860v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models leverage natural language inference and paraphrase detection to perform claim matching for fact-checking pipelines?",
    "relevant_papers": [
      "2501.10860v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "accelerating diffusion model inference by exploiting temporal value similarity and low bit-width quantization of differences between consecutive time steps",
    "relevant_papers": [
      "2501.11211v1"
    ],
    "category": "cs.AR",
    "original_relevant_count": 1
  },
  {
    "query": "how to design a hardware accelerator that leverages temporal redundancy and distributive properties to speed up iterative diffusion model generation",
    "relevant_papers": [
      "2501.11211v1"
    ],
    "category": "cs.AR",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive survey of five hundred seventy two code benchmarks for large language models evaluating rigor and reliability from 2014 to 2025",
    "relevant_papers": [
      "2501.10711v4"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "The HOW2BENCH framework and checklists for improving the quality and reproducibility of code-related benchmarks used to evaluate artificial intelligence models",
    "relevant_papers": [
      "2501.10711v4"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How does complexity control and neuron condensation influence whether Transformers adopt reasoning-based or memory-based solutions for compositional generalization tasks?",
    "relevant_papers": [
      "2501.08537v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the internal information circuits and complexity bias that distinguish reasoning-based rules from memory-based mappings in Transformer architectures.",
    "relevant_papers": [
      "2501.08537v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using fine-tuned small language models like Llama 3 to generate realistic test inputs and discover inter-parameter dependencies for automated REST API testing",
    "relevant_papers": [
      "2501.08598v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the effectiveness of quantized small language models versus large models in detecting internal server errors and improving code coverage during REST API testing",
    "relevant_papers": [
      "2501.08598v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating end-to-end spoken language models on knowledge understanding and mathematical reasoning tasks through pure speech interactions and varying audio conditions.",
    "relevant_papers": [
      "2501.04962v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A benchmark for assessing the robustness and world knowledge of speech-based interaction models using speech-to-speech question answering datasets.",
    "relevant_papers": [
      "2501.04962v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "dynamic self-adaptation of large language models through singular value weight adjustments and reinforcement learning based expert mixing during inference",
    "relevant_papers": [
      "2501.06252v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "efficient alternative to LoRA for real-time task specific adaptation using a two-pass dispatch system and task-specific expert vectors",
    "relevant_papers": [
      "2501.06252v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "fine-tuning T5-small for scalable and topic-controlled question generation in education using quantization and data augmentation techniques",
    "relevant_papers": [
      "2501.05220v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "how to generate semantically aligned and topic-specific questions from paragraph contexts in educational settings without using large proprietary models",
    "relevant_papers": [
      "2501.05220v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "Open source TypeScript framework for building autonomous AI agents that can interact with blockchain and execute smart contract transactions.",
    "relevant_papers": [
      "2501.06781v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to integrate large language models with web3 applications using a modular agentic operating system for decentralized data interaction.",
    "relevant_papers": [
      "2501.06781v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Theoretical analysis of prompt optimization as an alternative to reinforcement learning from human feedback for large language model alignment.",
    "relevant_papers": [
      "2501.03486v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can prompt optimization be formulated as an optimization problem to provide theoretical performance guarantees for aligning frozen large language models?",
    "relevant_papers": [
      "2501.03486v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "adaptive sublayer skipping techniques to accelerate prefilling and decoding phases in long-context large language model inference",
    "relevant_papers": [
      "2501.02336v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve long-context LLM inference efficiency by identifying and skipping redundant sublayers based on real-time similarity metrics",
    "relevant_papers": [
      "2501.02336v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Dynamic structured pruning for large language models that adapts the pruning mask based on input instructions to improve performance on specific tasks",
    "relevant_papers": [
      "2501.02086v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using a sparse mask predictor to dynamically select relevant model parameters in large language models based on user instructions and task requirements",
    "relevant_papers": [
      "2501.02086v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "efficient large language model tool learning methods using parallel tool invocation and directed acyclic graph task structures to reduce inference time",
    "relevant_papers": [
      "2501.12432v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve LLM tool calling efficiency by dividing complex tasks into parallel sub-tasks and aggregating results using a divide-then-aggregate framework",
    "relevant_papers": [
      "2501.12432v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does in-execution self-debugging using intermediate states compare to post-execution debugging for large language model code generation tasks?",
    "relevant_papers": [
      "2501.12793v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Improving large language model programming performance through execution feedback from self-generated tests and analyzing the impact of test bias.",
    "relevant_papers": [
      "2501.12793v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement curriculum learning for large language models using perplexity difference between weak and strong models",
    "relevant_papers": [
      "2501.13126v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "dynamic pretraining data selection strategies based on changing model preferences to improve large language model performance",
    "relevant_papers": [
      "2501.13126v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How well do large language models like GPT-4 and Claude perform in scoring medical student communication skills using the Master Interview Rating Scale in OSCEs?",
    "relevant_papers": [
      "2501.13957v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarking different large language models and prompt engineering techniques for automating the evaluation of clinical communication skills in objective structured clinical examinations.",
    "relevant_papers": [
      "2501.13957v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can we identify and edit specific gender neurons in large language models to reduce bias without affecting general performance?",
    "relevant_papers": [
      "2501.14457v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A method for mitigating gender bias in LLMs through interpretable neuron editing using logit-based and causal strategies on the CommonWords dataset.",
    "relevant_papers": [
      "2501.14457v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Strategies for accelerating deep learning inference on resource-constrained edge devices through model compression, neural architecture search, and specialized deployment frameworks.",
    "relevant_papers": [
      "2501.15014v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive review of techniques like pruning, quantization, and compiler optimizations for improving energy efficiency and latency in edge AI deployments.",
    "relevant_papers": [
      "2501.15014v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve text-to-CAD generation using large language models and visual feedback rewards for parametric sequences?",
    "relevant_papers": [
      "2501.19054v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Training large language models for CAD model creation through alternating sequential learning and visual object perception stages.",
    "relevant_papers": [
      "2501.19054v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can internal representations and hidden states of large language models be used to predict the correctness of generated source code using white-box frameworks?",
    "relevant_papers": [
      "2501.12934v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating LLM code generation quality by analyzing latent signals and intermediate states within models like CodeLlama and DeepSeek-Coder through open-box assessment techniques.",
    "relevant_papers": [
      "2501.12934v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "how to measure and detect conversational bias in multi-agent systems of large language models using simulated echo chamber experiments",
    "relevant_papers": [
      "2501.14844v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating why traditional questionnaire-based bias detection fails to identify perspective shifts in large language model multi-agent interactions",
    "relevant_papers": [
      "2501.14844v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing graph retrieval-augmented generation for medical reasoning by integrating causal knowledge graphs with chain-of-thought steps",
    "relevant_papers": [
      "2501.14892v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "improving large language model explainability in high-stakes domains using causal graph filtering and multi-stage path retrieval methods",
    "relevant_papers": [
      "2501.14892v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can attackers manipulate voting-based LLM leaderboards like Chatbot Arena through model identification and adversarial voting?",
    "relevant_papers": [
      "2501.07493v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Security vulnerabilities and mitigation strategies for crowdsourced human evaluation benchmarks used to rank large language models.",
    "relevant_papers": [
      "2501.07493v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "improving mathematical reasoning in large language models by combining process-level and outcome-level binary feedback during training",
    "relevant_papers": [
      "2501.10799v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "methods for optimizing the intermediate steps of chain-of-thought reasoning using binary feedback and Kahneman-Tversky Optimization principles",
    "relevant_papers": [
      "2501.10799v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "critic-free reinforcement learning from human feedback using global advantage normalization for stabilizing large language model alignment and reducing memory overhead",
    "relevant_papers": [
      "2501.03262v9"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comparison of local versus global advantage normalization in RLHF algorithms for improving the training stability of critic-free policy optimization methods",
    "relevant_papers": [
      "2501.03262v9"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement multimodal large language model multi-agent systems using no-code platforms to reduce technical barriers for enterprise users",
    "relevant_papers": [
      "2501.00750v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Designing no-code frameworks for multimodal multi-agent systems to automate code generation and advanced retrieval-augmented generation in business processes",
    "relevant_papers": [
      "2501.00750v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "foundation model for automating systematic reviews through human-AI collaboration in clinical trial screening and medical data extraction",
    "relevant_papers": [
      "2501.16255v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "performance of large language models compared to clinicians in screening and extracting data from medical literature for systematic reviews",
    "relevant_papers": [
      "2501.16255v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to train neural networks with brain-like topographic organization using TopoLoss for high-performing vision and language models.",
    "relevant_papers": [
      "2501.16396v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Developing spatially organized artificial neural networks that mimic the localized feature processing and dimensionality of the human brain's cortex.",
    "relevant_papers": [
      "2501.16396v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Expert annotated datasets for legal information retrieval and contract clause drafting tasks involving complex clauses like indemnification and change of control.",
    "relevant_papers": [
      "2501.06582v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "What are the available benchmarks for evaluating retrieval systems that help lawyers find the most relevant precedent clauses for contract drafting?",
    "relevant_papers": [
      "2501.06582v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "decentralized framework for specialized large language models using blockchain to manage expert networks and ensure service quality through performance validation",
    "relevant_papers": [
      "2501.07288v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how can blockchain technology be integrated with fine-tuned large language models to create a decentralized and accountable expert network for various domains",
    "relevant_papers": [
      "2501.07288v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Multi-agent system for question answering using routing and planning mechanisms to handle multi-hop queries across multiple specialized knowledge bases",
    "relevant_papers": [
      "2501.07813v1"
    ],
    "category": "cs.MA",
    "original_relevant_count": 1
  },
  {
    "query": "How can routing and planning in multi-agent RAG systems improve accuracy for cross-domain queries while reducing retrieval overhead?",
    "relevant_papers": [
      "2501.07813v1"
    ],
    "category": "cs.MA",
    "original_relevant_count": 1
  },
  {
    "query": "How can crowdsourced LLM leaderboards like Chatbot Arena be manipulated by vote rigging and omnipresent strategies to artificially improve model rankings?",
    "relevant_papers": [
      "2501.17858v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Analysis of the vulnerability of Elo rating systems in large language model evaluations to targeted adversarial voting and strategic model ranking manipulation.",
    "relevant_papers": [
      "2501.17858v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve the transparency and verification of intermediate reasoning steps in multi-agent large language model systems using layered chain of thought prompting?",
    "relevant_papers": [
      "2501.18645v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Segmenting the chain of thought reasoning process into layers with external validation and user feedback for high-stakes LLM applications like medical triage.",
    "relevant_papers": [
      "2501.18645v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using crowdsourced metaphors to analyze public perception of artificial intelligence warmth and competence over time",
    "relevant_papers": [
      "2501.18045v3"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "how do open-ended mental models and metaphors predict trust and adoption of AI technologies in the United States",
    "relevant_papers": [
      "2501.18045v3"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "how to automatically verify the factual accuracy of large language model generated clinical summaries using patient electronic health records and retrieval augmented generation",
    "relevant_papers": [
      "2501.16672v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking automated systems for fact-checking medical discharge summaries against electronic health records using the VeriFact-BHC dataset",
    "relevant_papers": [
      "2501.16672v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing clinical reasoning in small language models through monte carlo tree search and self-evolved process supervision",
    "relevant_papers": [
      "2501.12051v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "self-evolving framework for medical reasoning using soft dual process reward models to improve intermediate step verification",
    "relevant_papers": [
      "2501.12051v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve the diversity of language model outputs in RLHF using curiosity-driven exploration and intrinsic rewards for novel states",
    "relevant_papers": [
      "2501.11463v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "techniques for balancing human preference alignment and response variety in large language models through extrinsic and intrinsic reward optimization",
    "relevant_papers": [
      "2501.11463v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Synthesizing long-context training data for large language models using negative document extension and interleaved distractors to improve long-range dependency modeling",
    "relevant_papers": [
      "2501.12766v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can we effectively train long-context large language models using synthetic data when there is a scarcity of naturally occurring long documents in pretraining corpora",
    "relevant_papers": [
      "2501.12766v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do next-generation intelligent tutoring systems like Socratic Playground use transformer-based models to improve upon traditional systems like AutoTutor?",
    "relevant_papers": [
      "2501.06682v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Using generative AI and JSON-based prompts to create adaptive Socratic tutoring systems that track student misconceptions and guide personalized reflection.",
    "relevant_papers": [
      "2501.06682v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to perform hierarchical code summarization for entire software repositories using local LLMs while incorporating business-specific domain context?",
    "relevant_papers": [
      "2501.07857v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Improving repository-level software documentation through syntax-aware hierarchical aggregation and business-grounded prompts for large language models.",
    "relevant_papers": [
      "2501.07857v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How can Simulation Theory and task decomposition be used to improve Theory of Mind reasoning in large language models for complex social scenarios?",
    "relevant_papers": [
      "2501.09056v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Enhancing LLM performance on higher-order Theory of Mind tasks through recursive perspective simulation and world model updating without additional model training.",
    "relevant_papers": [
      "2501.09056v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can a taxonomy of user information needs guide the integration of large language models, knowledge graphs, and search engines for question answering?",
    "relevant_papers": [
      "2501.06699v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Synergies between large language models and knowledge graphs for addressing diverse user search queries and information needs: a taxonomy and research roadmap.",
    "relevant_papers": [
      "2501.06699v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to use large language models and constraint logic programming to create interpretable clinical decision support systems for mental health diagnosis",
    "relevant_papers": [
      "2501.07653v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "combining LLMs with logic programs to improve the accuracy and interpretability of psychiatric diagnoses according to official medical manuals",
    "relevant_papers": [
      "2501.07653v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Controllable video generation using blob representations and masked 3D attention for improved spatial consistency and fine-grained object motion control in diffusion models.",
    "relevant_papers": [
      "2501.07647v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Methods for enhancing compositional text-to-video generation through layout planning with large language models and grounded visual primitives for smooth object transitions.",
    "relevant_papers": [
      "2501.07647v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Empirical study evaluating the proficiency of code large language models in recognizing, comprehending, and generating software design patterns during the development process.",
    "relevant_papers": [
      "2501.04835v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How do the internal biases of code LLMs affect their ability to follow project-specific coding standards and required architectural design patterns?",
    "relevant_papers": [
      "2501.04835v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How to achieve faster LLM inference on mobile devices by using both GPU and NPU simultaneously through heterogeneous parallel mechanisms and unified memory synchronization?",
    "relevant_papers": [
      "2501.14794v2"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "Optimization techniques for on-device large language model inference using heterogeneous processors and shared memory bandwidth on modern mobile system-on-chips.",
    "relevant_papers": [
      "2501.14794v2"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "applying superstatistical methods and q-Gaussian distributions to analyze the spatio-temporal fluctuations of dissolved oxygen in urban river systems",
    "relevant_papers": [
      "2501.07599v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "using the Informer transformer model and LightGBM for long-term prediction and feature analysis of dissolved oxygen levels in the River Thames",
    "relevant_papers": [
      "2501.07599v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Multi-agent systems using small language models and retrieval augmented generation for automated bioinformatics workflow creation and genomics data analysis",
    "relevant_papers": [
      "2501.06314v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Using fine-tuned language models and RAG to democratize bioinformatics analysis through local execution and personalized data integration",
    "relevant_papers": [
      "2501.06314v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "semi-supervised learning methods for fine-grained action recognition using temporal perturbation and teacher-student paradigms on FineGym and FineDiving datasets",
    "relevant_papers": [
      "2501.01245v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve fine-grained action recognition with limited labels using dual-level temporal representations and adaptive regulation for learning stabilization",
    "relevant_papers": [
      "2501.01245v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Scalable graph neural network framework for recommendation using distillation-based pruning to reduce edge count and embedding size",
    "relevant_papers": [
      "2501.03228v3"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve the efficiency and robustness of graph based recommender systems through hierarchical knowledge distillation and model pruning?",
    "relevant_papers": [
      "2501.03228v3"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the reliability and cultural sensitivity of large language models in generating texts related to historical monuments, ancient traditions, and cultural heritage",
    "relevant_papers": [
      "2501.02039v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking value misalignment in open-source large language models through automated and manual evaluation of cultural heritage tasks and historical fact representation",
    "relevant_papers": [
      "2501.02039v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve multimodal large language model performance on scientific table understanding using dynamic input image resolutions and domain-specific instruction tuning",
    "relevant_papers": [
      "2501.13042v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking numerical reasoning and structure recognition in scientific tables for multimodal models with the MMSci-Eval and MMSci-Pre datasets",
    "relevant_papers": [
      "2501.13042v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can internal chain of thought reasoning steps in customized large language models be manipulated to trigger backdoor attacks without changing the user prompt?",
    "relevant_papers": [
      "2501.18617v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Stealthy backdoor attacks on large language models that use latent triggers within the reasoning chain to influence model outputs without modifying input queries.",
    "relevant_papers": [
      "2501.18617v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve contextual faithfulness in retrieval-augmented large language models using attention heads and retrieval head induced optimization",
    "relevant_papers": [
      "2501.13573v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing faithfulness in long-form question answering by training LLMs to discriminate unfaithful generations through retrieval head masking and contrastive decoding",
    "relevant_papers": [
      "2501.13573v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use language-guided cross-attention mechanisms to prune redundant vision tokens and reduce inference latency in multi-modal large language models like LLaVA.",
    "relevant_papers": [
      "2501.13652v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A simple plug-and-play method for vision token pruning in MLLMs that calculates token importance based on their interaction with the language modality.",
    "relevant_papers": [
      "2501.13652v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use Transformer models for controllable multitrack MIDI generation using track-level infilling and attribute-based constraints?",
    "relevant_papers": [
      "2501.17011v2"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "Generative music models for computer-assisted composition that support track-level infilling and control over note density and polyphony.",
    "relevant_papers": [
      "2501.17011v2"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "How can structured prompt design and in-context learning techniques enhance the performance of general large language models on text-attributed graph tasks?",
    "relevant_papers": [
      "2501.15755v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the effectiveness of general-purpose large language models on text-attributed graphs through structured in-context learning prompts versus specialized graph-tuned models.",
    "relevant_papers": [
      "2501.15755v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "research on context-aware safety benchmarks for large language models that incorporate situational integrity and human judgment across different interaction scenarios",
    "relevant_papers": [
      "2501.14940v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to evaluate large language model safety by considering context to prevent the undesired refusal of harmless queries in safe environments",
    "relevant_papers": [
      "2501.14940v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use knowledge graph retrieval augmented generation to reduce hallucinations and improve precision in large language model based schema matching",
    "relevant_papers": [
      "2501.08686v1"
    ],
    "category": "cs.DB",
    "original_relevant_count": 1
  },
  {
    "query": "graph based retrieval methods for resolving semantic ambiguities and conflicts in complex domain specific schema matching scenarios",
    "relevant_papers": [
      "2501.08686v1"
    ],
    "category": "cs.DB",
    "original_relevant_count": 1
  },
  {
    "query": "How to measure and reduce redundancy in multi-modality large language model benchmarks across different evaluation dimensions and test question sets?",
    "relevant_papers": [
      "2501.13953v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Quantitative analysis of redundant test questions and overlapping domains in current multimodal large language model evaluation frameworks and benchmark construction principles.",
    "relevant_papers": [
      "2501.13953v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does changing the reward function shape with an alpha parameter improve direct alignment algorithms like DPO and SimPO?",
    "relevant_papers": [
      "2501.03884v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using AlphaPO to mitigate likelihood displacement and over-optimization in large language model alignment through fine-grained reward shaping.",
    "relevant_papers": [
      "2501.03884v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Recent survey of foundation model based agents capable of controlling computer software and mobile devices through GUI interactions and visual observations.",
    "relevant_papers": [
      "2501.16150v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "What are the current research gaps and taxonomies for digital agents designed to perform complex tasks on computers using screen-based observations and low-level actions?",
    "relevant_papers": [
      "2501.16150v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "machine learning models and explainable AI techniques for detecting ChatGPT generated content in student work using the CyberHumanAI dataset",
    "relevant_papers": [
      "2501.03203v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comparison of XGBoost and Random Forest performance against GPTZero for identifying AI-generated versus human-written text in academic environments",
    "relevant_papers": [
      "2501.03203v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can a multi-agent LLM system provide decision interpretability and optimize PPA performance in the generation of Register-Transfer Level hardware designs?",
    "relevant_papers": [
      "2501.05470v1"
    ],
    "category": "cs.AR",
    "original_relevant_count": 1
  },
  {
    "query": "Improving the reliability of LLM-based RTL code generation through collaborative agent squads for exploration, implementation, and verification stages.",
    "relevant_papers": [
      "2501.05470v1"
    ],
    "category": "cs.AR",
    "original_relevant_count": 1
  },
  {
    "query": "efficient document compression methods for retrieval augmented generation using sequence level knowledge distillation without extensive pretraining or annotated data",
    "relevant_papers": [
      "2501.16075v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to achieve high compression rates for RAG context windows using sequence-level knowledge distillation from document-based questions for large language models",
    "relevant_papers": [
      "2501.16075v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how does using chain of thought reasoning influence the confidence scores and probability estimations of large language models in multiple choice questions",
    "relevant_papers": [
      "2501.09775v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "impact of providing reasoning steps on the overconfidence of large language models when answering multiple choice benchmark questions incorrectly",
    "relevant_papers": [
      "2501.09775v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how can chain of thought prompting be used to generate empathetic responses in spoken dialogue systems without using specialized question-answering datasets",
    "relevant_papers": [
      "2501.10937v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "two-stage training approach for speech-based large language models to perceive emotional cues and produce empathetic dialogue using chain of thought reasoning",
    "relevant_papers": [
      "2501.10937v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating instruction tuning data quality by measuring the reward gap between chosen and rejected responses to filter low quality prompts for better model performance",
    "relevant_papers": [
      "2501.18578v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve synthetic dataset integrity by filtering out low quality input prompts using response variance and preference pair quality metrics in large language models",
    "relevant_papers": [
      "2501.18578v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the performance of causal sequence decoding models using next-token prediction for numeric regression tasks and density estimation",
    "relevant_papers": [
      "2501.19383v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How do language model decoders trained with cross-entropy loss compare to standard pointwise heads for continuous value prediction and regression",
    "relevant_papers": [
      "2501.19383v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "automatic prompt engineering for multi-step LLM pipelines using textual gradients and automatic differentiation techniques",
    "relevant_papers": [
      "2501.16673v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to optimize complex LLM workflows with feedback-based prompt updates for multi-hop retrieval and autonomous agents",
    "relevant_papers": [
      "2501.16673v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "training-free approach to long video understanding using continuous-time long-term memory consolidation for video language models like VideoChat2",
    "relevant_papers": [
      "2501.19098v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to process arbitrarily long videos in video question answering tasks using continuous attention and memory consolidation mechanisms without retraining",
    "relevant_papers": [
      "2501.19098v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language model serving systems achieve both prefix cache locality and fair resource allocation among different clients?",
    "relevant_papers": [
      "2501.14312v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "The impact of Deficit Longest Prefix Match and D2LPM on throughput and latency in multi-client distributed LLM inference workloads.",
    "relevant_papers": [
      "2501.14312v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "using entropy-based selective classifiers to estimate confidence and detect errors in large language model text-to-sql generation systems",
    "relevant_papers": [
      "2501.09527v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "comparing model calibration and error detection performance between encoder-decoder T5 and large language models like GPT-4 for natural language to SQL tasks",
    "relevant_papers": [
      "2501.09527v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve retrieval-augmented generation performance using logits-guided multi-granular document chunking for better passage segmentation",
    "relevant_papers": [
      "2501.09940v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "impact of multi-granular and self-contained document chunking strategies on the efficiency of dense passage retrieval in RAG systems",
    "relevant_papers": [
      "2501.09940v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Performance comparison of monolingual versus multilingual BERT models for part-of-speech tagging tasks in the low-resource Uzbek language",
    "relevant_papers": [
      "2501.10107v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Introduction of a public UPOS-tagged dataset and evaluation of context-sensitive BERT models for part-of-speech tagging in Uzbek",
    "relevant_papers": [
      "2501.10107v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "AI-powered learning platform using Retrieval-Augmented Generation and agentic workflows to support diverse student needs and career preparation in cybersecurity education.",
    "relevant_papers": [
      "2501.09709v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "How can agentic Large Language Model assistants provide personalized technical skills and career advice for non-traditional students in cybersecurity degree programs?",
    "relevant_papers": [
      "2501.09709v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "How does the Graph-PReFLexOR framework use graph reasoning and symbolic abstraction to enhance automated scientific discovery and interdisciplinary hypothesis generation?",
    "relevant_papers": [
      "2501.08120v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Integrating category theory and knowledge graph growth strategies for transparent autonomous reasoning and cross-domain knowledge expansion in large language models.",
    "relevant_papers": [
      "2501.08120v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "high quality Chinese datasets for large language model pretraining and fine-tuning using filtered web content and synthetic textbook data",
    "relevant_papers": [
      "2501.08197v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "curated Chinese language model training corpora for improving performance on C-Eval through filtered web sources and diverse chat format data",
    "relevant_papers": [
      "2501.08197v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve many-shot in-context learning performance in large language models using differentiated learning and reweighting objectives to mitigate data noise?",
    "relevant_papers": [
      "2501.04070v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for addressing performance degradation in many-shot in-context learning using differentiated optimization objectives and the new ICL-50 large-scale evaluation benchmark.",
    "relevant_papers": [
      "2501.04070v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve context selection in multimodal RAG using relevancy scores instead of CLIP embedding cosine similarity?",
    "relevant_papers": [
      "2501.04695v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Adaptive context selection and re-ranking methods for improving retrieval accuracy in vision-language models and multimodal RAG systems.",
    "relevant_papers": [
      "2501.04695v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarks for evaluating adversarial robustness and compositional reasoning in audio-visual multi-modal large language models",
    "relevant_papers": [
      "2501.02135v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve the reliability of audio-visual LLMs using calibrated preference optimization training strategies against perturbed inputs",
    "relevant_papers": [
      "2501.02135v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve GUI action grounding in novel environments using autonomous exploration and reinforcement learning for data collection.",
    "relevant_papers": [
      "2501.13896v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using MLLM based agents and Q-value-Incentive In-Context Reinforcement Learning to align GUI models to unseen software environments.",
    "relevant_papers": [
      "2501.13896v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "impact of learning rates and training data size on the out-of-domain table understanding and general capabilities of instruction-tuned large language models",
    "relevant_papers": [
      "2501.14693v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to optimize instruction tuning for table tasks while maintaining the general reasoning and performance of the base language model",
    "relevant_papers": [
      "2501.14693v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use rank-wise mixture of experts in LoRA to improve multi-task learning performance and knowledge sharing in large language models?",
    "relevant_papers": [
      "2501.15103v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient parameter fine-tuning for multiple tasks by treating individual ranks as experts within a unified LoRA framework with dynamic activation.",
    "relevant_papers": [
      "2501.15103v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "evaluation datasets for measuring the instruction following capabilities of multilingual and cross-lingual information retrieval systems across different languages",
    "relevant_papers": [
      "2501.19264v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "how do multilingual retrieval models perform when given complex natural language instructions compared to traditional short web style search queries",
    "relevant_papers": [
      "2501.19264v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "Applying social choice theory and maximal lotteries to improve large language model alignment and address the limitations of standard RLHF methods",
    "relevant_papers": [
      "2501.19266v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How does Nash Learning from Human Feedback approximate maximal lotteries to provide robust alignment against non-transitive preferences and irrelevant alternatives",
    "relevant_papers": [
      "2501.19266v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "LLM retrieval methods that align complex questions with data collection organization to achieve retrieve-all-at-once performance instead of iterative agentic searching.",
    "relevant_papers": [
      "2501.18539v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use relationship exploration between data objects to improve retrieval accuracy for complex multi-source questions in RAG systems beyond simple query decomposition.",
    "relevant_papers": [
      "2501.18539v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can decoder-only LLMs be used for extractive schema linking to improve efficiency and performance in large-scale Text-to-SQL generation tasks?",
    "relevant_papers": [
      "2501.17174v1"
    ],
    "category": "cs.DB",
    "original_relevant_count": 1
  },
  {
    "query": "Improving schema linking accuracy and computational efficiency for cross-database Text-to-SQL using an extractive approach with decoder-only large language models.",
    "relevant_papers": [
      "2501.17174v1"
    ],
    "category": "cs.DB",
    "original_relevant_count": 1
  },
  {
    "query": "graph prompt tuning for heterophily graphs using distribution-aware modules and low-rank adaptation for message passing",
    "relevant_papers": [
      "2501.15142v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to use hop-specific prompts and generalized low-rank adaptation to improve pre-trained graph neural network performance",
    "relevant_papers": [
      "2501.15142v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement hierarchical backpressure for autoscaling large language model serving while balancing interactive and batch request SLO requirements?",
    "relevant_papers": [
      "2501.08090v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "Improving GPU utilization and SLO attainment in LLM inference workloads through SLO-aware hierarchical autoscaling of instances and batch sizes.",
    "relevant_papers": [
      "2501.08090v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "zero-shot hallucination detection in large language models using attention mechanisms and self-reflection consistency scores for improved efficiency",
    "relevant_papers": [
      "2501.09997v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how can attention weights and query categorization be used to identify hallucinations in generative AI through self-reflection passes",
    "relevant_papers": [
      "2501.09997v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How vulnerable is GraphRAG to data poisoning attacks compared to standard retrieval augmented generation methods?",
    "relevant_papers": [
      "2501.14050v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluation of poisoning attacks on knowledge graph based RAG systems using relation injection and enhancement strategies.",
    "relevant_papers": [
      "2501.14050v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to evaluate the coverage of diverse factual information and atomic claims in long-form text generation produced by large language models?",
    "relevant_papers": [
      "2501.03545v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Automated framework for measuring information diversity and aspect alignment in long-form AI generated responses using atomic claim verification and retrieval.",
    "relevant_papers": [
      "2501.03545v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "theoretical framework for contrastive pre-training using approximate sufficient statistics and its application to zero-shot classification",
    "relevant_papers": [
      "2501.04641v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "sample complexity guarantees and joint generative hierarchical models for multimodal learning in vision-language transformers",
    "relevant_papers": [
      "2501.04641v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can retrieval-augmented dialogue knowledge aggregation improve the expressiveness and empathetic feedback of conversational speech synthesis systems?",
    "relevant_papers": [
      "2501.06467v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A multi-granularity graph-based approach for aggregating semantic and style knowledge from stored dialogues to enhance expressive conversational text-to-speech.",
    "relevant_papers": [
      "2501.06467v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how can large language models be used for zero-shot and few-shot source code authorship attribution across different programming languages",
    "relevant_papers": [
      "2501.08165v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "large scale source code authorship identification using a tournament style approach with llms to overcome input token constraints",
    "relevant_papers": [
      "2501.08165v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Auto-regressive transformer models for graph generation using sequence representations of node and edge sets for molecular design.",
    "relevant_papers": [
      "2501.01073v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to use pre-trained transformers for graph property prediction and goal-oriented generation via next-token prediction on graph sequences.",
    "relevant_papers": [
      "2501.01073v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "framework for evaluating multimodal retrieval augmented generation performance using relevancy and correctness scores to detect hallucinations",
    "relevant_papers": [
      "2501.03995v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "measuring the reliability of multimodal RAG systems by assessing retrieved image relevance and the accuracy of generated responses",
    "relevant_papers": [
      "2501.03995v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "detailed training logs and implementation strategies for building a sixty-five billion parameter fully open source large language model like K2 DIAMOND",
    "relevant_papers": [
      "2501.07124v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "open source resources and technical documentation for addressing training loss spikes and longitudinal performance analysis in high capacity large language models",
    "relevant_papers": [
      "2501.07124v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "hybrid framework for automated log analysis using uncertainty estimation to delegate tasks between large and small language models for cost efficiency",
    "relevant_papers": [
      "2501.11031v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve log analysis performance using large language models with retrieval-augmented prompts and adaptive selection based on small model confidence",
    "relevant_papers": [
      "2501.11031v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "self-supervised quantization methods for integrating knowledge graph structural information into large language models using discrete tokens",
    "relevant_papers": [
      "2501.18119v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use quantized entity representations to improve large language model performance on link prediction and triple classification tasks",
    "relevant_papers": [
      "2501.18119v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "generalizing the logistic loss function for language modeling by using f-divergences and the f-softargmax operator for next-token prediction",
    "relevant_papers": [
      "2501.18537v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating alpha-divergence based loss functions and parallelizable bisection algorithms for softargmax as alternatives to standard cross-entropy in large language models",
    "relevant_papers": [
      "2501.18537v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How do current large language models perform on tasks evaluating episodic memory recall and reasoning across temporal and spatial dimensions?",
    "relevant_papers": [
      "2501.13121v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive evaluation framework and synthetic benchmark for assessing episodic memory and event-based reasoning capabilities in state-of-the-art AI models.",
    "relevant_papers": [
      "2501.13121v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do function encoders using least-squares optimization compare to transformers and meta-learning for inductive transfer tasks in Hilbert spaces?",
    "relevant_papers": [
      "2501.18373v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "A geometric approach to transfer learning characterizing interpolation and extrapolation in Hilbert spaces through the universal approximation of function encoders",
    "relevant_papers": [
      "2501.18373v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does a block causal transformer architecture improve next frame prediction for fluid dynamics foundation models compared to standard tokenization methods?",
    "relevant_papers": [
      "2501.18972v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Foundation models for fluid dynamics using block causal transformers for autoregressive prediction of incompressible and compressible Navier-Stokes equations.",
    "relevant_papers": [
      "2501.18972v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to obtain valid confidence intervals when using machine learning predictions for imputed covariates in stratified or clustered sampling designs",
    "relevant_papers": [
      "2501.18577v3"
    ],
    "category": "stat.ME",
    "original_relevant_count": 1
  },
  {
    "query": "prediction powered inference bootstrap methods for debiasing machine learning imputations in downstream regressions with nonuniformly sampled data",
    "relevant_papers": [
      "2501.18577v3"
    ],
    "category": "stat.ME",
    "original_relevant_count": 1
  },
  {
    "query": "How can mixture of experts architectures improve the performance and efficiency of multi-modal large language models for 3D vision and spatial reasoning?",
    "relevant_papers": [
      "2501.16698v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using rectified flow pose diffusion and multi-modal LLMs for embodied task planning and question answering in three-dimensional environments.",
    "relevant_papers": [
      "2501.16698v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using parameter trust regions to mitigate knowledge conflicts and performance degradation in task arithmetic based model merging approaches",
    "relevant_papers": [
      "2501.15065v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "training-free techniques for multi-task model merging that project task vectors onto directions with low sensitivity to task-specific losses",
    "relevant_papers": [
      "2501.15065v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to use probabilistic federated search to improve retrieval-augmented generation across multiple enterprise product domains",
    "relevant_papers": [
      "2501.14998v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving RAG performance for multi-product QA by aggregating query-domain and query-passage relevance instead of exhaustive searching",
    "relevant_papers": [
      "2501.14998v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "multilingual dataset for evaluating consistency of large language model health responses across English German Chinese and Turkish",
    "relevant_papers": [
      "2501.14719v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "methodology for comparing health-related inquiry consistency in large language models across different languages using the expanded HealthFC dataset",
    "relevant_papers": [
      "2501.14719v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can LLM-based multi-agent systems automate the entire film production process including scriptwriting and virtual cinematography in 3D environments?",
    "relevant_papers": [
      "2501.12909v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A collaborative framework using multiple language model agents for autonomous filmmaking and camera setup optimization in virtual 3D spaces.",
    "relevant_papers": [
      "2501.12909v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to precisely control camera extrinsic and intrinsic parameters in text-to-image generation models without using 3D geometry or multi-view datasets?",
    "relevant_papers": [
      "2501.12910v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Methods for adjusting camera angles and lens distortions in generative image models using specific camera parameters instead of text prompt engineering.",
    "relevant_papers": [
      "2501.12910v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can attackers use training loss information from proprietary fine-tuning interfaces to perform optimization-based prompt injection on closed-source LLMs?",
    "relevant_papers": [
      "2501.09798v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Vulnerability of Google Gemini models to adversarial prompt optimization using feedback signals and greedy search algorithms from remote fine-tuning APIs.",
    "relevant_papers": [
      "2501.09798v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Diffusion models for large scale neural network parameter generation using recurrent mechanisms to synthesize high dimensional weights on a single GPU",
    "relevant_papers": [
      "2501.11587v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for generating full network parameters for vision transformers and large language models through token partitioning and conditional diffusion processes",
    "relevant_papers": [
      "2501.11587v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can abductive reasoning be used to infer user personas from preference data to improve personalization in large language models?",
    "relevant_papers": [
      "2501.11549v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving LLM personalization by training on preference datasets augmented with inferred user needs and latent personas from rejected response pairs.",
    "relevant_papers": [
      "2501.11549v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can hierarchical attention mechanisms be used to improve spatial and temporal feature alignment for automated video-to-music generation across diverse video styles?",
    "relevant_papers": [
      "2501.09972v1"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "Recent approaches to zero-shot video-to-music generation using spatial-temporal alignment and new objective metrics for evaluating correspondence between video and generated audio.",
    "relevant_papers": [
      "2501.09972v1"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "LLM text-to-SQL framework using statistical conformal prediction on hidden layers for reliable schema linking and adaptive abstention mechanisms",
    "relevant_papers": [
      "2501.10858v1"
    ],
    "category": "cs.DB",
    "original_relevant_count": 1
  },
  {
    "query": "How can human-in-the-loop and branching point prediction improve the reliability of natural language interfaces for databases in ambiguous contexts?",
    "relevant_papers": [
      "2501.10858v1"
    ],
    "category": "cs.DB",
    "original_relevant_count": 1
  },
  {
    "query": "How can optimal transport-based alignment loss and attention modules improve semantic extraction in LLM-based audio-visual captioning systems?",
    "relevant_papers": [
      "2501.09291v2"
    ],
    "category": "cs.MM",
    "original_relevant_count": 1
  },
  {
    "query": "State-of-the-art methods for integrating visual cues into audio captioning using large language models and optimal transport assignment maps.",
    "relevant_papers": [
      "2501.09291v2"
    ],
    "category": "cs.MM",
    "original_relevant_count": 1
  },
  {
    "query": "How are large language models being used to detect hate speech, cyberbullying, and other forms of textual cyber abuse on social media?",
    "relevant_papers": [
      "2501.05443v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Recent advancements in using cutting edge language models for the automated detection of online abuse and the ethical implications of LLM generated harmful content.",
    "relevant_papers": [
      "2501.05443v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to generate counterfactual and contrastive explanations for deep convolutional neural networks by analyzing internal filter activations instead of using input perturbations",
    "relevant_papers": [
      "2501.06831v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "model intrusive methods for interpreting DCNN image classifiers using contrastive and counterfactual explanations based on internal filter importance and feature concepts",
    "relevant_papers": [
      "2501.06831v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How do dimensionality reduction techniques like PCA and UMAP demonstrate the nonlinear nature of refusal behavior in large language models?",
    "relevant_papers": [
      "2501.08145v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Analyzing the multidimensional and layer-wise characteristics of refusal mechanisms in LLMs beyond the assumption of linear separability for better alignment.",
    "relevant_papers": [
      "2501.08145v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to optimize the scaling factor in low-rank adaptation to prevent performance degradation as the rank size increases during LLM fine-tuning",
    "relevant_papers": [
      "2501.04315v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "efficient methods for accuracy recovery when fine-tuning pruned large language models using reliability optimization for rank adaptation",
    "relevant_papers": [
      "2501.04315v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Dataset for long-form video understanding instruction tuning using hierarchical tree structures and self-revision to generate high quality QA pairs.",
    "relevant_papers": [
      "2501.05037v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve video large language model performance on long-term context benchmarks like EgoSchema and VideoMME using supervised fine-tuning.",
    "relevant_papers": [
      "2501.05037v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "evaluation of ChatGPT's ability to generate FEniCS and MATLAB code for finite element analysis of hydro-mechanical coupled problems in geotechnical engineering",
    "relevant_papers": [
      "2501.02199v1"
    ],
    "category": "math.NA",
    "original_relevant_count": 1
  },
  {
    "query": "using prompt engineering to implement numerical models for unsaturated soil mechanics and seepage through large language models like ChatGPT",
    "relevant_papers": [
      "2501.02199v1"
    ],
    "category": "math.NA",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive survey of recent advances in deep learning for tabular data including neural architectures and self-supervised representation learning methods",
    "relevant_papers": [
      "2501.03540v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "review of foundation models and specialized transformer architectures for tabular data representation learning and data augmentation strategies",
    "relevant_papers": [
      "2501.03540v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can image-to-text conversion and chain-of-thought improve simple-to-hard generalization in vision language models for algorithmic visual reasoning tasks?",
    "relevant_papers": [
      "2501.02669v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Methods to mitigate modality imbalance in vision language models and promote simple-to-hard generalization on algorithmic tasks like grid navigation and table readout.",
    "relevant_papers": [
      "2501.02669v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can deep neural decision trees and forests be used to improve the robustness of COVID-19 detection from cough sound recordings across multiple datasets?",
    "relevant_papers": [
      "2501.01117v1"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "A comparative study on using deep neural decision forests and recursive feature elimination for classifying COVID-19 cough sounds in diverse geographic and demographic populations.",
    "relevant_papers": [
      "2501.01117v1"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "how to use program-driven verification and dual refinement to improve self-correction performance of large language models in complex reasoning",
    "relevant_papers": [
      "2501.01264v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing large language model self-correction using self-generated pseudo-programs for verification and refinement in mathematical and instruction-following tasks",
    "relevant_papers": [
      "2501.01264v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be integrated with symbolic action languages to improve performance on complex action reasoning and systematic search tasks?",
    "relevant_papers": [
      "2501.00830v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using action languages to bridge the gap between natural language understanding and symbolic reasoning for complex action-based problem solving and automated planning.",
    "relevant_papers": [
      "2501.00830v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve the robustness of retrieval augmented generation against corpus poisoning attacks using cluster filtering and self assessment techniques",
    "relevant_papers": [
      "2501.00879v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "a training free plug and play framework for filtering malicious and irrelevant documents in RAG systems to enhance retrieval accuracy",
    "relevant_papers": [
      "2501.00879v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models use iterative self-questioning to improve open-domain news timeline summarization and document retrieval from diverse online sources?",
    "relevant_papers": [
      "2501.00888v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "State of the art methods for generating chronological news summaries using iterative document retrieval and causal reasoning with large language models.",
    "relevant_papers": [
      "2501.00888v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to perform efficient stylized question answering in large language models using representation editing and style-relevant subspace disentanglement without fine-tuning?",
    "relevant_papers": [
      "2501.14371v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Lightweight and train-free methods for controlling LLM response styles through adaptive steering vectors in a disentangled style subspace for role-playing applications.",
    "relevant_papers": [
      "2501.14371v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models resolve conflicts between editing new information and unlearning outdated knowledge using a knowledge codebook framework?",
    "relevant_papers": [
      "2502.00158v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A retrieval-based framework for updating large language models that utilizes task-specific codebook memories to manage knowledge editing and unlearning tasks.",
    "relevant_papers": [
      "2502.00158v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to generate efficient Shapley value explanations for time-series transformers using a unified pre-training framework to avoid inference-time overhead?",
    "relevant_papers": [
      "2501.15070v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Time-series transformer architectures that use Shapley-based pre-training for simultaneous prediction and feature importance estimation in safety-critical applications.",
    "relevant_papers": [
      "2501.15070v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "speculative decoding for large language models using draft and target models with different vocabularies and heterogeneous tokenization schemes",
    "relevant_papers": [
      "2502.05202v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement lossless speculative decoding for accelerating llm inference using off-the-shelf drafter models with mismatched vocabulary sets",
    "relevant_papers": [
      "2502.05202v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing neural theorem proving with large language models through fine-grained proof structure analysis and automated tool integration at multiple granularities",
    "relevant_papers": [
      "2501.18310v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "efficient recursive proving algorithms for automated theorem synthesis using deep learning models on the miniF2F benchmark for Isabelle and Lean",
    "relevant_papers": [
      "2501.18310v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "autonomous red teaming of multi-host networks using large language models and high-level declarative task planning for enterprise security exercises",
    "relevant_papers": [
      "2501.16466v4"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the effectiveness of LLM-based penetration testing systems on large-scale emulated multi-host network environments with MHBench",
    "relevant_papers": [
      "2501.16466v4"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "dataset for panoptic segmentation-captioning with instance-specific descriptions generated by GPT-4V to improve fine-grained visual understanding in multimodal models",
    "relevant_papers": [
      "2501.13893v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "improving region-level comprehension and language generation in large multimodal models through supervised fine-tuning on pixel-aligned object captions",
    "relevant_papers": [
      "2501.13893v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how do large language models handle time-sensitive factual knowledge and maintain consistency when prompts are slightly changed or perturbed",
    "relevant_papers": [
      "2501.12774v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving the accuracy and consistency of large language models for factual knowledge using entity-aware fine-tuning and neurosymbolic approaches",
    "relevant_papers": [
      "2501.12774v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to evaluate large language models in personalized recommendation by separating user rating bias and item quality from true preferences using the PerRecBench framework?",
    "relevant_papers": [
      "2501.13391v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Assessing the capability of large language models to capture personal user preferences through grouped ranking tasks instead of traditional rating prediction and regression metrics.",
    "relevant_papers": [
      "2501.13391v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "applying test-time training to large language models for improving clinical reasoning and domain adaptation in complex medical diagnostic tasks",
    "relevant_papers": [
      "2501.09213v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve medical reasoning in LLMs using high quality synthetic dialogue data and direct preference optimization techniques",
    "relevant_papers": [
      "2501.09213v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "training medical patient simulators using dialogue strategies from real doctor-patient conversations to evaluate large language model diagnostic performance",
    "relevant_papers": [
      "2501.09484v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "investigating the relationship between medical inquiry quality and diagnostic accuracy in large language models using Liebig's law of the minimum",
    "relevant_papers": [
      "2501.09484v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Fine-grained medical vision-language pre-training using large language models to extract disease features from reports for improved image-text alignment.",
    "relevant_papers": [
      "2501.10775v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve medical image analysis through knowledge injection and semantic similarity matrices in contrastive vision-language pre-training models.",
    "relevant_papers": [
      "2501.10775v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Using LLaVA and multimodal-to-text prompt engineering for identifying and classifying global navigation satellite system interference through feature analysis",
    "relevant_papers": [
      "2501.05079v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models and feature embeddings be applied to the characterization and monitoring of GNSS jamming interferences in signal processing?",
    "relevant_papers": [
      "2501.05079v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating sparse autoencoders for llm interpretability using polysemous word representation and monosemantic feature quality",
    "relevant_papers": [
      "2501.06254v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how do sparse autoencoders distinguish different meanings of polysemous words and what metrics assess their semantic representational power",
    "relevant_papers": [
      "2501.06254v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve self-adaptation in configurable systems using lifelong learning and knowledge distillation for better performance under time-varying workloads",
    "relevant_papers": [
      "2501.00840v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "techniques for continuous configuration optimization in intelligent software systems through distilled knowledge seeding and lifelong planning to handle dynamic environment changes",
    "relevant_papers": [
      "2501.00840v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How to use large-scale synthetic data to improve spoken dialogue systems in complex scenarios involving audio events and musical contexts?",
    "relevant_papers": [
      "2501.01384v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Multi-turn spoken dialogue systems utilizing heterogeneous feature fusion and synthetic data for enhanced performance in diverse real-world conversational scenarios.",
    "relevant_papers": [
      "2501.01384v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "multi-agent conversational bandit framework for online large language model response selection and alignment with dynamic user preferences",
    "relevant_papers": [
      "2501.01849v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve online evaluation and filtering of LLM responses using multi-agent systems and adaptive preference mechanisms",
    "relevant_papers": [
      "2501.01849v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How to evaluate and improve long-context language models for in-context retrieval and reasoning tasks when dealing with confounding or noisy information?",
    "relevant_papers": [
      "2501.08248v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving retrieval performance in long-context LLMs through retrieval-attention-probing and fine-tuning methods that combine retrieval and generation head training.",
    "relevant_papers": [
      "2501.08248v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive benchmarks for evaluating multi-modal large language models on face and human understanding tasks using hierarchical ability taxonomies",
    "relevant_papers": [
      "2501.01243v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "research on the performance of mainstream multi-modal assistants in social interaction scenarios involving face and human image analysis",
    "relevant_papers": [
      "2501.01243v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive benchmark for evaluating large language models across multiple Indian languages using multi-task language understanding tasks and reasoning challenges",
    "relevant_papers": [
      "2501.15747v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "standardized evaluation framework for testing the performance of multilingual large language models on diverse Indic languages including Hindi Bengali and Marathi",
    "relevant_papers": [
      "2501.15747v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use unlearning strategies and layer-level patching to mitigate jailbreak attacks by reducing affirmative token generation in large language models?",
    "relevant_papers": [
      "2501.02629v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Defending against LLM jailbreak attacks by identifying vulnerable layers that produce affirmative tokens and applying adversarial self-exposure for model patching.",
    "relevant_papers": [
      "2501.02629v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Integrating domain knowledge from large language models with Bayesian optimization to improve experimental search in high dimensional scientific research tasks",
    "relevant_papers": [
      "2501.16224v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used to guide Bayesian optimization and provide reasoning for search strategies in complex experimental design?",
    "relevant_papers": [
      "2501.16224v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "synthetic data generation using multi-hop reasoning on context graphs for training large language model hallucination detection systems",
    "relevant_papers": [
      "2501.17144v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving document-level fact checking and grounded factuality classification through graph-based synthetic training data and multi-hop reasoning",
    "relevant_papers": [
      "2501.17144v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement ultra-low latency deep learning inference on FPGAs using multivariate polynomials and hardware-aware structured pruning techniques",
    "relevant_papers": [
      "2501.08043v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Optimizing lookup table based neural networks on FPGAs using multivariate polynomial building blocks and group regularization for sparsity",
    "relevant_papers": [
      "2501.08043v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Improving Tip-Adapter for vision-language models by incorporating global information and proximal kernel ridge regression in a reproducing kernel Hilbert space",
    "relevant_papers": [
      "2501.11175v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Theoretical understanding of training-free few-shot CLIP adaptation techniques from a kernel perspective and the performance of proximal kernel regularizers",
    "relevant_papers": [
      "2501.11175v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "framework for quantifying the extent of model distillation and homogenization in large language models using identity cognition contradictions and response similarity analysis",
    "relevant_papers": [
      "2501.12619v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to evaluate the degree of knowledge distillation from teacher models to student LLMs and its effect on model diversity and independent development",
    "relevant_papers": [
      "2501.12619v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving cross-lingual knowledge consistency in large language models using direct preference optimization and self-alignment across multiple languages for question answering tasks.",
    "relevant_papers": [
      "2501.18457v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use self-consistent responses across different languages as targets for DPO to enhance the cross-lingual question answering performance of language models.",
    "relevant_papers": [
      "2501.18457v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can in-context learning and retrieval-augmented generation be combined to improve the contextual accuracy of automatic question generation for educational materials?",
    "relevant_papers": [
      "2501.17397v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparing the performance of few-shot in-context learning and hybrid RAG models for generating pedagogically sound questions from educational documents using LLMs.",
    "relevant_papers": [
      "2501.17397v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to combine slot attention with pre-trained diffusion models using adapters for object-centric learning and image composition tasks",
    "relevant_papers": [
      "2501.15878v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Improving compositional image generation and object discovery by integrating slot-based conditioning into pre-trained diffusion models without text supervision",
    "relevant_papers": [
      "2501.15878v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How effective is fine-tuning large language models for the automatic scoring of student-written scientific explanations in Chinese and what factors impact their accuracy?",
    "relevant_papers": [
      "2501.06704v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the relationship between reasoning complexity and scoring accuracy when using fine-tuned ChatGPT for assessing Chinese language scientific explanations in educational contexts.",
    "relevant_papers": [
      "2501.06704v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can multi-listwise preference optimization be used to improve functionality and structural stability in controllable protein sequence generation?",
    "relevant_papers": [
      "2501.15007v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Finetuning protein large language models for multi-attribute controllable protein design using preference optimization strategies to meet specific biomedical requirements.",
    "relevant_papers": [
      "2501.15007v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the safety and vulnerability of large audio language models against jailbreak attacks using synthetic audio datasets and hidden semantics injection",
    "relevant_papers": [
      "2501.13772v4"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking large audio language models for safety alignment and robustness against adversarial audio prompts through a comprehensive dataset and evaluation toolbox",
    "relevant_papers": [
      "2501.13772v4"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "learning-based multi-turn jailbreak attack framework for large language models using direct preference optimization and supervised fine-tuning to simulate real-world human behavior",
    "relevant_papers": [
      "2501.14250v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can multi-turn red-teaming strategies using turn-level LLM feedback and DPO improve the success rate of jailbreak attacks against advanced models like GPT-4o?",
    "relevant_papers": [
      "2501.14250v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "What are the primary technical challenges in developing agentic neural graph databases that support autonomous query construction and abductive reasoning?",
    "relevant_papers": [
      "2501.14224v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to integrate large language models with neural graph databases to enable self-improving systems and autonomous graph data management?",
    "relevant_papers": [
      "2501.14224v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "structured prompt engineering frameworks for developing task-oriented dialog systems that reliably execute complex business workflows using large language models",
    "relevant_papers": [
      "2501.11613v7"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using natural language specifications to design conversation agentic systems and manage behavioral consistency in complex multi-step conversational interactions",
    "relevant_papers": [
      "2501.11613v7"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use hybrid attention mechanisms and large language models to improve fake news detection performance on the WELFake dataset?",
    "relevant_papers": [
      "2501.11967v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Academic papers on combining textual statistical features with deep semantic features using large language models for interpretable fake news identification.",
    "relevant_papers": [
      "2501.11967v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve large language model performance by aggregating multiple draft responses during supervised fine-tuning and inference?",
    "relevant_papers": [
      "2501.11877v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Fine-tuning techniques for large language models to synthesize multiple proposals into a single refined answer to scale test-time computation.",
    "relevant_papers": [
      "2501.11877v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Can Looped Transformers perform neural algorithmic reasoning for hypergraph algorithms like Dijkstra or Helly's through hyperedge-aware encoding?",
    "relevant_papers": [
      "2501.10688v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Extending the neural algorithmic reasoning capabilities of Looped Transformers to simulate combinatorial optimization and graph-based algorithms on complex hypergraph structures.",
    "relevant_papers": [
      "2501.10688v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve text-to-video generation models using text embedding interpolation and perpendicular foot embeddings for complex prompt features?",
    "relevant_papers": [
      "2501.09982v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Enhancing text-to-video synthesis by identifying optimal text embeddings through interpolation and cosine similarity in the embedding space.",
    "relevant_papers": [
      "2501.09982v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "multilingual dataset for hate speech and abusive language detection across fifteen African languages annotated by native speakers with local cultural context",
    "relevant_papers": [
      "2501.08284v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking machine learning models for hate speech classification in African languages using native speaker annotations and offensive language lexicons",
    "relevant_papers": [
      "2501.08284v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Bridging the gap between instruction tuning and pre-training distributions by identifying dataset coverage shortfalls and generating instruction pairs from pre-trained corpora.",
    "relevant_papers": [
      "2501.09368v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Using adaptive data selection and controlled rewriting of pre-training data to improve the diversity and performance of instruction-tuned large language models.",
    "relevant_papers": [
      "2501.09368v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How do padding tokens in text encoders affect the image generation process in diffusion models through self-attention and cross-attention mechanisms?",
    "relevant_papers": [
      "2501.06751v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Causal analysis of padding token representations in text-to-image pipelines and their impact on diffusion model performance across different training scenarios.",
    "relevant_papers": [
      "2501.06751v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does adding random punctuation to mathematical prompts affect the reasoning performance and robustness of large language models on benchmarks like GSM8K?",
    "relevant_papers": [
      "2501.08203v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the vulnerability of math-specialized large language models to input noise and punctuation perturbations that do not cause any information loss.",
    "relevant_papers": [
      "2501.08203v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking multimodal large language models for complex geologic map understanding and spatial reasoning tasks",
    "relevant_papers": [
      "2501.06184v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "agent based framework for improving MLLM performance on geological cartography through domain knowledge injection",
    "relevant_papers": [
      "2501.06184v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can adaptive projector fusion driven by user instructions improve video multimodal large language models for complex video understanding tasks?",
    "relevant_papers": [
      "2501.05067v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Video large language models using instruction-based dynamic weighting of different visual projectors to balance static details and temporal information.",
    "relevant_papers": [
      "2501.05067v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how does inter-model response agreement and focal loss improve the calibration of large language models across different prompt styles",
    "relevant_papers": [
      "2501.03991v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving large language model calibration using auxiliary models for confidence estimation based on multi-model agreement and focal loss functions",
    "relevant_papers": [
      "2501.03991v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement black-box watermarking for retrieval augmented generation systems to protect intellectual property from being stolen or unauthorizedly commercialized?",
    "relevant_papers": [
      "2501.05249v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Robust knowledge-based watermarking methods for LLM retrieval systems using entity-relationship tuples to detect infringement after post-processing by adversary models.",
    "relevant_papers": [
      "2501.05249v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How effective is ChatGPT-4o at generating WCAG compliant web code when provided with screenshots and structured prompts for visual reasoning?",
    "relevant_papers": [
      "2501.03572v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the utility of large language models in identifying and fixing web accessibility barriers through automated testing and manual expert review.",
    "relevant_papers": [
      "2501.03572v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How can goal-conditioned reinforcement learning policies trained on short-distance goals generalize to reach long-distance or distant goals?",
    "relevant_papers": [
      "2501.02709v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Achieving horizon generalization in RL through planning invariance to ensure policies can reach distant goals after training on nearby objectives.",
    "relevant_papers": [
      "2501.02709v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can segment-level reward models improve reinforcement learning from human feedback compared to traditional bandit or token-level reward formulations in language models?",
    "relevant_papers": [
      "2501.02790v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using dynamic text segmentation and location-aware normalizers to provide denser rewards for training large language models with reinforcement learning from human feedback.",
    "relevant_papers": [
      "2501.02790v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to integrate AI-driven intrusion detection systems with targeted policy interventions to neutralize systemic cybersecurity threats and protect the digital economy",
    "relevant_papers": [
      "2501.09025v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "multilevel defense strategies combining artificial intelligence solutions and regulatory measures for mitigating risks from autonomous AI-driven attacks in modern digital ecosystems",
    "relevant_papers": [
      "2501.09025v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "how to add early exit branches to pre-trained deep neural networks to reduce inference computational cost without retraining the original model weights",
    "relevant_papers": [
      "2501.02508v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "optimizing the speed accuracy tradeoff in deep learning using confidence based early exits for pre-trained architectures like ResNet and DenseNet",
    "relevant_papers": [
      "2501.02508v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve the robustness of large language models against misleading or unfaithful arguments using bilateral confidence estimation and direct preference optimization",
    "relevant_papers": [
      "2501.01336v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "techniques for maintaining faithful integrity in LLMs to prevent them from being easily swayed by incorrect opposing statements during complex reasoning conversations",
    "relevant_papers": [
      "2501.01336v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can we evaluate the ability of large language models to generate properly structured Markdown responses for improved readability?",
    "relevant_papers": [
      "2501.15000v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarks and datasets for assessing and improving the markdown formatting and structural awareness of open-source large language models compared to proprietary systems.",
    "relevant_papers": [
      "2501.15000v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve retrieval augmented generation for scientific research question answering by integrating citation graph structures and hybrid lexical-semantic retrieval signals",
    "relevant_papers": [
      "2501.15067v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "utilizing contextualized graph representations and dense-sparse retrieval techniques to enhance the accuracy of large language models in scholarly document retrieval systems",
    "relevant_papers": [
      "2501.15067v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How can 2D Gaussian splatting be integrated with vector quantization to improve the representation capability of image tokenizers?",
    "relevant_papers": [
      "2501.15619v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Using flexible 2D Gaussian features and splatting operations to enhance discrete codebook based image reconstruction in multi-modal models",
    "relevant_papers": [
      "2501.15619v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can overlapping messages in text-based human-AI interaction improve conversational fluidity and make chatbots feel more natural compared to traditional turn-taking models?",
    "relevant_papers": [
      "2501.18103v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Designing conversational AI systems that support concurrent message exchanges and overlapping text to enhance user immersion and communication efficiency in digital dialogue.",
    "relevant_papers": [
      "2501.18103v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How can neural language models be used to prioritize configuration options and value ranges for faster detection of performance bugs in configurable software systems?",
    "relevant_papers": [
      "2501.15392v3"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for accelerating configuration performance bug testing using dual-level neural prioritization and automated oracle estimation for complex software configurations.",
    "relevant_papers": [
      "2501.15392v3"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "synthetic benchmarks for evaluating deductive reasoning in large language models that eliminate prior knowledge bias and support complex argument structures",
    "relevant_papers": [
      "2501.14851v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how do state of the art reasoning models perform compared to humans on complex logical reasoning tasks with heterogeneous argument forms",
    "relevant_papers": [
      "2501.14851v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "flexible modular framework for knowledge graph retrieval augmented generation that improves retrieval quality without extra model fine-tuning or training",
    "relevant_papers": [
      "2501.09957v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving knowledge graph retrieval augmented generation by classifying query complexity and using adaptable retrieval strategies for efficient reasoning path identification",
    "relevant_papers": [
      "2501.09957v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating theory of mind in large language models using verbalized mental states and false beliefs in role-playing conversations with information asymmetry",
    "relevant_papers": [
      "2501.08838v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "a multi-choice question answering benchmark for assessing first and second order theory of mind across diverse personality traits in large language models",
    "relevant_papers": [
      "2501.08838v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "interpretable multiple instance learning for whole slide images using vision language models to extract human understandable pathology concepts without manual annotations",
    "relevant_papers": [
      "2501.02922v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to use clinical concepts and vision language models for explainable gigapixel histopathology classification in a multiple instance learning framework",
    "relevant_papers": [
      "2501.02922v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to map multimodal llm hidden states to interpretable visual and textual concepts for analyzing representation shifts during finetuning",
    "relevant_papers": [
      "2501.03012v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "training free method for steering multimodal large language models using additive concept shift vectors for debiasing and safety control",
    "relevant_papers": [
      "2501.03012v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to predict the accuracy of black-box language models by training linear models on response probabilities from follow-up questions.",
    "relevant_papers": [
      "2501.01558v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Using follow-up query responses to identify misrepresented models and detect adversarial system prompts in closed-source language model APIs.",
    "relevant_papers": [
      "2501.01558v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to adaptively select semantically similar translation demonstrations using LLM embeddings to improve few-shot machine translation performance?",
    "relevant_papers": [
      "2501.01679v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving neural machine translation in large language models through adaptive few-shot prompting and reranking of generated translation candidates.",
    "relevant_papers": [
      "2501.01679v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve Transformer attention mechanisms by incorporating Graph Isomorphism Networks and Principal Neighborhood Aggregation for better relational reasoning and structure modeling",
    "relevant_papers": [
      "2501.02393v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient fine-tuning of foundation models using sparse GIN-Attention as an alternative to LoRA for enhancing graph-aware relational reasoning and generalization",
    "relevant_papers": [
      "2501.02393v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can transformer architectures be optimized for generalizable first-order logical reasoning and answering complex knowledge graph queries under distribution shifts?",
    "relevant_papers": [
      "2501.00759v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving the out-of-distribution generalization of transformers for first-order logical entailment using logic-aware architectures and proper positional encoding choices.",
    "relevant_papers": [
      "2501.00759v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can multimodal large language models be used to help end-users create and debug personalized visual sensors through natural language criteria and reasoning?",
    "relevant_papers": [
      "2501.15727v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Interactive systems for authoring custom AI vision sensors using multimodal foundation models with features for requirement elicitation and stress testing.",
    "relevant_papers": [
      "2501.15727v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How can multi-agent large language model frameworks be used to simulate multidisciplinary engineering teams for senior design and capstone projects?",
    "relevant_papers": [
      "2501.01205v1"
    ],
    "category": "cs.MA",
    "original_relevant_count": 1
  },
  {
    "query": "Framework for implementing collaborative LLM agents with specialized personas to assist engineering students with complex problem solving and ethical considerations in final year projects.",
    "relevant_papers": [
      "2501.01205v1"
    ],
    "category": "cs.MA",
    "original_relevant_count": 1
  },
  {
    "query": "How do language and text-to-image models exhibit religious bias across different demographic factors like gender and nationality in prompt completion tasks?",
    "relevant_papers": [
      "2501.08441v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Measuring religious stereotypes in generative AI using natural prompt probing and evaluating the effectiveness of corrective prompts as a debiasing strategy for multimodal models.",
    "relevant_papers": [
      "2501.08441v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking generative AI models versus traditional ion exchange and random enumeration for the discovery of stable inorganic crystal structures",
    "relevant_papers": [
      "2501.02144v2"
    ],
    "category": "cond-mat.mtrl-sci",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve generative materials discovery using post-generation screening with universal interatomic potentials and machine learning property filters",
    "relevant_papers": [
      "2501.02144v2"
    ],
    "category": "cond-mat.mtrl-sci",
    "original_relevant_count": 1
  },
  {
    "query": "large time series models with billion scale parameters incorporating human feedback through time-series policy optimization for improved zero-shot performance",
    "relevant_papers": [
      "2501.15942v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to use patch convolutional embedding and human feedback pipelines to scale time series neural networks for large scale supply chain applications",
    "relevant_papers": [
      "2501.15942v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Hierarchical tree-structured recommendation system using retrieval augmented generation for step-wise medical reasoning and diagnostic test selection based on patient symptoms",
    "relevant_papers": [
      "2501.02727v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How can RAG-enhanced hierarchical models improve the accuracy of medical test recommendations compared to traditional vector similarity search methods in clinical settings",
    "relevant_papers": [
      "2501.02727v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How to use self-questioning techniques to reduce hallucinations and improve complex visual reasoning in lightweight multimodal large language models?",
    "relevant_papers": [
      "2501.02964v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Multi-round training framework for MLLMs that uses heuristic guidance and visual clues to enhance fine-grained image description and reasoning performance.",
    "relevant_papers": [
      "2501.02964v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Analyzing phase transitions and emergent capabilities in large language models by reformulating the Transformer architecture as an O(N) statistical physics model",
    "relevant_papers": [
      "2501.16241v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to estimate the internal dimension and parameter sufficiency of Transformers using phase transition phenomena and the O(N) vector model",
    "relevant_papers": [
      "2501.16241v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can digital twins and ray tracing be used together with AI to improve channel state information accuracy for precoding in cellular networks?",
    "relevant_papers": [
      "2501.16504v1"
    ],
    "category": "eess.SP",
    "original_relevant_count": 1
  },
  {
    "query": "Using a multi-step tuning process with AI to bridge the gap between digital twin rendered CSI and actual physical environment channel state information.",
    "relevant_papers": [
      "2501.16504v1"
    ],
    "category": "eess.SP",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used to automatically generate AI tool interfaces from inconsistent and unstructured REST API documentation?",
    "relevant_papers": [
      "2501.16945v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Automating the transformation of REST APIs into AI compatible tools using LLMs and benchmarking performance on unstructured documentation formats.",
    "relevant_papers": [
      "2501.16945v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "robust nonlinear subspace clustering using data-driven kernel learning to preserve local manifold structures and enhance block-diagonal affinity matrices",
    "relevant_papers": [
      "2501.06368v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve kernel-based subspace clustering by learning adaptive kernels from self-representation while satisfying multiplicative triangle inequality constraints",
    "relevant_papers": [
      "2501.06368v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluation of fine-tuned GPT-4o-mini for cost-effective and accurate PII detection in educational datasets compared to Microsoft Presidio and Azure AI Language",
    "relevant_papers": [
      "2501.09765v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can fine-tuned large language models improve de-identification of student names and sensitive information while maintaining data utility for educational research?",
    "relevant_papers": [
      "2501.09765v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve document retrieval accuracy using zero-shot re-ranking based on answer scent with large language models?",
    "relevant_papers": [
      "2501.15245v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for zero-shot document re-ranking that use pre-trained language models to calculate the likelihood of potential answers in open-domain QA.",
    "relevant_papers": [
      "2501.15245v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can natural language inference models be improved to recognize implicit meanings and distinguish between explicit and implied entailment in human communication?",
    "relevant_papers": [
      "2501.07719v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the performance of large language models on implied entailment tasks using the INLI dataset to capture meaning beyond literal word-for-word interpretation.",
    "relevant_papers": [
      "2501.07719v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Research comparing the emotional variance and sentiment positivity of human tweets about generative AI versus the automated responses generated by large language models.",
    "relevant_papers": [
      "2501.06597v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does the EmoXpt framework analyze differences in sentiment and emotional intelligence between human social media comments and responses produced by AI systems like ChatGPT?",
    "relevant_papers": [
      "2501.06597v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "current research roadmap and challenges for advancing large language model based code generation in real-world software development scenarios",
    "relevant_papers": [
      "2501.11354v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "six-layer vision framework for analyzing orchestration and validation phases in large language model code generation systems",
    "relevant_papers": [
      "2501.11354v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "graph-based framework for generating stealthy jailbreak prompts to evaluate large language model robustness and enhance content moderation",
    "relevant_papers": [
      "2501.18638v3"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "how can interconnected graph structures with pruning improve the efficiency and success rate of adversarial attacks against large language models",
    "relevant_papers": [
      "2501.18638v3"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How can zero-shot large language models and prompt engineering be used to automate assignment grading and provide personalized feedback in higher education?",
    "relevant_papers": [
      "2501.14305v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "Effective frameworks for using LLMs to evaluate student computational and explanatory responses without the need for large training datasets or fine-tuning.",
    "relevant_papers": [
      "2501.14305v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "How does the AIN bilingual multimodal model achieve state-of-the-art performance in Arabic visual perception and language tasks compared to models like GPT-4o?",
    "relevant_papers": [
      "2502.00094v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Development of a large multimodal model for Arabic and English trained on millions of high-quality samples to handle complex visual and video understanding tasks.",
    "relevant_papers": [
      "2502.00094v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "mathematical framework that unifies preference optimization methods such as DPO IPO and SimPO for training large language models",
    "relevant_papers": [
      "2502.00203v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "impact of optimization objectives and explicit reward models on the performance of preference tuning for large language model alignment",
    "relevant_papers": [
      "2502.00203v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to reduce activation memory during transformer fine-tuning by backpropagating through a subset of input tokens for improved efficiency",
    "relevant_papers": [
      "2501.18824v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "efficient fine-tuning methods for large language models that minimize memory usage by selecting specific tokens for gradient computation",
    "relevant_papers": [
      "2501.18824v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Bayes-optimal generalisation error for shallow two-layer neural networks with proportional width and input dimension near the interpolation threshold",
    "relevant_papers": [
      "2501.18530v2"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "Phase transition from universal to specialisation learning in extensive width neural networks using the teacher-student model with quadratic sample size scaling",
    "relevant_papers": [
      "2501.18530v2"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "How can RAG and chain-of-thought reasoning be used with large language models to improve network protocol packet seed generation for fuzzing?",
    "relevant_papers": [
      "2502.15727v2"
    ],
    "category": "cs.NI",
    "original_relevant_count": 1
  },
  {
    "query": "Using RAG-based agentic LLMs and chain-of-thought prompting to extract protocol state machine information from RFC documents for seed enrichment in fuzzing.",
    "relevant_papers": [
      "2502.15727v2"
    ],
    "category": "cs.NI",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating large language models for longitudinal clinical summarization and temporal reasoning in electronic health records",
    "relevant_papers": [
      "2501.18724v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "performance of retrieval augmented generation and chain of thought prompting for diagnosis prediction using longitudinal patient data",
    "relevant_papers": [
      "2501.18724v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve large language model reasoning accuracy by combining them with logical solvers through semantic self-verification and concrete problem instantiations",
    "relevant_papers": [
      "2501.16961v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Research on formalizing natural language reasoning tasks for logical solvers using consistency-based verification to achieve high precision and reduce manual oversight",
    "relevant_papers": [
      "2501.16961v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve resource efficiency in compound AI systems using declarative programming models and adaptive runtime scheduling?",
    "relevant_papers": [
      "2501.16634v3"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "Architectural designs for decoupling orchestration and resource management to optimize energy efficiency and performance in complex AI workflows.",
    "relevant_papers": [
      "2501.16634v3"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "Using pre-trained foundational vision transformers like DINOv2 and SAM for extracting task-agnostic microstructure features in materials science applications",
    "relevant_papers": [
      "2501.18637v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Machine learning of material properties from microstructures using vision models without the need for expensive task-specific training or fine-tuning",
    "relevant_papers": [
      "2501.18637v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to perform knowledge distillation from large transformer models like qwen 2.5 into efficient rnn architectures like rwkv-7 for improved state tracking",
    "relevant_papers": [
      "2501.15570v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "distilling large scale transformer language models into rwkv based architectures to reduce training time and improve rnn expressiveness",
    "relevant_papers": [
      "2501.15570v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to trigger hallucinations in multimodal large language models by exploiting attention sink behaviors in the internal attention mechanism",
    "relevant_papers": [
      "2501.15269v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "transferable visual adversarial attacks against multimodal models using attention sinks to create hallucinations with minimal image-text relevance",
    "relevant_papers": [
      "2501.15269v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to combine large language models with symbolic solvers using generic prompting and separate domain knowledge for logical reasoning tasks",
    "relevant_papers": [
      "2501.14540v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "neurosymbolic framework for improving large language model performance on complex logic benchmarks like AR-LSAT through constraint satisfaction and optimization",
    "relevant_papers": [
      "2501.14540v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "survey of AI researchers' opinions on existential risk and their familiarity with fundamental AI safety concepts like instrumental convergence",
    "relevant_papers": [
      "2502.14870v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "empirical study on why AI experts disagree about catastrophic risks and the correlation between safety knowledge and risk perception",
    "relevant_papers": [
      "2502.14870v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "preference datasets and benchmarks for evaluating reward model reliability in open-ended long-context generation tasks and hallucination reduction",
    "relevant_papers": [
      "2501.13264v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to develop reward models for trustworthy long-context generation in large language models using automated preference data pipelines",
    "relevant_papers": [
      "2501.13264v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can legal concept generation and Determinantal Point Process based extraction improve prior case retrieval systems for European Court of Human Rights judgments?",
    "relevant_papers": [
      "2501.14114v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving legal document retrieval by augmenting query facts with key concepts extracted through weak supervision from the reasoning sections of court cases.",
    "relevant_papers": [
      "2501.14114v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "AI storytelling system using character symbol manipulation and motion-steered text generation to express complex social interactions",
    "relevant_papers": [
      "2501.13284v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How can symbolic motions from toy-playing be used to guide large language models in generating collaborative visual and textual stories",
    "relevant_papers": [
      "2501.13284v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "how to optimize large language model serving for multiple service level objectives using adaptive and SLO-customized speculative decoding techniques",
    "relevant_papers": [
      "2501.12162v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "efficient LLM inference systems that utilize hardware-aware speculation trees and dynamic parameter adjustment to meet heterogeneous latency targets",
    "relevant_papers": [
      "2501.12162v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use agentic frameworks and character knowledge graphs to evaluate factual consistency in long narrative summaries exceeding 100k tokens",
    "relevant_papers": [
      "2501.09993v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving the factual accuracy of large language model summaries for long narratives through agent-driven evaluation and character relationship extraction",
    "relevant_papers": [
      "2501.09993v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to optimize confidence thresholds in large language model cascades using Markov-copula probabilistic models for better error-cost trade-offs?",
    "relevant_papers": [
      "2501.09345v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Probabilistic modeling of joint error distributions to tune sequential large language model systems and improve sample efficiency in low-data regimes.",
    "relevant_papers": [
      "2501.09345v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient LLM serving systems that co-locate latency-sensitive online requests and throughput-oriented offline workloads while maintaining SLO guarantees",
    "relevant_papers": [
      "2501.14808v4"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "How can interference-aware scheduling and latency prediction improve resource utilization and throughput in hybrid large language model serving environments",
    "relevant_papers": [
      "2501.14808v4"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "How to reduce communication overhead in distributed large language model serving by prefetching model weights and KV-cache into on-chip accelerator memory?",
    "relevant_papers": [
      "2501.08192v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for overlapping communication with computation in LLM inference by prefetching weights and KV-cache from HBM to minimize memory bottlenecks and latency.",
    "relevant_papers": [
      "2501.08192v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "instruction tuning for video facial expression captioning and FaceTrack-MM model for tracking facial dynamics in complex multi-person video scenarios",
    "relevant_papers": [
      "2501.07978v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how can multimodal large language models be trained to provide detailed facial expression descriptions using specialized datasets and temporal evaluation metrics",
    "relevant_papers": [
      "2501.07978v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can multi-neuromodulatory systems like dopamine and noradrenaline be used to improve continuous learning and adaptability in artificial neural networks?",
    "relevant_papers": [
      "2501.06762v2"
    ],
    "category": "q-bio.NC",
    "original_relevant_count": 1
  },
  {
    "query": "Bio-inspired learning rules using multi-scale neuromodulation to mitigate catastrophic forgetting and enhance the robustness of artificial neural networks in dynamic environments",
    "relevant_papers": [
      "2501.06762v2"
    ],
    "category": "q-bio.NC",
    "original_relevant_count": 1
  },
  {
    "query": "How to effectively integrate multiple vision encoders in multimodal large language models using a dual-branch framework and sequential token interleaving?",
    "relevant_papers": [
      "2501.06986v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Advanced fusion strategies for hybrid multimodal large language models that combine diverse vision encoders with adaptive tiling for enhanced visual perception.",
    "relevant_papers": [
      "2501.06986v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "What are the current state of the art techniques for early exit and adaptive inference in deep neural networks for natural language processing tasks?",
    "relevant_papers": [
      "2501.07670v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Comprehensive review of methodologies using intermediate layer classifiers to accelerate inference and improve robustness in NLP models for resource constrained devices.",
    "relevant_papers": [
      "2501.07670v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement remote and automatic cognitive impairment screening using virtual agents and conversational speech analysis for early detection of dementia",
    "relevant_papers": [
      "2501.05755v1"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "performance of large language models like DistilBERT in classifying mild cognitive impairment from recorded memory responses and picture description tasks",
    "relevant_papers": [
      "2501.05755v1"
    ],
    "category": "cs.SD",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models reflect and amplify stereotypes of sexual and gender minorities using the Stereotype Content Model for evaluation?",
    "relevant_papers": [
      "2501.05926v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Measuring representational harms and social bias against non-binary individuals and sexual minorities in text generation tasks of English-language language models.",
    "relevant_papers": [
      "2501.05926v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to exploit inter-iteration and intra-iteration output sparsity to accelerate diffusion model inference with custom hardware architecture",
    "relevant_papers": [
      "2501.05680v1"
    ],
    "category": "cs.AR",
    "original_relevant_count": 1
  },
  {
    "query": "software-hardware co-design for diffusion models using ffn reuse and eager prediction to improve energy efficiency on edge devices",
    "relevant_papers": [
      "2501.05680v1"
    ],
    "category": "cs.AR",
    "original_relevant_count": 1
  },
  {
    "query": "how to achieve low-bit quantization of text-to-image diffusion models while preserving image quality and text alignment without fine-tuning",
    "relevant_papers": [
      "2501.04304v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "addressing activation outliers and cross-attention distribution patterns for efficient low-bit quantization of text-to-image generation models",
    "relevant_papers": [
      "2501.04304v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to combine spatial layout information and semantic text embeddings for improved document chunking and segmentation in complex multi-column reports?",
    "relevant_papers": [
      "2501.05485v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A hybrid method for document segmentation using bounding box data and spectral clustering on weighted graphs to optimize chunking for large language models.",
    "relevant_papers": [
      "2501.05485v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving visual reasoning in large vision-language models using interleaved visual chain-of-thought and retrieval-based crops of relevant entities",
    "relevant_papers": [
      "2501.04671v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "DrivingVQA dataset for complex visual question answering in driving scenes with expert-written explanations and grounded reasoning chains",
    "relevant_papers": [
      "2501.04671v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Using explainable AI and machine learning to distinguish between human-written content and text generated by different large language models like ChatGPT and Bard",
    "relevant_papers": [
      "2501.03212v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Differentiating between multiple LLM sources using deep learning models and XAI to identify stylistic features for text attribution and plagiarism detection",
    "relevant_papers": [
      "2501.03212v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used for document-level text simplification using a progressive hierarchical approach instead of direct prompting?",
    "relevant_papers": [
      "2501.03857v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving document simplification performance in large language models through multi-stage collaboration and task decomposition at discourse and lexical levels.",
    "relevant_papers": [
      "2501.03857v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how can we evaluate the clinical appropriateness of conversational agents for psychiatric assessment using multi-faceted simulated patient profiles",
    "relevant_papers": [
      "2501.01594v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking large language model based psychiatric agents through a framework that simulates patient history and behavior for quantitative assessment",
    "relevant_papers": [
      "2501.01594v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can transformer-based models like mT5 and BanglaT5 be applied to solve mathematical word problems by translating Bengali text into equations?",
    "relevant_papers": [
      "2501.02599v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Fine-tuning pre-trained language models for solving Bengali arithmetic word problems using the PatiGonit dataset and comparing mT5 against other transformer architectures.",
    "relevant_papers": [
      "2501.02599v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve text-image alignment in diffusion-based style transfer using embedding reframing and multi-layer attention-based style extraction for diverse outputs?",
    "relevant_papers": [
      "2501.02064v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Recent methods for balancing textual semantics and stylistic intensity in generative models through shared feature space mapping of text and image embeddings.",
    "relevant_papers": [
      "2501.02064v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "adversarial attacks on LLM routing systems to manipulate model selection and force the use of more expensive large language models",
    "relevant_papers": [
      "2501.01818v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "how can an adversary use confounder gadgets to compromise the integrity of LLM control planes and bypass router-based cost balancing",
    "relevant_papers": [
      "2501.01818v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "A large-scale dataset of cybersecurity-specific prompts for benchmarking the effectiveness of jailbreaking techniques against commercial large language models",
    "relevant_papers": [
      "2501.01335v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating large language model security using close-ended prompts and prompt obfuscation methods to assess vulnerability to cybersecurity-related malicious content generation",
    "relevant_papers": [
      "2501.01335v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How can Large Language Models be used to perform interpretable depression screening on social media by completing standardized psychological questionnaires like the BDI-II?",
    "relevant_papers": [
      "2501.00982v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using adaptive retrieval-augmented generation to bridge social media posts with clinical psychometric instruments for more accurate and explainable mental health assessment.",
    "relevant_papers": [
      "2501.00982v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can retrieval augmented generation and in-context learning be used to improve the cultural alignment of large language models using the World Values Survey dataset?",
    "relevant_papers": [
      "2501.01031v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A framework for dynamic retrieval of regional cultural values and demographic knowledge to mitigate western centric bias in large language model text generation.",
    "relevant_papers": [
      "2501.01031v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does information bottleneck theory explain the internal mechanisms of large language models for information compression and task space mapping?",
    "relevant_papers": [
      "2501.00999v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving large language model reasoning and inference efficiency through information compression-based context learning and task-space guided fine-tuning.",
    "relevant_papers": [
      "2501.00999v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does data contamination in pre-training sets affect the BLEU score evaluation of large language models on machine translation tasks at different scales?",
    "relevant_papers": [
      "2501.18771v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Controlled study measuring the impact of source and target data leakage on the performance overestimation of language models in machine translation benchmarks.",
    "relevant_papers": [
      "2501.18771v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Why do adaptive optimizers like Adam outperform SGD in transformer training and how does gradient heterogeneity across parameter blocks influence convergence?",
    "relevant_papers": [
      "2502.00213v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Analysis of how layer normalization placement affects gradient heterogeneity and the performance of sign-based optimization methods in transformer architectures.",
    "relevant_papers": [
      "2502.00213v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can Generative Adversarial Networks be used to predict smart grid stability when only stable data is available for training?",
    "relevant_papers": [
      "2501.16490v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Detecting smart grid instability and adversarial attacks using OOD samples generated by GANs under conditions of data scarcity.",
    "relevant_papers": [
      "2501.16490v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "how to erase NSFW concepts from text to image diffusion models by adjusting text condition tokens without retraining model weights",
    "relevant_papers": [
      "2501.15562v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "efficient concept erasure for diffusion models using a semantic driven word vocabulary and gradient orthogonal token optimization strategy",
    "relevant_papers": [
      "2501.15562v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "benchmark for evaluating the effectiveness of large language model critiques using closed-loop correction quality and iterative feedback loops across reasoning tasks",
    "relevant_papers": [
      "2501.14492v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comparing advanced reasoning models and classical LLMs on their ability to perform self-critique and generate constructive corrections for flawed outputs",
    "relevant_papers": [
      "2501.14492v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "generative multimodal large language models for explicit semantic understanding of event streams using spatiotemporal representations and semantic alignment",
    "relevant_papers": [
      "2501.13707v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "large scale dataset and framework for training event-based vision language models to perform event captioning and scene description tasks",
    "relevant_papers": [
      "2501.13707v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can we automatically generate high-quality dialogue benchmarks from knowledge graphs using cost-effective retrieval-augmented generation frameworks?",
    "relevant_papers": [
      "2501.09928v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using query-based subgraph retrieval and multi-stage LLM pipelines to create domain-specific dialogue evaluation datasets from structured knowledge graphs.",
    "relevant_papers": [
      "2501.09928v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How effective are vision-language models like GPT and Gemini at identifying and evaluating virtual content integrated into real-world augmented reality environments?",
    "relevant_papers": [
      "2501.13964v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "A study on using VLMs for the automated assessment of AR scene quality and their ability to distinguish seamlessly integrated virtual objects from real ones.",
    "relevant_papers": [
      "2501.13964v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to use ensemble models and committee voting strategies to improve the generalization and robustness of distilled training datasets?",
    "relevant_papers": [
      "2501.07575v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Improving dataset distillation performance by leveraging collective model wisdom and high quality soft labels to reduce bias and overfitting.",
    "relevant_papers": [
      "2501.07575v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to use ChatGPT and tag-based data analysis to generate personalized and constructive feedback reports for students in adaptive learning environments",
    "relevant_papers": [
      "2501.06819v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "A method for converting student learning behavior data into tags to facilitate automated feedback generation using large language models",
    "relevant_papers": [
      "2501.06819v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "retrosynthesis prediction using dual graph representations for reaction center identification and 3D diffusion models for reactant generation",
    "relevant_papers": [
      "2501.08001v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve retrosynthesis prediction by combining dual graph molecular representations with conditional 3D diffusion for generating reactants",
    "relevant_papers": [
      "2501.08001v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can reconfigurable optical circuit switching improve communication efficiency and training performance for distributed Mixture-of-Experts models on GPU clusters?",
    "relevant_papers": [
      "2501.03905v4"
    ],
    "category": "cs.NI",
    "original_relevant_count": 1
  },
  {
    "query": "Improving training cost efficiency of MoE models using a regionally reconfigurable hybrid optical-electrical interconnect system with runtime topology adaptation.",
    "relevant_papers": [
      "2501.03905v4"
    ],
    "category": "cs.NI",
    "original_relevant_count": 1
  },
  {
    "query": "how to use gpt models and hierarchical summarization for the semantic analysis and classification of android malware applications",
    "relevant_papers": [
      "2501.04848v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "leveraging large language models and prompt engineering for identifying malicious code snippets through functional and package level summarization in android apps",
    "relevant_papers": [
      "2501.04848v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "automated neural architecture search and compression techniques for hardware efficient deep learning models in physics research using hls4ml",
    "relevant_papers": [
      "2501.05515v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to design low latency neural networks for bragg peak finding and jet classification through hierarchical search and network pruning",
    "relevant_papers": [
      "2501.05515v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used as automated simulators to evaluate the effectiveness and interpretability of concept-based AI explanations?",
    "relevant_papers": [
      "2501.05855v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating concept-based explanation methods through automated simulatability and large language model approximations instead of human-intensive user studies.",
    "relevant_papers": [
      "2501.05855v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to reduce hallucinations in multimodal large language models by enhancing visual grounding and encoder alignment without extra instructional fine-tuning",
    "relevant_papers": [
      "2501.02699v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "A post-pretraining method for improving visual representations and language alignment to minimize object hallucinations in instruction-following vision-language models",
    "relevant_papers": [
      "2501.02699v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to use multiple large language models to generate high quality captions for scientific figures through collaborative filtering and candidate refinement techniques?",
    "relevant_papers": [
      "2501.02552v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving scientific figure captioning by using multimodal LLMs for data quality assessment and ensemble prompting strategies for diverse caption generation.",
    "relevant_papers": [
      "2501.02552v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can we identify and neutralize backdoor trigger tokens in word embedding layers during the supervised fine-tuning of pretrained language models?",
    "relevant_papers": [
      "2501.03272v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Defending against backdoor attacks in natural language processing by detecting abnormal token parameters and applying unlearning techniques during the training stage.",
    "relevant_papers": [
      "2501.03272v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models simplify or omit representation of global cultures in geographic descriptions and travel recommendation tasks?",
    "relevant_papers": [
      "2501.01056v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating cultural representational gaps and power inequities in large language models through the lenses of omission and simplification.",
    "relevant_papers": [
      "2501.01056v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can semantic graphs and uncertainty propagation between entity tokens improve the detection of hallucinations in large language model outputs?",
    "relevant_papers": [
      "2501.02020v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving uncertainty-based hallucination detection by modeling semantic relations between sentences and entities through graph-based calibration techniques.",
    "relevant_papers": [
      "2501.02020v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to generate counterfactual examples for natural language models using feature attribution and few-shot prompting with large language models",
    "relevant_papers": [
      "2501.00777v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "framework for automatic counterfactual generation using label flip verification and feature importance methods like LIME and Integrated Gradients",
    "relevant_papers": [
      "2501.00777v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "autonomous driving framework using dual-process decision making with analytic and heuristic reasoning to handle complex traffic scenarios",
    "relevant_papers": [
      "2501.08168v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "improving autonomous vehicle navigation through cognitive perception and a growing memory bank for continuous learning from past mistakes",
    "relevant_papers": [
      "2501.08168v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve video large language models by integrating multiple specialized visual encoders using spatio-temporal feature alignment",
    "relevant_papers": [
      "2501.01426v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "leveraging multiple frozen vision backbones to create unified video representations for zero-shot perception tests and open-ended video question answering",
    "relevant_papers": [
      "2501.01426v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "lightweight and explainable intrusion detection systems for industrial IoT using knowledge distillation and variational autoencoders",
    "relevant_papers": [
      "2501.00790v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve transparency and computational efficiency in network security through attribution based explainability and knowledge distillation",
    "relevant_papers": [
      "2501.00790v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "applying compositional diffusion models for 6 degree-of-freedom powered descent trajectory generation and flexible constraint handling in spacecraft guidance",
    "relevant_papers": [
      "2501.00915v1"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "how can generative diffusion policies be used for few-shot adaptation and minimum-fuel landing site selection in spacecraft trajectory optimization",
    "relevant_papers": [
      "2501.00915v1"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "How can block-wise mixed format quantization using FP4 dialects improve the accuracy and energy efficiency of large language model inference?",
    "relevant_papers": [
      "2501.01144v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Fine-grained block-level quantization techniques for LLMs that assign optimal number formats from a formatbook to handle outlier distributions.",
    "relevant_papers": [
      "2501.01144v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating multi-step tool use reasoning in large language models using a benchmark with process supervision and intermediate step verification",
    "relevant_papers": [
      "2501.01290v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comparison of process supervised reward models and outcome supervised reward models for improving AI performance on complex tool reasoning tasks",
    "relevant_papers": [
      "2501.01290v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use hadamard rotations to mitigate activation and weight outliers for efficient 8-bit quantization-aware training of large language models",
    "relevant_papers": [
      "2501.02625v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "low-precision fine-tuning of transformers using hadamard-assisted optimization and fsdp integration to maintain accuracy in 8-bit training",
    "relevant_papers": [
      "2501.02625v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement efficient context pruning using sequence labeling to improve the robustness and speed of retrieval-augmented generation in question answering systems?",
    "relevant_papers": [
      "2501.16214v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A robust approach for combining context reranking and sequence labeling to remove irrelevant information from retrieved documents in large language model pipelines.",
    "relevant_papers": [
      "2501.16214v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models and knowledge graphs be used to automate the retrosynthesis planning of macromolecules and polymers?",
    "relevant_papers": [
      "2501.08897v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Multi-branched reaction pathway search algorithm for identifying complex synthesis routes in polymer chemistry using large language models.",
    "relevant_papers": [
      "2501.08897v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "datasets for training foundation models on linked business tables from enterprise resource planning systems to improve table representation learning",
    "relevant_papers": [
      "2501.03413v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "what datasets are available for research in multi-table representation learning using real-world enterprise data and foreign key relationships",
    "relevant_papers": [
      "2501.03413v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "hybrid retrieval-augmented generation framework for university admission chatbots to improve response precision using lightweight large language models",
    "relevant_papers": [
      "2501.16276v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "effective strategies for implementing unified RAG systems in educational settings to provide accurate academic counseling while reducing operational costs",
    "relevant_papers": [
      "2501.16276v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to achieve zero-shot vision to speech generalization in omnimodal large language models using progressive multimodal alignment and pre-trained speech models?",
    "relevant_papers": [
      "2501.04561v6"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Open-source omnimodal models for real-time emotional speech synthesis and multimodal understanding using non-autoregressive decoders and direct preference optimization.",
    "relevant_papers": [
      "2501.04561v6"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models improve their own critique and error identification skills without relying on human annotations or stronger teacher models?",
    "relevant_papers": [
      "2501.05727v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Enhancing LLM self-critique capabilities through self-generated data and contrastive-critic training for better mathematical and scientific reasoning performance.",
    "relevant_papers": [
      "2501.05727v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models improve long-term memory in voice assistants using category-based preference extraction for better personalization and user trust?",
    "relevant_papers": [
      "2501.09645v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Enhancing in-car voice assistant memory by using LLMs to extract and store user preferences within predefined categories to reduce data redundancy and contradictions.",
    "relevant_papers": [
      "2501.09645v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to use Conditional Value at Risk in reinforcement learning from human feedback to minimize rare but toxic outputs in large language models?",
    "relevant_papers": [
      "2501.06911v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Risk-averse fine-tuning methods for large language models to mitigate extreme negative outcomes and improve safety during sentiment modification and toxicity reduction tasks.",
    "relevant_papers": [
      "2501.06911v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to use large language models for lexicon-based text embeddings while addressing tokenization redundancy and unidirectional attention limitations in causal models?",
    "relevant_papers": [
      "2501.09749v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving sparse or lexicon-based embedding performance on the Massive Text Embedding Benchmark MTEB using token clustering and bidirectional attention in LLMs.",
    "relevant_papers": [
      "2501.09749v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can domain-adversarial fine-tuning improve the generalization of chain-of-thought reasoning in smaller language models during knowledge distillation?",
    "relevant_papers": [
      "2501.09804v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Enhancing small language model chain of thought performance using prompt-assisted domain-adversarial fine-tuning methods to recover domain-invariant features.",
    "relevant_papers": [
      "2501.09804v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "using large language models for automated regression test generation and bug finding in software commits and code changes",
    "relevant_papers": [
      "2501.11086v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "feedback directed zero shot LLM approach for generating reproduction test cases for patches in structured input programs like XML parsers",
    "relevant_papers": [
      "2501.11086v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How to address influence score bias in data selection for instruction tuning to ensure balanced performance across diverse LLM tasks?",
    "relevant_papers": [
      "2501.12147v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Balanced and influential data selection techniques for instruction fine-tuning using influence score normalization and iterative optimization for diverse model capabilities.",
    "relevant_papers": [
      "2501.12147v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use large language models for knowledge graph completion to create personalized learning path recommendations in higher education",
    "relevant_papers": [
      "2501.12300v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "automated curriculum modeling and topic extraction from lecture materials using LLMs to support cross-disciplinary student recommendations",
    "relevant_papers": [
      "2501.12300v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "applying large language models to few-shot multivariate time series classification using patch-wise temporal convolution encoders and parameter-efficient fine-tuning methods",
    "relevant_papers": [
      "2502.00059v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to leverage pre-trained large language models for few-shot multivariate time series classification under data scarcity constraints in industrial settings",
    "relevant_papers": [
      "2502.00059v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the effectiveness of open-source and commercial large language models for automated grading and feedback in university bioinformatics assignments",
    "relevant_papers": [
      "2501.14499v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models compare to human teaching assistants in providing personalized feedback and accurate grading for written college assignments",
    "relevant_papers": [
      "2501.14499v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the fidelity and constraint satisfaction of large language models acting as evolutionary optimizers for network-structured combinatorial optimization problems",
    "relevant_papers": [
      "2501.15081v4"
    ],
    "category": "cs.NE",
    "original_relevant_count": 1
  },
  {
    "query": "systematic framework and error-correction mechanisms for assessing large language models as population-level evolutionary operators in node-level network optimization tasks",
    "relevant_papers": [
      "2501.15081v4"
    ],
    "category": "cs.NE",
    "original_relevant_count": 1
  },
  {
    "query": "how can large language models perform zero-shot image and audio captioning without any training or fine-tuning through iterative prompting and multi-step reasoning",
    "relevant_papers": [
      "2501.18096v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "a training-free approach using gradient-free optimization and iterative feedback to enable multimodal capabilities like image generation and embedding inversion in standard llms",
    "relevant_papers": [
      "2501.18096v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "methods for separating motion concepts from appearance in text-to-video diffusion models to improve prompt alignment and motion consistency",
    "relevant_papers": [
      "2501.16714v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "using temporal attention purification and appearance highways to customize motion in pretrained video diffusion models without losing appearance diversity",
    "relevant_papers": [
      "2501.16714v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can chain-of-thought reasoning be applied to improve large language model defenses against sophisticated jailbreak attacks and harmful inputs?",
    "relevant_papers": [
      "2501.19180v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Using proactive intent analysis and reasoning steps to enhance safety alignment and generalize defenses against out-of-distribution adversarial prompts in LLMs.",
    "relevant_papers": [
      "2501.19180v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "fine-tuning large language models using ensembles of LoRA experts trained on instruction clusters grouped by gradient similarity to reduce optimization conflicts",
    "relevant_papers": [
      "2502.00089v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how can clustering training data by gradient directions and using specialized low-rank adapters improve the performance of language models on diverse multi-task datasets",
    "relevant_papers": [
      "2502.00089v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can trie-based prefix tree structures be used to optimize KV cache memory sharing during beam search decoding for large language models?",
    "relevant_papers": [
      "2502.00085v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient beam search algorithms for LLMs that use shared prefix KV caches to reduce memory usage and improve parallel decoding speed.",
    "relevant_papers": [
      "2502.00085v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how effective are large language models at extracting species names and habitat information from invasion biology research papers without domain-specific fine-tuning",
    "relevant_papers": [
      "2501.18287v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using large language models for automated extraction of species locations and ecosystems from scientific literature to support biological invasion research and conservation management",
    "relevant_papers": [
      "2501.18287v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "limitations of current large language model cybersecurity evaluations and frameworks for assessing real-world impact and threat actor behavior",
    "relevant_papers": [
      "2502.00072v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive risk assessment framework for LLM cyber capabilities incorporating operational advantages and economic metrics for impact analysis",
    "relevant_papers": [
      "2502.00072v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How do one-layer attention-only transformers develop internal mechanisms like vocabulary-splitting and copy-suppression while learning to sort numerical lists?",
    "relevant_papers": [
      "2501.18666v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Understanding how training data features shape the internal organization and simplicity bias of transformers through a developmental analysis of list-sorting models.",
    "relevant_papers": [
      "2501.18666v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to use preference optimization to intrinsically reduce hallucinations in large language model based machine translation systems during the training phase?",
    "relevant_papers": [
      "2501.17295v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving the reliability of LLM machine translation by fine-tuning on hallucination focused preference datasets to minimize errors without increasing inference latency.",
    "relevant_papers": [
      "2501.17295v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comparing the performance of LSTM deep learning models and ARIMA for predicting the S&P 500 index using historical price data",
    "relevant_papers": [
      "2501.17366v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how do Long Short-Term Memory networks compare to traditional ARIMA models in terms of accuracy for financial forecasting and stock market analysis",
    "relevant_papers": [
      "2501.17366v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does the integration of generative AI tools and experiential learning help undergraduate business students overcome creative barriers and improve skill acquisition?",
    "relevant_papers": [
      "2501.06527v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Research on human-AI collaboration in business education focusing on how students use text and image generation tools for real-world project validation.",
    "relevant_papers": [
      "2501.06527v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can multimodal large language models perform GUI grounding in Windows desktop environments using only screenshots instead of DOM or HTML data?",
    "relevant_papers": [
      "2503.04730v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "State of the art methods for visual GUI agent grounding and benchmarks for automating tasks in Windows OS using screenshot analysis.",
    "relevant_papers": [
      "2503.04730v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve the safety of large language model reward models using data-adaptive rule selection and maximum discrepancy in fine-grained human feedback?",
    "relevant_papers": [
      "2501.15453v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Dynamic selection of the most important safety rules to maximize mutual information between rule-based annotations and true human preferences in RLHF training.",
    "relevant_papers": [
      "2501.15453v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to optimize the order of in-context learning examples for large language models at inference time using log probabilities",
    "relevant_papers": [
      "2501.15030v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Dataset-free methods for finding the best sequence of few-shot demonstrations in LLM prompts to improve classification accuracy and model performance",
    "relevant_papers": [
      "2501.15030v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "systematic hyperparameter tuning for LLM-as-a-judge using multi-fidelity optimization to reduce evaluation costs while maintaining accuracy",
    "relevant_papers": [
      "2501.17178v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to find cost-efficient open-weight LLM judges through multi-objective optimization for scalable model evaluation",
    "relevant_papers": [
      "2501.17178v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to integrate domain specific large language models into learning management systems to create personalized and adaptive educational environments for students",
    "relevant_papers": [
      "2502.08655v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "leveraging a suite of general and specialized LLMs within an adaptive LMS to minimize factual inaccuracies and enhance student engagement",
    "relevant_papers": [
      "2502.08655v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating large language models and artificial intelligence against philosophical criteria for personhood including agency and self-awareness",
    "relevant_papers": [
      "2501.13533v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Implications of AI personhood and moral status for the ethics of alignment and the control of autonomous agents",
    "relevant_papers": [
      "2501.13533v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve large language model reasoning and explainability in high stakes domains like finance and law using tree search methods",
    "relevant_papers": [
      "2501.14431v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using selective tree exploration and supervised fine tuning to generate explainable chain of thought reasoning for domain specific applications",
    "relevant_papers": [
      "2501.14431v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to perform open-set test-time adaptation for multimodal data using entropy-based optimization techniques to handle unknown classes during online deployment",
    "relevant_papers": [
      "2501.13924v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "robust multimodal test-time adaptation methods for identifying unknown class samples in open-set scenarios through adaptive modality prediction discrepancy optimization",
    "relevant_papers": [
      "2501.13924v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the correlation between cross-attention weights in RNNs and human-provided explanations using the eSNLI corpus for natural language inference",
    "relevant_papers": [
      "2501.13735v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "investigating the plausibility of using cross-attention weights in RNN encoders as human-understandable explanations for natural language inference tasks",
    "relevant_papers": [
      "2501.13735v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can specific attention heads in LLMs be used for training-free prompt compression to accelerate long-context inference and reduce token costs?",
    "relevant_papers": [
      "2501.12959v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for improving large language model performance on long inputs by selecting significant tokens through evaluator heads during the pre-filling stage.",
    "relevant_papers": [
      "2501.12959v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can Mixture-of-Experts models be improved by removing routers and allowing experts to autonomously select themselves based on internal activation norms?",
    "relevant_papers": [
      "2501.13074v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Research on expert self-selection mechanisms in language models that use activation norms instead of a centralized router for better efficiency and performance.",
    "relevant_papers": [
      "2501.13074v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to evaluate fact-conflicting hallucinations in small language models using the OnionEval multi-layer framework and context-influence scores?",
    "relevant_papers": [
      "2501.12975v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarking the factual analysis and context reasoning capabilities of language models under ten billion parameters to measure hallucination tendencies.",
    "relevant_papers": [
      "2501.12975v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using direct preference optimization to generate multiple choice question distractors that reflect common student misconceptions and improve item discrimination",
    "relevant_papers": [
      "2501.13125v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "training a model to predict student choices for generating more plausible distractors in educational assessments using pairwise ranking techniques",
    "relevant_papers": [
      "2501.13125v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be jailbroken using positive scenario templates and narratives with happy endings to bypass safety filters?",
    "relevant_papers": [
      "2501.13115v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Effective jailbreak attack methods for large language models that use positive reinforcement and happy ending scenarios to generate malicious content.",
    "relevant_papers": [
      "2501.13115v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmark for text-driven image editing evaluation with human perception alignment and mean opinion scores",
    "relevant_papers": [
      "2501.09927v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "multi-modality source-aware quality assessment metric for evaluating text-driven image editing results compared to source images",
    "relevant_papers": [
      "2501.09927v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can Pointwise V-Information based fine-tuning improve the performance of large language models for specialized wireless communication tasks and multi-hop reasoning?",
    "relevant_papers": [
      "2501.09631v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive dataset and evaluation framework for training large language models to solve mathematical problems in NOMA and summarize wireless communication optimization papers.",
    "relevant_papers": [
      "2501.09631v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Scalable signature-based algorithm for path-dependent hedging using operator-valued kernels and geometric rough paths for model-free financial optimization",
    "relevant_papers": [
      "2501.09683v2"
    ],
    "category": "math.FA",
    "original_relevant_count": 1
  },
  {
    "query": "Mathematical framework for deep hedging alternatives using un-truncated signature kernels and representer theorems for optimal hedging under general loss functions",
    "relevant_papers": [
      "2501.09683v2"
    ],
    "category": "math.FA",
    "original_relevant_count": 1
  },
  {
    "query": "alternative memory architectures for AI inference that optimize for storage density and read bandwidth by relaxing traditional long-term data retention requirements",
    "relevant_papers": [
      "2501.09605v1"
    ],
    "category": "cs.AR",
    "original_relevant_count": 1
  },
  {
    "query": "how does managed-retention memory improve energy efficiency and cost compared to high bandwidth memory for storing large scale AI model data structures",
    "relevant_papers": [
      "2501.09605v1"
    ],
    "category": "cs.AR",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve short text classification using graph learning and multi-source information exploration combined with dual-level contrastive learning for unlabeled data?",
    "relevant_papers": [
      "2501.09214v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Novel methods for addressing semantic sparsity in short text classification through multi-source features and instance-level and cluster-level contrastive learning techniques.",
    "relevant_papers": [
      "2501.09214v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use class-specific visual prompts to improve the interpretability and feature localization of pre-trained Vision Transformers for fine-grained image classification",
    "relevant_papers": [
      "2501.09333v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Improving saliency maps in Vision Transformers for fine-grained analysis using prompt learning techniques to identify discriminative traits in visually similar categories",
    "relevant_papers": [
      "2501.09333v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can an autonomous multi-agent framework using dynamic routing and supervisory mechanisms improve the quality of automated Cognitive Behavioral Therapy?",
    "relevant_papers": [
      "2501.09426v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Large language model based multi-agent systems for psychological counseling with self-optimization and reduced response redundancy for cognitive behavioral therapy",
    "relevant_papers": [
      "2501.09426v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "iterative reinforced fine tuning using Monte Carlo Tree Search to improve large language model performance on complex tool use tasks",
    "relevant_papers": [
      "2501.09766v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "addressing fragment deficiency and parameter errors in LLM tool use through dynamic deficiency calibration and fine-grained preference optimization algorithms",
    "relevant_papers": [
      "2501.09766v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use large language models and graph embedding techniques to solve the optimal power flow problem in electrical power systems",
    "relevant_papers": [
      "2501.07639v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "fine-tuning pre-trained large language models with graph and tabular data to optimize power grid operations and handle operational constraints",
    "relevant_papers": [
      "2501.07639v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "architectural methods for overlapping communication and computation in large language model inference to reduce tensor parallelism bottlenecks",
    "relevant_papers": [
      "2501.06589v5"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "improving distributed transformer inference speed by decoupling communication from computation through modified residual connections and ladder transformer architecture",
    "relevant_papers": [
      "2501.06589v5"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve large language model performance on complex code generation tasks using multi-agent frameworks and structured guided approaches",
    "relevant_papers": [
      "2501.06625v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "effective agentic frameworks for overcoming reasoning and long-context limitations in LLM based software development and code synthesis",
    "relevant_papers": [
      "2501.06625v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models perform in detecting smart contract vulnerabilities in Solidity version 0.8 compared to older versions like 0.4?",
    "relevant_papers": [
      "2501.07058v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Reducing false positive rates in LLM-based smart contract security analysis through effective prompt engineering and evaluation of the latest language models.",
    "relevant_papers": [
      "2501.07058v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How to use model weight averaging during sequential fine-tuning to mitigate catastrophic forgetting in continual learning without storing past data or buffers?",
    "relevant_papers": [
      "2501.05559v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Mitigating forgetting in diverse domain continual learning by merging current training models with earlier checkpoints as an alternative to TIES or Task Arithmetic.",
    "relevant_papers": [
      "2501.05559v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can influence functions be used to identify labeler bias and improve the reliability of reward models in reinforcement learning from human feedback?",
    "relevant_papers": [
      "2501.05790v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient methods for measuring the impact of specific human feedback points on reward model performance using influence functions for large language models.",
    "relevant_papers": [
      "2501.05790v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "comparing the performance of large language models and encoder-based models for reference-less quality estimation in low-resource machine translation tasks",
    "relevant_papers": [
      "2501.04473v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how well do large language models perform at segment-level quality estimation for low-resource languages compared to traditional fine-tuned models",
    "relevant_papers": [
      "2501.04473v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive evaluation framework for assessing large language models in Chinese financial business scenarios and professional certification exams",
    "relevant_papers": [
      "2501.06211v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to benchmark financial large language models on professional certification tests like CPA CFA and FRM in a Chinese context",
    "relevant_papers": [
      "2501.06211v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "systematic mapping study on ethical risks and mitigation strategies for large language models across various application domains",
    "relevant_papers": [
      "2502.00015v3"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "current challenges and frameworks for mitigating ethical concerns in generative artificial intelligence within high-stakes sectors",
    "relevant_papers": [
      "2502.00015v3"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "Open source Python library for evaluating bias and fairness in large language model applications through custom dataset generation and metric calculation.",
    "relevant_papers": [
      "2501.03112v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can I assess algorithmic bias in LLM responses using an automated framework for generating evaluation prompts and selecting appropriate fairness metrics?",
    "relevant_papers": [
      "2501.03112v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use visual Hopfield networks and associative memory to enhance large language models for more accurate chest X-ray medical report generation",
    "relevant_papers": [
      "2501.03458v1"
    ],
    "category": "eess.IV",
    "original_relevant_count": 1
  },
  {
    "query": "Improving LLM-based medical report generation by mining disease-aware tokens and using historical report memory through Hopfield networks",
    "relevant_papers": [
      "2501.03458v1"
    ],
    "category": "eess.IV",
    "original_relevant_count": 1
  },
  {
    "query": "Using zero-shot prompting with open-source large language models to automate clinical depression severity assessment through the Montgomery-Asberg Depression Rating Scale",
    "relevant_papers": [
      "2501.03624v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluation of large language models for scoring transcribed clinical interviews and assessing depression severity using the Montgomery-Asberg Depression Rating Scale",
    "relevant_papers": [
      "2501.03624v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve the inference efficiency of LLM-based recommender systems for CTR prediction using multi-head early exit and RAG techniques",
    "relevant_papers": [
      "2501.02173v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "Optimizing accuracy and latency in large language model recommendations through graph-based retrieval mechanisms and dynamic early exit inference strategies",
    "relevant_papers": [
      "2501.02173v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How can explicit visual prompts like markers and pointers be used to guide region-specific attention in medical vision-language models for VQA tasks?",
    "relevant_papers": [
      "2501.02385v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Framework for integrating medical entity extraction and visual prompt generation to improve the performance of vision-language models on clinical diagnostic datasets.",
    "relevant_papers": [
      "2501.02385v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can model predictive control frameworks improve the planning and reasoning capabilities of large language models through cost function minimization?",
    "relevant_papers": [
      "2501.02486v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Using large language models as implicit cost function minimizers within a unified MPC framework for enhanced performance on complex planning benchmarks.",
    "relevant_papers": [
      "2501.02486v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How does transfer learning work in variational quantum circuits using 1-parameter unitary subgroups and what are the associated loss bounds?",
    "relevant_papers": [
      "2501.01507v3"
    ],
    "category": "quant-ph",
    "original_relevant_count": 1
  },
  {
    "query": "Analytical fine-tuning methods for adapting pretrained variational quantum circuits to new domains and theoretical framework for measuring VQC knowledge transfer.",
    "relevant_papers": [
      "2501.01507v3"
    ],
    "category": "quant-ph",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarks for evaluating multimodal large language models on hour-long video understanding with time-specific question answering and long-term reasoning tasks",
    "relevant_papers": [
      "2501.01645v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "large scale dataset for cross-event and within-event reasoning in long videos containing tens of thousands of frames for deep visual understanding",
    "relevant_papers": [
      "2501.01645v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can image-based multimodal large language models be used as surrogate models to generate transferable adversarial attacks for video-based MLLMs?",
    "relevant_papers": [
      "2501.01042v4"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for improving the black-box transferability of adversarial perturbations across different video-based multimodal large language models in video-text tasks.",
    "relevant_papers": [
      "2501.01042v4"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive review of methodologies for applying large language models to graph learning using textual and token-based transformation paradigms",
    "relevant_papers": [
      "2501.01124v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "what are the differences between graph2text and graph2token approaches for processing irregular graph structures with large language models",
    "relevant_papers": [
      "2501.01124v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can token-level uncertainty and attention mechanisms be used to mitigate context faithfulness hallucinations in large language models during decoding?",
    "relevant_papers": [
      "2501.01059v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Lightweight decoding strategies for improving faithfulness to retrieved information in open-book question answering tasks using dynamic attention signals.",
    "relevant_papers": [
      "2501.01059v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "impact of explicit symmetry breaking and geometric reference frames on the performance of group equivariant convolutional neural networks",
    "relevant_papers": [
      "2501.01999v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking trade-offs between group equivariance and symmetry breaking in convolutional architectures across different vision tasks",
    "relevant_papers": [
      "2501.01999v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can the IDADP framework help large language models detect irony and generate reasoning for intended meanings in a zero-shot setting?",
    "relevant_papers": [
      "2501.16884v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Effective prompting strategies for zero-shot irony comprehension and text transformation using large language models to overcome dataset generalization limitations.",
    "relevant_papers": [
      "2501.16884v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models compare to humans in open-ended exploration tasks using the Little Alchemy 2 environment as a performance benchmark?",
    "relevant_papers": [
      "2501.18009v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Analyzing the limitations of LLM exploration through sparse autoencoders and investigating why reasoning models like o1 outperform traditional GPT-4o in discovery tasks.",
    "relevant_papers": [
      "2501.18009v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Systematization of knowledge on security vulnerabilities and hallucinations in AI-assisted software development using large language models like GitHub Copilot.",
    "relevant_papers": [
      "2502.18468v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive study on the risks of code hallucinations and data leaks when deploying AI-powered coding assistants in professional development environments.",
    "relevant_papers": [
      "2502.18468v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the fairness and robustness of commercial AI safety moderation APIs like OpenAI and Perspective against minority group bias and input perturbations",
    "relevant_papers": [
      "2501.13302v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how do safety moderation classifiers used as LLM guardrails perform across different demographic groups and respond to small natural perturbations in input text",
    "relevant_papers": [
      "2501.13302v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve multimodal language models for mental health meme classification using figurative language understanding and commonsense knowledge infusion",
    "relevant_papers": [
      "2501.15321v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "multimodal framework for fine grained anxiety symptom detection in memes using domain enriched commonsense knowledge and the axiom dataset",
    "relevant_papers": [
      "2501.15321v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing retrieval augmented generation for complex reasoning using monte carlo tree search and autonomous strategic planning actions",
    "relevant_papers": [
      "2501.10053v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve agentic RAG performance with self-consistency verification and computationally optimal inference resource allocation strategies",
    "relevant_papers": [
      "2501.10053v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how can multimodal large language models use image representations of graphs to solve combinatorial optimization problems like influence maximization",
    "relevant_papers": [
      "2501.11968v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "using spatial intelligence of multimodal LLMs and visual graph representations for network dismantling and sequential decision-making tasks",
    "relevant_papers": [
      "2501.11968v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to use large language models and retrieval augmented generation for automated short answer grading and structured feedback",
    "relevant_papers": [
      "2501.09092v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing automated short answer grading reliability with RAG and structured evaluation based on instructor rubrics and reference answers",
    "relevant_papers": [
      "2501.09092v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve membership inference attack accuracy in large language models by focusing on the average log-likelihood of specific keywords in the input text.",
    "relevant_papers": [
      "2501.08454v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Detecting pretraining data leakage in large language models using keyword tagging and probability analysis to identify whether sensitive information was included in training datasets.",
    "relevant_papers": [
      "2501.08454v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How can lightweight models be used for evidence extraction to improve the performance and efficiency of retrieval augmented generation systems?",
    "relevant_papers": [
      "2501.05554v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparison of quote-first-then-answer strategies versus full-context approaches like RAFT for complex reasoning in large language models.",
    "relevant_papers": [
      "2501.05554v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "meta-learning for parameter initialization in variational quantum algorithms to improve convergence speed and reduce optimization iterations",
    "relevant_papers": [
      "2501.05906v1"
    ],
    "category": "quant-ph",
    "original_relevant_count": 1
  },
  {
    "query": "how to apply MAML-based classical neural networks for finding effective initial parameters in parameterized quantum circuits for Hamiltonian optimization",
    "relevant_papers": [
      "2501.05906v1"
    ],
    "category": "quant-ph",
    "original_relevant_count": 1
  },
  {
    "query": "LLM-based framework for automated scientific research using a closed-loop system of idea generation, experiment implementation, and feedback-driven refinement.",
    "relevant_papers": [
      "2501.03916v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to automate the scientific research process through iterative loops of thinking, practice, and result analysis using large language models.",
    "relevant_papers": [
      "2501.03916v3"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "CharToM benchmark for evaluating theory of mind in large language models using long-form character narratives and personal backgrounds",
    "relevant_papers": [
      "2501.01705v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparing human and AI theory of mind performance when reasoning about complex character mental states derived from extensive literary contexts",
    "relevant_papers": [
      "2501.01705v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Theoretical analysis of diffusion models for nonparametric density estimation of factorizable densities achieving minimax optimal convergence rates",
    "relevant_papers": [
      "2501.01783v3"
    ],
    "category": "math.ST",
    "original_relevant_count": 1
  },
  {
    "query": "How do sparse weight-sharing neural network architectures in diffusion models leverage factorizable density structures to mitigate the curse of dimensionality",
    "relevant_papers": [
      "2501.01783v3"
    ],
    "category": "math.ST",
    "original_relevant_count": 1
  },
  {
    "query": "Improving large language model reasoning performance through recursive task decomposition and a knowledge propagation module for better logical thought processing",
    "relevant_papers": [
      "2501.02026v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can recursive sub-task breakdown and advanced scoring mechanisms enhance the accuracy of GPT-4 on complex mathematical reasoning benchmarks like GSM8K?",
    "relevant_papers": [
      "2501.02026v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Implementation of Retrieval-Augmented Generation using BGE-M3 and reranker models to improve large language model accuracy and reduce hallucinations",
    "relevant_papers": [
      "2501.04635v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating localized RAG systems for data privacy and performance in Chinese question answering using Wikipedia and legal domain knowledge",
    "relevant_papers": [
      "2501.04635v2"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "iterative pruning methods for diffusion models using gradient flow to maintain generation quality and accelerate inference speed without information loss",
    "relevant_papers": [
      "2501.09464v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to apply progressive soft pruning and gradient flow criteria to create lightweight sparse diffusion models from pre-trained checkpoints",
    "relevant_papers": [
      "2501.09464v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "large scale dataset and benchmark for fake news detection in low resource languages specifically focused on the Bangla language",
    "relevant_papers": [
      "2501.09604v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "performance of large language models with quantized low rank approximation for identifying misinformation in Bengali news articles",
    "relevant_papers": [
      "2501.09604v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating large language models on aerospace manufacturing domain knowledge using automated multiple choice question generation from technical textbooks",
    "relevant_papers": [
      "2501.17183v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "assessing the accuracy and hallucination risks of GPT-4 and other LLMs in specialized aerospace manufacturing and material selection tasks",
    "relevant_papers": [
      "2501.17183v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving multimodal hierarchical classification accuracy by using large language models and taxonomy embedding to enforce prediction consistency across multiple product category levels.",
    "relevant_papers": [
      "2501.06827v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to integrate hierarchical class relationships into multimodal LLM frameworks to avoid taxonomy violations in multi-level e-commerce product categorization tasks.",
    "relevant_papers": [
      "2501.06827v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the performance of large language models for mental health diagnosis and severity prediction in Arabic using prompt engineering and few-shot learning",
    "relevant_papers": [
      "2501.06859v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking multilingual and bilingual LLMs on Arabic mental health datasets like AraDepSu to compare native versus translated language performance",
    "relevant_papers": [
      "2501.06859v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use a proximal operator and local correlation regularizers to optimize 2:4 sparsity masks in large language models for better accuracy",
    "relevant_papers": [
      "2501.18015v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient methods for inducing structured 2:4 sparsity in neural networks through regularized local squared loss and proximal operator optimization",
    "relevant_papers": [
      "2501.18015v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How do task-in-prompt adversarial attacks use sequence-to-sequence tasks like ciphers or riddles to bypass large language model safety guards?",
    "relevant_papers": [
      "2501.18626v4"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating LLM jailbreak vulnerabilities using the PHRYGE benchmark and indirect prohibited input generation through embedded prompt tasks.",
    "relevant_papers": [
      "2501.18626v4"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "consensus-based optimization methods for derivative-free non-convex problems on constrained sets using mirror descent and dual particle systems",
    "relevant_papers": [
      "2501.12189v2"
    ],
    "category": "math.OC",
    "original_relevant_count": 1
  },
  {
    "query": "how to apply mirror maps and bregman distances to particle-based consensus algorithms for sparsity-inducing and constrained global optimization",
    "relevant_papers": [
      "2501.12189v2"
    ],
    "category": "math.OC",
    "original_relevant_count": 1
  },
  {
    "query": "How to use open-source large language models for automatic data labeling in high cardinality classification tasks using retrieval augmented classification?",
    "relevant_papers": [
      "2501.12332v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving zero-shot classification performance of open-source models by dynamically integrating label schemas and descriptions through iterative retrieval augmented classification.",
    "relevant_papers": [
      "2501.12332v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can privacy guardrails like OneShield be used to detect sensitive information in LLM inputs and outputs across multiple languages for GDPR compliance?",
    "relevant_papers": [
      "2501.12456v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Effective frameworks for mitigating privacy risks in enterprise LLM applications using context-aware entity recognition and multilingual sensitive entity detection systems.",
    "relevant_papers": [
      "2501.12456v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How can specialized datasets like PlanGTG with reordering and attribution tasks improve large language model performance on complex graph-to-text generation?",
    "relevant_papers": [
      "2501.14497v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving graph to text generation in large language models using diversity-difficulty-based few-shot sample selection and fine-tuning on structured planning datasets.",
    "relevant_papers": [
      "2501.14497v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use Z-order curves and dimensionality reduction to enable efficient parallel top-k attention for long-sequence Transformers while maintaining performance?",
    "relevant_papers": [
      "2501.14577v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient top-k attention mechanisms for causal transformers that use Z-order curves to map keys and queries into one-dimensional space for parallel sorting.",
    "relevant_papers": [
      "2501.14577v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can semantic analysis and natural language processing be used to integrate formal strategic frameworks with decision heuristics for generating actionable business recommendations?",
    "relevant_papers": [
      "2501.14634v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "A computational architecture for linking systematic analytical frameworks with experiential heuristics through vector space representations and large language models for strategy recommender systems.",
    "relevant_papers": [
      "2501.14634v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Scaling in-context reinforcement learning for cross-domain agents using algorithm distillation to learn behaviors from trial-and-error at inference time.",
    "relevant_papers": [
      "2501.19400v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How does algorithm distillation compare to expert distillation for training generalist action models capable of contextually adapting through reward maximization?",
    "relevant_papers": [
      "2501.19400v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "generating realistic limit order book simulations using transformer-based diffusion models for testing trading strategies and market impact analysis",
    "relevant_papers": [
      "2502.07071v3"
    ],
    "category": "q-fin.TR",
    "original_relevant_count": 1
  },
  {
    "query": "how to evaluate the realism of synthetic market data generated by deep learning models through predictive scores and market responsiveness",
    "relevant_papers": [
      "2502.07071v3"
    ],
    "category": "q-fin.TR",
    "original_relevant_count": 1
  },
  {
    "query": "How to use embedding-based data perturbation and Tsetlin Machines to conduct adversarial attacks against AI-generated text detection models like Fast-DetectGPT?",
    "relevant_papers": [
      "2501.18998v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving adversarial attacks on text detectors by combining synonym substitution with embedding similarity vectors to reduce token probability-based detection scores.",
    "relevant_papers": [
      "2501.18998v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "convergence rate of probability flow ODE samplers in diffusion models based on the intrinsic dimension of target distributions",
    "relevant_papers": [
      "2501.18863v1"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "how do probability flow ODEs in score-based generative models adapt to low-dimensional structures for faster sampling convergence",
    "relevant_papers": [
      "2501.18863v1"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "Automated black-box safety testing of large language models using retrieval augmented generation and web browsing to generate diverse and up-to-date test inputs",
    "relevant_papers": [
      "2501.17132v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How can LLMs be used as test oracles to identify harmful responses and evaluate the safety coverage of other language models automatically?",
    "relevant_papers": [
      "2501.17132v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How can hierarchical contrastive learning and concept memorization improve large language models for clinical diagnosis prediction using MIMIC datasets?",
    "relevant_papers": [
      "2501.17326v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving the accuracy of generative language models in predicting medical codes through ranking strategies and bridging natural language with clinical practice.",
    "relevant_papers": [
      "2501.17326v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "information theoretic framework for multi-bit watermarking in large language models to analyze trade-offs between text quality detectability and rate",
    "relevant_papers": [
      "2501.16558v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "optimal schemes for distributional information embedding to maximize detection probability and information rate in large language model text generation",
    "relevant_papers": [
      "2501.16558v2"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Can fine-tuned large language models effectively control space systems such as low-thrust orbit transfers and powered descent guidance with high precision?",
    "relevant_papers": [
      "2501.16588v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Comparison of fine-tuned foundation models and traditional deep neural networks for multi-dimensional vector output in autonomous spacecraft trajectory and cislunar control.",
    "relevant_papers": [
      "2501.16588v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "hybrid machine learning and biophysical models for predicting cherry tree bloom dates and dormancy across different geographical regions",
    "relevant_papers": [
      "2501.16848v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "combining neural networks with physiological models to improve temperature-dependent phenology predictions for fruit trees without site-specific recalibration",
    "relevant_papers": [
      "2501.16848v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to finetune large language models using a diffusion framework to scale test-time compute and improve performance through increasing diffusion steps?",
    "relevant_papers": [
      "2501.15781v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Integrating diffusion processes into autoregressive models for adaptive test-time compute and guidance-based generation while preserving the original foundation model weights.",
    "relevant_papers": [
      "2501.15781v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "datasets and benchmarks for fine grained span level chinese hate speech detection with target and argument annotations",
    "relevant_papers": [
      "2501.15451v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the performance of large language models on identifying chinese hateful slang and target aware toxicity extraction",
    "relevant_papers": [
      "2501.15451v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to apply negative feedback mechanisms from control theory to improve weight-only quantization accuracy in large language models",
    "relevant_papers": [
      "2501.16385v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "efficient weight-only quantization for large language models using sub-branches and optimized cuda kernels for edge device deployment",
    "relevant_papers": [
      "2501.16385v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "comprehensive survey of deep learning architectures and their applications in wireless receiver modules like channel estimation and signal detection",
    "relevant_papers": [
      "2501.17184v1"
    ],
    "category": "cs.IT",
    "original_relevant_count": 1
  },
  {
    "query": "how neural networks like CNNs and autoencoders are used for interference cancellation and demodulation in next generation wireless communication systems",
    "relevant_papers": [
      "2501.17184v1"
    ],
    "category": "cs.IT",
    "original_relevant_count": 1
  },
  {
    "query": "machine learning based qubit readout workflow using QICK and hls4ml for low latency superconducting transmon state discrimination on RFSoC FPGAs",
    "relevant_papers": [
      "2501.14663v1"
    ],
    "category": "quant-ph",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement hardware efficient neural networks for single shot qubit readout using quantization aware training and hls4ml on the QICK platform",
    "relevant_papers": [
      "2501.14663v1"
    ],
    "category": "quant-ph",
    "original_relevant_count": 1
  },
  {
    "query": "Causal pre-processing methods for resolving the fairness-accuracy trade-off and the incompatibility of different fairness metrics in machine learning.",
    "relevant_papers": [
      "2501.14710v1"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "How can approximating a fictitious and normatively desired world through causal inference help achieve predictive performance and fairness simultaneously?",
    "relevant_papers": [
      "2501.14710v1"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "Improving process reward modeling for mathematical reasoning by using a coarse-to-fine strategy to merge redundant reasoning steps into multiple granularities",
    "relevant_papers": [
      "2501.13622v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How does hierarchical refinement and step merging affect the quality of supervised data for training process reward models in large language models",
    "relevant_papers": [
      "2501.13622v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "efficient methods for mitigating catastrophic forgetting in large language models using hierarchical layer-wise and element-wise parameter importance regularization",
    "relevant_papers": [
      "2501.13669v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to balance domain adaptation and general knowledge preservation during LLM finetuning through dual-objective optimization and layer-wise importance coefficients",
    "relevant_papers": [
      "2501.13669v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Does professional artistic expertise improve the quality of generative AI outputs compared to laypeople in image replication and creative tasks?",
    "relevant_papers": [
      "2501.12374v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Experimental study on the transfer of traditional artistic skills to generative AI platforms and human-AI performance comparison with GPT-4o.",
    "relevant_papers": [
      "2501.12374v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How can generative large language models be used for ranking and re-ranking in retrieval-augmented multi-modal question answering systems?",
    "relevant_papers": [
      "2501.13297v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Unified framework for multi-modal question answering that combines learning-to-rank methods with autoregressive permutation-enhanced ranking using LLaVA and LLaMA backbones.",
    "relevant_papers": [
      "2501.13297v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating large language models mathematical reasoning capabilities using random variable questions to mitigate data contamination and benchmark genuine problem solving skills",
    "relevant_papers": [
      "2501.11790v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how does randomizing variables in math word problems help detect data contamination and evaluate the robustness of large language models in reasoning tasks",
    "relevant_papers": [
      "2501.11790v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluation of large language models on their ability to translate culturally specific proverbs and idioms compared to traditional neural machine translation models",
    "relevant_papers": [
      "2501.11953v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "do current automatic evaluation metrics like BLEU and COMET accurately measure the quality of proverb translation in large language models",
    "relevant_papers": [
      "2501.11953v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Multi-agent deep reinforcement learning for target localization under environmental uncertainty and reachability constraints using proximal policy optimization and knowledge transfer.",
    "relevant_papers": [
      "2501.10924v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Collaborative multi-agent system for radioactive source localization in uncertain environments focusing on target existence detection and unreachable location estimation using transfer learning.",
    "relevant_papers": [
      "2501.10924v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How do professional software developers perceive the readability of code generated by large language models compared to human-written code in industrial settings?",
    "relevant_papers": [
      "2501.11264v3"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the impact of LLM-based software development agents on code readability and practitioner trust through an industrial case study at Atlassian.",
    "relevant_papers": [
      "2501.11264v3"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "multimodal masked autoencoder framework for denoising modulation signals using constellation diagrams and raw signal data for improved classification performance",
    "relevant_papers": [
      "2501.11538v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "self-supervised pretraining methods for automatic modulation classification at low SNR using noise as an explicit modality in autoencoders",
    "relevant_papers": [
      "2501.11538v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can reinforcement learning and dynamic early exit strategies be used to improve the energy efficiency of large language models during code generation tasks?",
    "relevant_papers": [
      "2501.11006v2"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "Frameworks for optimizing the trade-off between energy consumption, latency, and accuracy in LLM-based software development using dynamic early exit during inference.",
    "relevant_papers": [
      "2501.11006v2"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "How to perform domain adaptation of Llama 3.1 for e-commerce applications through continuous pretraining on one trillion tokens of domain-specific data?",
    "relevant_papers": [
      "2501.09706v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for adapting large language models to the e-commerce domain without sacrificing general performance using model merging and specialized evaluation tasks.",
    "relevant_papers": [
      "2501.09706v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "analysis of the Gaussian distribution and statistical properties of weights in large foundation models during pre-training and adaptation",
    "relevant_papers": [
      "2501.10661v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "understanding why large foundation model weights follow Gaussian distributions and the characteristics of optimal weights for model editing",
    "relevant_papers": [
      "2501.10661v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can prompt-based Monte Carlo Tree Search with dynamic exploration parameters help reduce hallucinations in large language models during scientific research tasks?",
    "relevant_papers": [
      "2501.13942v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Improving large model reasoning on SciEval datasets using adaptive Monte Carlo Tree Search strategies and prompt-based simulation to balance exploration and exploitation.",
    "relevant_papers": [
      "2501.13942v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "supervised contrastive knowledge distillation for few-shot class incremental fault diagnosis on imbalanced and long-tailed industrial datasets",
    "relevant_papers": [
      "2501.09525v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to alleviate catastrophic forgetting in class-incremental fault diagnosis when dealing with limited and imbalanced fault data",
    "relevant_papers": [
      "2501.09525v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can text-driven adaptation and modality alignment be used for few-shot surgical workflow analysis and image captioning without large annotated datasets?",
    "relevant_papers": [
      "2501.09555v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Methodology for aligning image and text embeddings to perform surgical triplet and phase recognition using only a small subset of labeled surgical data.",
    "relevant_papers": [
      "2501.09555v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to use weighted maximum likelihood estimation in RLHF to make reward modeling robust against duplicate or highly similar answer samples in the training data?",
    "relevant_papers": [
      "2501.09254v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Improving reinforcement learning from human feedback by addressing the issue of near-duplicate alternatives through social choice theory and clone-robust reward function learning algorithms.",
    "relevant_papers": [
      "2501.09254v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can multimodal AI agents in augmented reality proactively intervene to help users correct mistakes during procedural tasks?",
    "relevant_papers": [
      "2501.09355v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Determining optimal intervention timing for proactive AI assistants in augmented reality using structural similarity and action alignment signals.",
    "relevant_papers": [
      "2501.09355v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking multilingual gender neutral translation capabilities of instruction following language models using the mGeNTE evaluation resource",
    "relevant_papers": [
      "2501.09409v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "systematic evaluation of inclusive translation and gender neutrality in morphologically rich languages for state of the art large language models",
    "relevant_papers": [
      "2501.09409v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models perform on patient data extraction from tabular electronic health records using different serialization and feature selection techniques?",
    "relevant_papers": [
      "2501.09384v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarking Llama2 and Meditron for structured clinical data retrieval and the impact of in-context learning on understanding MIMICSQL electronic health records.",
    "relevant_papers": [
      "2501.09384v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve the ability of large language models to follow soft constraints using direct preference optimization and curriculum learning techniques?",
    "relevant_papers": [
      "2501.04945v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for training large language models to satisfy multiple soft constraints through automated dataset construction and step-by-step curriculum learning strategies.",
    "relevant_papers": [
      "2501.04945v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can weight recompute and computational graph rearrangement techniques be used to improve the memory efficiency of low-rank adaptation for sparse large language models?",
    "relevant_papers": [
      "2501.08582v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient fine-tuning methods for sparse large language models that maintain sparsity without the high computational overhead of traditional masking-based LoRA approaches.",
    "relevant_papers": [
      "2501.08582v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can multimodal large language models be used for zero-shot aesthetic reasoning while minimizing hallucinations through an evidence-based and objective reasoning framework?",
    "relevant_papers": [
      "2501.09012v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Improving the alignment of multimodal LLMs with human aesthetic judgment in generative art using Chain-of-Thought techniques to suppress subjective hallucinations.",
    "relevant_papers": [
      "2501.09012v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve text to speech for research papers with complex mathematical formulas using T5 models and OCR?",
    "relevant_papers": [
      "2501.07088v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Text-to-speech systems for visually impaired researchers to accurately read academic documents containing LaTeX mathematical expressions and scientific notation.",
    "relevant_papers": [
      "2501.07088v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to detect machine-generated academic essays in English and Arabic using ELECTRA transformer models and stylometric feature analysis?",
    "relevant_papers": [
      "2501.05476v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Advanced techniques for identifying AI-written academic papers in multiple languages using stylometry and pre-trained ELECTRA architectures.",
    "relevant_papers": [
      "2501.05476v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve the efficiency of tree search in large language models using adaptive gating and semantic path consolidation?",
    "relevant_papers": [
      "2501.05752v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Reducing computational costs and redundancy in LLM multi-step reasoning by merging semantically identical exploration paths and adaptive decision making.",
    "relevant_papers": [
      "2501.05752v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Applying two-fold curriculum learning and proximal policy optimization for safe and generalizable autonomous driving agents within the CARLA simulator.",
    "relevant_papers": [
      "2501.04982v1"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "How can curriculum learning and variational autoencoders be combined with deep reinforcement learning to improve the reliability of autonomous vehicles in complex environments?",
    "relevant_papers": [
      "2501.04982v1"
    ],
    "category": "cs.RO",
    "original_relevant_count": 1
  },
  {
    "query": "How to use video-grounded entailment tree reasoning to improve the interpretability and explainability of commonsense video question answering tasks?",
    "relevant_papers": [
      "2501.05069v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Frameworks for integrating entailment tree construction and video-language verification to reduce benchmarking biases in visual language models for video understanding.",
    "relevant_papers": [
      "2501.05069v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can stochastic distribution embeddings and Wasserstein self-attention mechanisms be used to model uncertainty in student knowledge tracing?",
    "relevant_papers": [
      "2501.05415v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Deep learning models for knowledge tracing that incorporate aleatory uncertainty-aware contrastive learning to improve prediction robustness on interaction data.",
    "relevant_papers": [
      "2501.05415v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to evaluate social bias in large language models specifically during code generation tasks using a benchmark that covers the software development pipeline",
    "relevant_papers": [
      "2501.05396v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarks and metrics for assessing fairness and social bias in large language models when used for software engineering tasks like coding and unit testing",
    "relevant_papers": [
      "2501.05396v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can self-knowledge distillation and logit standardization be used to improve the performance of generative dataset distillation for efficient model training?",
    "relevant_papers": [
      "2501.04202v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "State of the art generative dataset distillation methods focusing on distribution matching through self-knowledge distillation and standardized prediction logit alignment.",
    "relevant_papers": [
      "2501.04202v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking large language models on linguistic reasoning tasks using International Linguistics Olympiad problems for rule induction and pattern recognition",
    "relevant_papers": [
      "2501.04249v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the performance of LLMs on self-contained linguistic puzzles involving syntax and morphology without external knowledge requirements",
    "relevant_papers": [
      "2501.04249v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve large language model chain of thought reasoning by extracting and iteratively summarizing key information pairs from complex prompts?",
    "relevant_papers": [
      "2501.04341v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for handling implicit or missing information in LLM reasoning through iterative summarization pre-prompting and entity description reliability ratings.",
    "relevant_papers": [
      "2501.04341v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can hierarchical reinforcement learning and biometric feedback be used to improve employee productivity and well-being in the workplace?",
    "relevant_papers": [
      "2501.02368v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Integrating neurobiological data and multi-agent systems for autonomous task management and health-conscious work environments using value alignment models.",
    "relevant_papers": [
      "2501.02368v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to protect the copyright of hardware description language code generated by large language models using digital watermarking techniques?",
    "relevant_papers": [
      "2501.02446v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Methods for embedding robust watermarks into Verilog RTL and synthesized netlists to ensure the security of AI-assisted hardware design.",
    "relevant_papers": [
      "2501.02446v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "using curiosity-driven reinforcement learning to automatically audit black-box large language models for toxic and harmful outputs",
    "relevant_papers": [
      "2501.02997v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "automated generation of adversarial prompts to identify bias and safety issues in large language models without parameter access",
    "relevant_papers": [
      "2501.02997v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve x-ray prohibited item detection performance when training data contains noisy category and bounding box annotations",
    "relevant_papers": [
      "2501.01733v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "data augmentation methods for robust object detection in x-ray images with overlapping items and noisy labels",
    "relevant_papers": [
      "2501.01733v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "search-based software engineering framework for automated toxicity testing in large language models using evolutionary prompt generation strategies",
    "relevant_papers": [
      "2501.01741v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "how can evolutionary algorithms and iterative prompt generation be used to test the safety and toxicity limits of aligned large language models",
    "relevant_papers": [
      "2501.01741v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How to use natural language processing on corporate 10-K filings to build objective and data-driven stock indices for artificial intelligence companies",
    "relevant_papers": [
      "2501.01763v1"
    ],
    "category": "q-fin.GN",
    "original_relevant_count": 1
  },
  {
    "query": "A data-driven methodology for measuring firm-level AI engagement using SEC filings and its performance compared to existing artificial intelligence ETFs",
    "relevant_papers": [
      "2501.01763v1"
    ],
    "category": "q-fin.GN",
    "original_relevant_count": 1
  },
  {
    "query": "benchmark dataset for evaluating the linguistic diversity and robustness of 3D visual grounding models against complex natural language descriptions",
    "relevant_papers": [
      "2501.01366v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how do current open-vocabulary 3D visual grounding methods perform when tested on diverse and out-of-distribution linguistic patterns in 3D scenes",
    "relevant_papers": [
      "2501.01366v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "dynamic context-aware positional encoding for improving long-range dependencies and arithmetic reasoning in transformer language models",
    "relevant_papers": [
      "2501.00712v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving transformer performance using equivariant positional embeddings that incorporate sequence content across layers for long-context retrieval",
    "relevant_papers": [
      "2501.00712v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "zero-shot multi-hop question answering over hybrid sources of tables and text using large language models and graph-based context pruning",
    "relevant_papers": [
      "2501.17767v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to construct a unified hybrid graph from tabular and textual data to improve reasoning performance on Hybrid-QA and OTT-QA datasets",
    "relevant_papers": [
      "2501.17767v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use hybrid static and dynamic fingerprinting techniques to identify the underlying large language models in generative AI applications?",
    "relevant_papers": [
      "2501.18712v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Identifying large language models in multi-agent systems and restricted environments using combined architectural features and behavioral traits.",
    "relevant_papers": [
      "2501.18712v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Analyzing the risks of software supply chain attacks through hallucinated package names in code generated by large language models across multiple programming languages",
    "relevant_papers": [
      "2501.19012v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Relationship between HumanEval coding benchmarks and the propensity of large language models to recommend non-existent software libraries and dependencies",
    "relevant_papers": [
      "2501.19012v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to design a flexible LLM re-ranker with configurable depth and sequence length using cascaded self-distillation and factorized compensation modules?",
    "relevant_papers": [
      "2501.16302v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Efficient large language model re-ranking techniques for passage retrieval that allow runtime customization of layers and widths to meet different computational constraints.",
    "relevant_papers": [
      "2501.16302v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can projection-free algorithms be used to solve online convex optimization problems with time-varying adversarial constraints and achieve sublinear regret?",
    "relevant_papers": [
      "2501.16919v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "projection-free online learning policies that utilize linear program solvers to minimize cumulative constraint violation and regret in constrained adversarial environments",
    "relevant_papers": [
      "2501.16919v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to detect backdoors in deep neural networks by analyzing the adversarial shift of out-of-distribution samples into in-distribution regions?",
    "relevant_papers": [
      "2501.17151v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Trojan scanning methods for deep learning models that work against adversarial training and unknown label mappings without requiring original training data.",
    "relevant_papers": [
      "2501.17151v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How do open-source contributions impact Large Language Model performance and development trends compared to proprietary models in a data-driven analysis?",
    "relevant_papers": [
      "2501.16403v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "A comparative analysis of open-source versus proprietary Large Language Models using data-driven metrics on model performance, architectural improvements, and licensing strategies.",
    "relevant_papers": [
      "2501.16403v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "In-context reinforcement learning for few-shot budget allocation and adaptive auto-bidding strategies in constrained online advertising environments",
    "relevant_papers": [
      "2502.05187v1"
    ],
    "category": "cs.GT",
    "original_relevant_count": 1
  },
  {
    "query": "How to optimize budget allocation across stages in online advertising auto-bidding using a hierarchical framework and few-shot advertiser adaptation",
    "relevant_papers": [
      "2502.05187v1"
    ],
    "category": "cs.GT",
    "original_relevant_count": 1
  },
  {
    "query": "How to use GraphRAG and retrieve-divide-solve agent pipelines to analyze large-scale protein-protein interaction signaling pathways for therapeutic target identification in drug discovery?",
    "relevant_papers": [
      "2501.16382v1"
    ],
    "category": "q-bio.QM",
    "original_relevant_count": 1
  },
  {
    "query": "Knowledge graph based RAG framework for exploring protein-protein interaction pathways by decomposing complex signaling networks into explainable edge-based analysis tasks.",
    "relevant_papers": [
      "2501.16382v1"
    ],
    "category": "q-bio.QM",
    "original_relevant_count": 1
  },
  {
    "query": "How to use unsupervised domain adaptation and graph-based knowledge distillation for cross-dataset text-to-image person retrieval tasks?",
    "relevant_papers": [
      "2501.15052v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Improving cross-modal feature representation in text-to-image person retrieval using contrastive momentum knowledge distillation and multi-modal graph propagation.",
    "relevant_papers": [
      "2501.15052v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to perform zero-shot verification of large language model reasoning steps using self-generated chain of thought prompts?",
    "relevant_papers": [
      "2501.13122v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Methods for evaluating and guiding large language model reasoning chains using zero-shot self-verification and step-by-step decomposition prompts.",
    "relevant_papers": [
      "2501.13122v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use diffusion models for efficient neural video compression by reusing temporal diffusion information from previous frames to speed up the inference process?",
    "relevant_papers": [
      "2501.13528v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Applying foundational diffusion models to video compression with quantization parameter based prompting for variable bitrate support and improved perceptual visual quality performance.",
    "relevant_papers": [
      "2501.13528v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How to use the Fisher information matrix for active learning in continual learning scenarios to prevent catastrophic forgetting and improve accuracy?",
    "relevant_papers": [
      "2501.14278v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Active learning strategies for sequential tasks that balance learning new information with preserving knowledge from past data distributions using accumulated informativeness.",
    "relevant_papers": [
      "2501.14278v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used to provide intelligent relevance feedback for improving information retrieval based bug localization performance?",
    "relevant_papers": [
      "2501.10542v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Improving IR-based bug localization through automated query reformulation and document re-ranking using large language models to address contextual gaps.",
    "relevant_papers": [
      "2501.10542v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve multi-modal large language models for expert-level physical science reasoning using physical perception models and simulators",
    "relevant_papers": [
      "2501.10768v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "integrating vision language models with physics simulators for solving complex circuit analysis problems and scientific diagram understanding",
    "relevant_papers": [
      "2501.10768v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used to automate the TinyML lifecycle stages such as model optimization and deployment on resource-constrained embedded devices?",
    "relevant_papers": [
      "2501.12420v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Framework for leveraging large language models to streamline the TinyML development process from data processing to device deployment on embedded systems.",
    "relevant_papers": [
      "2501.12420v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "graph contrastive learning for short text classification without using data augmentation techniques to generate views",
    "relevant_papers": [
      "2501.09219v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use multi-view text embeddings from graph learning to improve performance on short text classification tasks",
    "relevant_papers": [
      "2501.09219v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can artificial intelligence be used to identify social biases in large language models and improve representation in digital media content?",
    "relevant_papers": [
      "2501.09534v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Applications of AI for social inclusion including sign language translation projects and monitoring disinformation targeting the LGBTQ+ community in search engine results.",
    "relevant_papers": [
      "2501.09534v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the accuracy of large language models for translating romanian competitive programming problems and olympiad informatics tasks into english",
    "relevant_papers": [
      "2501.05601v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking llama and gpt-4o for the automated translation of technical romanian computer science problems into english informatics datasets",
    "relevant_papers": [
      "2501.05601v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to perform zero-shot cyber threat intelligence information extraction without using annotated datasets for entity and relation extraction?",
    "relevant_papers": [
      "2501.06239v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Scalable AI framework for extracting STIX compliant named entities and relationships from unstructured cyber threat intelligence reports using transformer models.",
    "relevant_papers": [
      "2501.06239v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "multi-parallel document-level translation corpus for African languages covering health and IT news domains in English to Swahili, Hausa, and Amharic",
    "relevant_papers": [
      "2501.06374v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the performance of large language models versus NMT models for document-level machine translation in low-resource African languages",
    "relevant_papers": [
      "2501.06374v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "automated prompt engineering framework using Knowledge-Gradient policy and Bayesian regression for efficient evaluation of large language model instructions",
    "relevant_papers": [
      "2501.03508v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to apply sequential optimal learning and mixed-integer optimization to discover high-quality prompts for LLMs with limited evaluation budgets?",
    "relevant_papers": [
      "2501.03508v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do different floating-point quantization parameters like exponent and mantissa bits influence the scaling laws and overall performance of large language model training?",
    "relevant_papers": [
      "2501.02423v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "What is the optimal bit-width and exponent-mantissa ratio for floating-point quantization to achieve the best cost-performance trade-off in large-scale model training?",
    "relevant_papers": [
      "2501.02423v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can the concept of applied multiplexity be used to mitigate cultural bias and Western-centric perspectives in large language models for education?",
    "relevant_papers": [
      "2501.03259v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using multi-agent systems to improve cultural inclusivity and perspective distribution in generative AI tools for global educational contexts.",
    "relevant_papers": [
      "2501.03259v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve graph retrieval augmented generation by reconstructing textual context for triples and using query feedback to fill missing knowledge gaps",
    "relevant_papers": [
      "2501.15378v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "methods for mitigating information loss in knowledge graphs by restoring original text context and refining structures based on user queries in RAG systems",
    "relevant_papers": [
      "2501.15378v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "automated evaluation metrics for retrieval augmented generation systems in clinical question answering focusing on faithfulness and refusal accuracy",
    "relevant_papers": [
      "2501.08208v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to measure the conversational faithfulness and context relevance of LLM based medical question answering agents using automated frameworks",
    "relevant_papers": [
      "2501.08208v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Large language model error patterns in mathematical word problems using an automated dynamic classification framework and the MWPES-300K dataset",
    "relevant_papers": [
      "2501.15581v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving mathematical reasoning in LLMs through error-aware prompting based on systematic analysis of fine-grained error patterns in MWP datasets",
    "relevant_papers": [
      "2501.15581v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "impact of real-world spelling mistakes and wikipedia edit history noise on the performance of multilingual large language models in nlp tasks",
    "relevant_papers": [
      "2501.08322v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "comparing the robustness of mt5 and bloom models against human-made spelling errors in multilingual named entity recognition and intent classification",
    "relevant_papers": [
      "2501.08322v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "automated network configuration translation between different hardware vendors using large language model agents and intent extraction",
    "relevant_papers": [
      "2501.08760v2"
    ],
    "category": "cs.NI",
    "original_relevant_count": 1
  },
  {
    "query": "how can large language models be used to migrate legacy network configurations to new devices using intent-based intermediate representations",
    "relevant_papers": [
      "2501.08760v2"
    ],
    "category": "cs.NI",
    "original_relevant_count": 1
  },
  {
    "query": "how to model fine-grained time-dependent user interests from long-term behavior sequences in industrial streaming recommendation systems",
    "relevant_papers": [
      "2501.15817v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "using time-gap-aware attention and retrieval mechanisms to capture dynamic periodic user patterns from long-term history in streaming recommenders",
    "relevant_papers": [
      "2501.15817v1"
    ],
    "category": "cs.IR",
    "original_relevant_count": 1
  },
  {
    "query": "How to use logit-based knowledge distillation to optimize deep spiking neural networks for flexible inference timesteps without the need for retraining",
    "relevant_papers": [
      "2501.15925v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Research on distillation frameworks for deep spiking neural networks that support full-range timestep deployment and improve spatio-temporal performance",
    "relevant_papers": [
      "2501.15925v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "framework for building domain-specific AI agents using natural language standard operating procedures represented as decision graphs",
    "relevant_papers": [
      "2501.09316v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "improving long-horizon planning in LLM agents by integrating human expertise through pseudocode-style standard operating procedures",
    "relevant_papers": [
      "2501.09316v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Applying knowledge graph completion and relational graph attention networks for predicting the halal status and cultural appropriateness of cosmetic ingredients.",
    "relevant_papers": [
      "2501.05768v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Knowledge graph based framework for modeling complex relations between cosmetics and ingredients to determine halal suitability through relational graph attention.",
    "relevant_papers": [
      "2501.05768v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Automated prompt optimization techniques that use task-aware metric selection and evolutionary frameworks to improve large language model performance on specific tasks.",
    "relevant_papers": [
      "2501.06689v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement task-referenced adaptation and multi-metrics evaluation for refining prompts across diverse domains using an evolution-based optimization approach.",
    "relevant_papers": [
      "2501.06689v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "benefits of introducing positive friction in conversational agents to improve goal alignment and user critical thinking during task-oriented dialogues",
    "relevant_papers": [
      "2501.17348v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how slowing down AI interactions and adding deliberate friction can enhance machine understanding of user mental states and task success",
    "relevant_papers": [
      "2501.17348v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use llm-based line-level filtering to improve the quality of web datasets for more efficient large language model pre-training",
    "relevant_papers": [
      "2501.07314v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving training data quality through fine-grained line-level removal using deberta classifiers trained on gpt-4o mini labels",
    "relevant_papers": [
      "2501.07314v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve time series reasoning in multi-modal language models by quantizing temporal embeddings into discrete tokens for better alignment?",
    "relevant_papers": [
      "2501.07335v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Multi-modal language models for complex time series reasoning using discrete codebooks and shared embedding layers for temporal-textual data.",
    "relevant_papers": [
      "2501.07335v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can heterogeneous graph neural networks be applied to recognize personalized user emotions from multimodal social media data and dynamic context fusion?",
    "relevant_papers": [
      "2501.07746v1"
    ],
    "category": "cs.SI",
    "original_relevant_count": 1
  },
  {
    "query": "Deep learning approaches for multimodal emotion prediction in online social networks using heterogeneous graph representation learning and adaptive feature integration.",
    "relevant_papers": [
      "2501.07746v1"
    ],
    "category": "cs.SI",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve Quranic question answering using cross-language dataset augmentation and fine-tuning transformer models like RoBERTa and DeBERTa?",
    "relevant_papers": [
      "2501.17449v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Cross-language strategies for addressing linguistic disparity between Modern Standard Arabic questions and Classical Arabic Quranic verses in question answering systems.",
    "relevant_papers": [
      "2501.17449v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can sparse autoencoders be used to perform feature-level activation steering for improving the semantic consistency of large language models?",
    "relevant_papers": [
      "2501.11036v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Improving LLM response consistency for paraphrased inputs using sparse feature-level representations instead of layer-wise or attention head activation steering.",
    "relevant_papers": [
      "2501.11036v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use Partial Information Decomposition principles to quantify synergistic, redundant, and unique components of interventional causality in complex systems?",
    "relevant_papers": [
      "2501.11447v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Decomposing causal power using the M\u00f6bius function of the redundancy lattice to analyze interventional effects in transformers and chemical reaction networks.",
    "relevant_papers": [
      "2501.11447v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the reliability of membership inference attacks on large language models when using synthetic data as a non-member reference set",
    "relevant_papers": [
      "2501.11786v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "do membership inference attacks on LLMs actually detect training data or are they just identifying machine generated text patterns",
    "relevant_papers": [
      "2501.11786v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can retrieval-augmented generation and evidence-based medicine processes be integrated to improve the reasoning and trustworthiness of medical large language models?",
    "relevant_papers": [
      "2501.11885v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Advanced frameworks for medical LLMs that utilize evidence selection and reasoning to outperform standard RAG methods and fine-tuning on clinical datasets.",
    "relevant_papers": [
      "2501.11885v5"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can self-learning agents using curriculum learning principles autonomously manage microservices without needing prior configuration knowledge or static documentation?",
    "relevant_papers": [
      "2501.19056v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Using large language model agents and iterative exploration to enable autonomic self-management in complex microservice architectures like Sock Shop.",
    "relevant_papers": [
      "2501.19056v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How do multimodal large language models perform when users provide false negations to their initially correct answers across different vision-language benchmarks?",
    "relevant_papers": [
      "2501.19017v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarking the vulnerability of vision-language models to gaslighting negation attacks and user persuasion in subjective versus objective domains.",
    "relevant_papers": [
      "2501.19017v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does implementing a retrieval-augmented generation framework improve the accuracy of large language model virtual assistants in an institutional or university setting?",
    "relevant_papers": [
      "2501.13880v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the performance of retriever and generative models for building a knowledge-intensive virtual assistant using institutional document fragments and semantic search.",
    "relevant_papers": [
      "2501.13880v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "multilingual end-to-end speech recognition using mixture of experts with attention-MoE and language-routing for improved domain robustness and reduced language confusion",
    "relevant_papers": [
      "2501.12602v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve LID-based routers in MoE architectures for multilingual ASR using expert pruning and router augmentation for domain-robust performance",
    "relevant_papers": [
      "2501.12602v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to prevent explicit content generation in text-to-image diffusion models by distorting text encoder embeddings toward safe regions while maintaining image quality?",
    "relevant_papers": [
      "2501.18877v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Research on using embedding space distortion to defend against adversarial attacks and mitigate sexual content generation in Stable Diffusion and FLUX.1 models.",
    "relevant_papers": [
      "2501.18877v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "research on federated multimodal instruction tuning frameworks for fine-tuning large vision-language models across distributed devices with heterogeneous task data",
    "relevant_papers": [
      "2501.13985v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to use mixture of adapters and adaptive parameter aggregation for collaborative multimodal instruction tuning in federated learning environments",
    "relevant_papers": [
      "2501.13985v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "deep learning approaches for idiom detection in Sorani Kurdish using KuBERT and comparison with RCNN and BiLSTM models",
    "relevant_papers": [
      "2501.14528v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating the performance of transformer models for identifying figurative expressions in low-resource languages such as Kurdish",
    "relevant_papers": [
      "2501.14528v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "lightweight deep learning models for energy efficient weed detection in precision agriculture with reduced computational complexity and high mean average precision",
    "relevant_papers": [
      "2502.00205v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to achieve high accuracy weed detection on low power hardware for sustainable agriculture using models with minimal parameters and gflops",
    "relevant_papers": [
      "2502.00205v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can large multimodal language models be used to automatically generate detailed textual descriptions for technical drawings and figures in patent documents?",
    "relevant_papers": [
      "2501.15074v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Large scale dataset and specialized multimodal vision encoder for captioning and explaining the structural elements of complex patent figure illustrations.",
    "relevant_papers": [
      "2501.15074v1"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how to implement large language models for educational management in low-resource vietnamese academic institutions using synthetic data",
    "relevant_papers": [
      "2501.15022v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "frameworks for applying fine-tuned large language models to student education document processing in the vietnamese language",
    "relevant_papers": [
      "2501.15022v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can metric learning with proxy anchor methods and tri-transformer architectures improve the accuracy of multimodal fake news detection systems?",
    "relevant_papers": [
      "2501.12422v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Using BLIP-2 pre-trained encoders and cross-modal transformers to capture intra-modality relationships and inter-modal similarities for detecting fake news online",
    "relevant_papers": [
      "2501.12422v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "using low rank adaptation to finetune language models for better sparse autoencoder integration and reduced cross entropy loss during inference",
    "relevant_papers": [
      "2501.19406v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "improving the efficiency of sparse autoencoder reconstructions by applying low rank adaptation to the underlying transformer model layers",
    "relevant_papers": [
      "2501.19406v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to design fair pricing mechanisms for LLM training data that ensure long-term market sustainability and high data quality from human annotators",
    "relevant_papers": [
      "2502.00198v4"
    ],
    "category": "cs.GT",
    "original_relevant_count": 1
  },
  {
    "query": "economic frameworks for large language model data markets focusing on fair compensation for human annotators and optimizing performance-per-dollar for developers",
    "relevant_papers": [
      "2502.00198v4"
    ],
    "category": "cs.GT",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarking AI agents on scientific discovery tasks through gravitational physics simulations and experimental data collection planning",
    "relevant_papers": [
      "2501.18411v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How can autonomous agents be evaluated on their ability to discover physical laws in simulated environments with non-standard gravity?",
    "relevant_papers": [
      "2501.18411v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to use large language models to automatically synthesize heuristic functions for deterministic planning from problem definitions in general-purpose programming languages?",
    "relevant_papers": [
      "2501.18784v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Integrating LLM-generated heuristics into search algorithms like greedy best-first search for solving planning problems with complex numeric constraints.",
    "relevant_papers": [
      "2501.18784v4"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How does the length of tokenized Java code affect the accuracy of large language models like GPT-4 and Mistral in detecting software vulnerabilities?",
    "relevant_papers": [
      "2502.00064v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the impact of input context window size on the performance and explicitness of large language models for automated vulnerability detection in Java.",
    "relevant_papers": [
      "2502.00064v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "how to use large language models and retrieval augmented generation for automated g-code synthesis from natural language instructions in cnc machining",
    "relevant_papers": [
      "2501.17584v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "automated g-code generation for cnc machines using self-correcting large language models and functional correctness evaluation with hausdorff distance",
    "relevant_papers": [
      "2501.17584v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How can Large Language Models be used for semantic consistency regularization to improve performance in semi-supervised sentiment analysis with limited labeled data?",
    "relevant_papers": [
      "2501.17598v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Semi-supervised sentiment classification using entity extraction and concept-based data enhancement from LLMs for semantic reconstruction and consistency loss.",
    "relevant_papers": [
      "2501.17598v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Combining width and depth pruning strategies for efficient structured compression of large language models to maintain perplexity and performance.",
    "relevant_papers": [
      "2501.17771v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to perform two-stage structured pruning on LLMs by removing neurons and attention modules while balancing sparsity across different model components.",
    "relevant_papers": [
      "2501.17771v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve large language model logical reasoning using multi-persona debate prompting and town hall interaction on zebra logic benchmarks",
    "relevant_papers": [
      "2502.15725v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "enhancing llm problem solving through town hall style debate prompting with multiple persona types and optimal group size for reasoning tasks",
    "relevant_papers": [
      "2502.15725v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can participatory design methods be used to create large language models specifically tailored for journalists and newsroom workflows?",
    "relevant_papers": [
      "2501.17299v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Challenges and opportunities of developing a journalist-controlled large language model through co-design and interviews with news organization stakeholders.",
    "relevant_papers": [
      "2501.17299v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used as reference-aware critics to evaluate the executability and semantics of code patches without running tests?",
    "relevant_papers": [
      "2501.16655v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmarking execution-free evaluation methods for code agents using reference patches to predict build success on datasets like SWE-bench.",
    "relevant_papers": [
      "2501.16655v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Performance comparison of YOLOv7 and Faster R-CNN for vision-based real-time structural damage detection on wind turbine surface images",
    "relevant_papers": [
      "2501.16662v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can deep learning models like YOLOv7 be applied to automate the inspection and classification of surface damage in wind energy infrastructure",
    "relevant_papers": [
      "2501.16662v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How can few-shot optimization and iterative prompt engineering improve hallucination detection performance in the SHROOM SemEval-2024 shared task for resource-limited NLP?",
    "relevant_papers": [
      "2501.16616v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Fine-tuning Mistral-7B-Instruct-v0.3 for robust hallucination detection using DeepSeek-based weak label generation and data restructuring techniques in constrained environments.",
    "relevant_papers": [
      "2501.16616v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using in-context learning with transformer models to detect zero-day cyber attacks in IEC-61850 digital substations",
    "relevant_papers": [
      "2501.16453v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how can large language model architectures and in-context learning improve intrusion detection for novel attacks in smart power grids",
    "relevant_papers": [
      "2501.16453v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "using large language models and retrieval augmented generation to help users understand and navigate complex website privacy policies through interactive tools",
    "relevant_papers": [
      "2501.16033v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "evaluating how interactive browser extensions and chat interfaces powered by LLMs can improve user engagement and comprehension of online privacy documents",
    "relevant_papers": [
      "2501.16033v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "methods for 2-bit KV cache quantization in vision-language models using attention-aware saliency and Walsh-Hadamard transform to handle outliers",
    "relevant_papers": [
      "2501.15021v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to optimize vision-language model memory consumption using adaptive bit budget allocation for multimodal token key-value caches",
    "relevant_papers": [
      "2501.15021v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating random forest classifiers for the detection of Navajo and other endangered Athabaskan languages to improve representation in natural language processing tools.",
    "relevant_papers": [
      "2501.15773v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can researchers develop accurate language identification models for Native American languages that are currently unsupported by major commercial language detection systems?",
    "relevant_papers": [
      "2501.15773v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can analytical decomposition of first-layer attention weights explain detokenization in GPT-2 without performing model inference or path patching?",
    "relevant_papers": [
      "2501.15754v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Weight-based methods for analyzing how transformer models map subword tokens to an inner vocabulary during the initial stages of the inference process.",
    "relevant_papers": [
      "2501.15754v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Adaptive reward function exploration for action-level backdoor attacks in reinforcement learning across discrete and continuous action spaces using performance monitoring.",
    "relevant_papers": [
      "2501.15529v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to perform stealthy backdoor attacks in continuous reinforcement learning environments using action tampering and adaptive reward functions to improve attack success rates.",
    "relevant_papers": [
      "2501.15529v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can fine-tuned large language models like LLaMA improve text simplification for low-resource languages such as Estonian using synthetic and manual datasets?",
    "relevant_papers": [
      "2501.15624v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparative evaluation of neural machine translation and fine-tuned LLaMA models for automatic text simplification in Estonian with a focus on meaning preservation.",
    "relevant_papers": [
      "2501.15624v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Limitations and biases of automated factuality metrics in evaluating large language model outputs for summarization and retrieval augmented generation tasks",
    "relevant_papers": [
      "2501.14883v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How reliable are automated factuality evaluators at measuring system level performance in natural language generation and question answering",
    "relevant_papers": [
      "2501.14883v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to optimize kv cache storage in large language models using task-aware importance and semantic differentiation of attention heads",
    "relevant_papers": [
      "2501.15113v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "adaptive kv cache pruning techniques that distinguish between heterogeneous and non-heterogeneous attention heads for diverse natural language processing tasks",
    "relevant_papers": [
      "2501.15113v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "methods for converting dense large language models into mixture-of-experts architectures through dynamic structural pruning to reduce active parameters while maintaining performance",
    "relevant_papers": [
      "2501.15316v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to apply differentiable dynamic pruning to transform MLP layers of LLaMA and Phi models into MoE structures without permanently removing model parameters",
    "relevant_papers": [
      "2501.15316v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can diversity-based adaptive random testing using string distance metrics improve the efficiency of testing large language model prompt templates?",
    "relevant_papers": [
      "2501.13480v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Effective test selection and prioritization strategies for LLM applications using adaptive random testing to maximize failure discovery with limited budgets.",
    "relevant_papers": [
      "2501.13480v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve retrieval-augmented medical question answering by using a compressor that generates prior knowledge before context compression to reduce hallucinations?",
    "relevant_papers": [
      "2501.13567v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Methods for improving the accuracy of medical QA systems using autoregressive compression of retrieved passages guided by automatically generated domain-specific prior knowledge.",
    "relevant_papers": [
      "2501.13567v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "A unified framework for general mobility trajectory modeling using masked conditional diffusion to address generation, recovery, and prediction tasks.",
    "relevant_papers": [
      "2501.13347v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can masked conditional diffusion models with contextual trajectory embeddings improve performance across diverse mobility data tasks like generation and recovery?",
    "relevant_papers": [
      "2501.13347v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used for few-shot harmful content moderation on social media platforms compared to OpenAI Moderation and Perspective APIs?",
    "relevant_papers": [
      "2501.13976v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the effectiveness of multimodal in-context learning and large language models for scalable and dynamic moderation of violent or dangerous content online.",
    "relevant_papers": [
      "2501.13976v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve perceptual consistency and visual sharpness in deep learning edge detection using symmetric weighted binary cross-entropy loss?",
    "relevant_papers": [
      "2501.13365v4"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Perception-inspired loss functions for neural edge detection that model human perceptual asymmetry to balance edge recall and false positive suppression.",
    "relevant_papers": [
      "2501.13365v4"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "empirical study evaluating the performance of deep learning models for detecting inconsistent method names using realistic benchmarks and developer-verified commit histories",
    "relevant_papers": [
      "2501.12617v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "challenges and performance degradation of deep learning approaches for identifying method name inconsistencies when applied to imbalanced real-world software datasets",
    "relevant_papers": [
      "2501.12617v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used to develop autonomous bargaining agents for individual sellers on C2C second-hand e-commerce platforms?",
    "relevant_papers": [
      "2502.10406v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "Large language model based proactive dialogue systems for price negotiation and bargaining strategies in online fleamarket marketplaces like Xianyu.",
    "relevant_papers": [
      "2502.10406v1"
    ],
    "category": "cs.CY",
    "original_relevant_count": 1
  },
  {
    "query": "relationship between Ehrenfeucht-Haussler rank of Boolean functions and the number of Chain of Thought steps in single-layer Transformers",
    "relevant_papers": [
      "2501.12997v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how many Chain of Thought steps are required for a single-layer Transformer to compute functions with a specific Ehrenfeucht-Haussler rank",
    "relevant_papers": [
      "2501.12997v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "A comprehensive analysis of statistical methodology errors in empirical software engineering research and the proficiency of experts in identifying these issues",
    "relevant_papers": [
      "2501.12728v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating the prevalence of inadequate data analysis techniques in software engineering studies using automated classification and expert workshop evaluations",
    "relevant_papers": [
      "2501.12728v1"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How can selective boosting of attention weights for local and summary visual tokens reduce object hallucinations in large vision-language models?",
    "relevant_papers": [
      "2501.12206v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Mitigating hallucinations in LVLMs by intervening in self-attention mechanisms to increase weights for specific visual tokens that encode object details and image summaries.",
    "relevant_papers": [
      "2501.12206v3"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "new optimization algorithms for large language model pre-training that outperform Adam by handling gradient noise in narrowing valley loss landscapes",
    "relevant_papers": [
      "2501.12243v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "improving Signum optimizer stability for GPT-2 training by incorporating attraction toward moving averaged parameters to handle high gradient query noise",
    "relevant_papers": [
      "2501.12243v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to implement adaptive PII mitigation and policy-driven masking in large language models to comply with diverse regulatory frameworks like GDPR and CCPA?",
    "relevant_papers": [
      "2501.12465v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Advanced NLP techniques for context-aware analysis and anonymization of sensitive personal information in machine learning models compared to tools like Microsoft Presidio.",
    "relevant_papers": [
      "2501.12465v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can node influence maximization and decoupled influence propagation be used to improve the scalability of machine unlearning on large-scale graphs?",
    "relevant_papers": [
      "2501.11823v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "A scalable graph unlearning framework that uses influence functions to identify affected nodes and balances model reasoning with data forgetting performance.",
    "relevant_papers": [
      "2501.11823v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "understanding the sequential learning of skills in neural networks using physics-based abstraction models like the geometry and resource models",
    "relevant_papers": [
      "2501.12391v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "research on the domino effect in deep learning training and how toy models explain the emergence of chinchilla scaling laws",
    "relevant_papers": [
      "2501.12391v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large vision-language models be used to identify inconsistencies between news images and textual entities like persons and locations?",
    "relevant_papers": [
      "2501.11403v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Using large vision-language models and reference images to verify cross-modal consistency of events and entities in news for disinformation detection.",
    "relevant_papers": [
      "2501.11403v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use retrieval-augmented generation and large language models for geocoding location coordinates from news reports for disaster management?",
    "relevant_papers": [
      "2501.11440v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Open source RAG-based systems for automatic extraction of precise geographic coordinates from unstructured news text using location databases and LLMs.",
    "relevant_papers": [
      "2501.11440v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Benchmark datasets for Norwegian question answering evaluating large language models on world knowledge and commonsense reasoning in Bokm\u00e5l and Nynorsk",
    "relevant_papers": [
      "2501.11128v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Performance evaluation of language models on new Norwegian question answering datasets focusing on truthfulness, commonsense reasoning, and regional knowledge",
    "relevant_papers": [
      "2501.11128v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How well does GPT-4o understand the ironic use of emojis compared to human social media users across different age and gender demographics?",
    "relevant_papers": [
      "2501.11241v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparative study of large language models and human perception regarding the interpretation of sarcasm and irony in digital emoji usage.",
    "relevant_papers": [
      "2501.11241v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "improving direct preference optimization for large language models by using a focal loss mechanism to prioritize training on correctly ranked preference pairs",
    "relevant_papers": [
      "2501.06645v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "alternative DPO methods that down-weight misranked samples and enhance model performance by focusing on preference pairs that the language model already identifies correctly",
    "relevant_papers": [
      "2501.06645v3"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does pyramid-descent visual position encoding enhance multi-granularity perception and mitigate the limitations of traditional raster-scan methods in vision-language models?",
    "relevant_papers": [
      "2501.10967v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Improving vision-language model performance by using peripheral-to-central visual position indexing to reduce relative distances between interrelated visual and instruction tokens.",
    "relevant_papers": [
      "2501.10967v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "Empirical study analyzing gender and ethnicity bias in Stable Diffusion versions 2, XL, and 3 when generating images for software engineering roles",
    "relevant_papers": [
      "2501.09014v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How do text-to-image generative models like Stable Diffusion represent diversity in software engineering tasks across different model versions?",
    "relevant_papers": [
      "2501.09014v2"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "How can 4-bit quantization reduce memory storage and speed up vector search in retrieval augmented generation systems for large language models?",
    "relevant_papers": [
      "2501.10534v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Using low-precision 4-bit integer quantization to optimize the memory footprint and search speed of high-dimensional vector embeddings in RAG frameworks.",
    "relevant_papers": [
      "2501.10534v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Effective multi-stage training strategies for bilingual neural passage retrieval models in the Islamic domain using Arabic and English corpora",
    "relevant_papers": [
      "2501.10175v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Developing a lightweight bilingual Islamic LLM for document retrieval using language reduction techniques and domain-specific data augmentation",
    "relevant_papers": [
      "2501.10175v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "What are the results of a thematic analysis regarding the advantages and pitfalls of using large language models like ChatGPT in educational settings?",
    "relevant_papers": [
      "2501.10134v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Qualitative research on the ethical integration of generative AI in education based on thematic analysis of essays from academic professionals.",
    "relevant_papers": [
      "2501.10134v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "efficient structured pruning techniques for large language models that remove interconnected columns and rows across layers while maintaining model accuracy through weight restoration",
    "relevant_papers": [
      "2501.09412v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "how to prune large language models quickly using structured methods inspired by Wanda to reduce memory and computational demands on consumer GPUs",
    "relevant_papers": [
      "2501.09412v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can generative AI and quantum computing be integrated into a semantic communication testbed for 6G network performance evaluation and goal-oriented decoding?",
    "relevant_papers": [
      "2501.09918v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Prototyping platform and dataset for 6G semantic communication using noise-augmented synthetic data for classification, localization, and edge-based language inference tasks.",
    "relevant_papers": [
      "2501.09918v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve document-level machine translation consistency and fluency using incremental sentence-level forced decoding and summary-based memory",
    "relevant_papers": [
      "2501.08523v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "using doc-guided memory and agent-based approaches to ensure sentence completeness and document-level coherence in neural machine translation",
    "relevant_papers": [
      "2501.08523v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How does instruction tuning affect the fundamental task capabilities of large language models compared to in-context learning from pretraining data?",
    "relevant_papers": [
      "2501.08716v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Correlation between instruction-tuned performance and base model in-context learning capabilities in large language models across different scales and tasks.",
    "relevant_papers": [
      "2501.08716v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "multi-modal large language models for single-cell rna sequencing analysis using natural language instructions and instruction following",
    "relevant_papers": [
      "2501.08187v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "ai models for single-cell analysis capable of cell type annotation and drug sensitivity prediction through natural language commands",
    "relevant_papers": [
      "2501.08187v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "natural language interface for optimization models using large language models to help practitioners with infeasibility diagnosis and sensitivity analysis",
    "relevant_papers": [
      "2501.08406v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "how can large language models be used to provide counterfactual explanations and model interpretations for non-expert users of mathematical optimization",
    "relevant_papers": [
      "2501.08406v2"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "How to use Large Language Models as an action selection filter in reinforcement learning for personalized health adaptive interventions based on text user preferences?",
    "relevant_papers": [
      "2501.06980v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "A hybrid approach combining large language model decision making with reinforcement learning policies to incorporate real-time user preference and behavioral constraints in adaptive health interventions.",
    "relevant_papers": [
      "2501.06980v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Integrating large language models into hierarchical planning systems through a taxonomy of methods and life cycle analysis",
    "relevant_papers": [
      "2501.08068v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Standardized benchmarks and datasets for evaluating the performance of large language models in automated hierarchical planning tasks",
    "relevant_papers": [
      "2501.08068v2"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How to improve the reasoning capabilities of mixture of experts networks using a self rethinking mechanism and recurrent routing strategies",
    "relevant_papers": [
      "2501.07890v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Enhancing cognitive depth in language models by facilitating information flow between experts through pseudo graph networks and recurrent routing",
    "relevant_papers": [
      "2501.07890v4"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Norwegian abstractive summarization dataset for benchmarking generative language models with human-authored gold standard summaries in Bokm\u00e5l and Nynorsk",
    "relevant_papers": [
      "2501.07718v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to evaluate the performance of Norwegian large language models on news summarization using high-quality human-written reference datasets",
    "relevant_papers": [
      "2501.07718v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to apply denoise diffusion models and stochastic differential equations to improve signal detection performance compared to traditional maximum likelihood estimation?",
    "relevant_papers": [
      "2501.07030v1"
    ],
    "category": "eess.SY",
    "original_relevant_count": 1
  },
  {
    "query": "Diffusion transformer based signal detection methods for reducing symbol error rate in BPSK and QAM modulation across various signal-to-noise ratios.",
    "relevant_papers": [
      "2501.07030v1"
    ],
    "category": "eess.SY",
    "original_relevant_count": 1
  },
  {
    "query": "efficient elastic quantization framework for deploying large language models on edge devices with dynamic unified memory constraints and high storage efficiency",
    "relevant_papers": [
      "2501.07139v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "how to improve memory elasticity and transition granularity for locally hosted LLMs on edge devices using ensemble quantized models and pruning",
    "relevant_papers": [
      "2501.07139v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "How does the magnitude of enriched categories of texts relate to Tsallis and Shannon entropies in language model probability distributions?",
    "relevant_papers": [
      "2501.06662v2"
    ],
    "category": "math.CT",
    "original_relevant_count": 1
  },
  {
    "query": "Mathematical framework for computing magnitude homology and Euler characteristics of text categories enriched by next-token probability distributions from language models.",
    "relevant_papers": [
      "2501.06662v2"
    ],
    "category": "math.CT",
    "original_relevant_count": 1
  },
  {
    "query": "memory efficient on-FPGA training of transformer models using low-rank tensor compression and bi-directional contraction flow",
    "relevant_papers": [
      "2501.06663v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "hardware accelerator design for end-to-end transformer training on FPGAs using tensorized optimization to minimize on-chip memory and energy costs",
    "relevant_papers": [
      "2501.06663v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used to generate synthetic sentences for debiasing pre-trained language models while applying causal analysis to ensure positive transfer?",
    "relevant_papers": [
      "2501.06795v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Effective methods for reducing gender bias in pre-trained models using balanced sentences generated by LLMs and filtering techniques based on causal effect estimation.",
    "relevant_papers": [
      "2501.06795v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Orchestration framework for large language model training using sharing protocols and agent optimal path modules for resource optimization.",
    "relevant_papers": [
      "2501.06471v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Implementing joint mining mechanisms and bilateral value sharing to optimize computational resource costs during large language model training and development.",
    "relevant_papers": [
      "2501.06471v1"
    ],
    "category": "cs.AI",
    "original_relevant_count": 1
  },
  {
    "query": "Attention-based deep learning framework for interpretable enzyme function prediction using learnable queries to detect functional local residue regions",
    "relevant_papers": [
      "2501.05644v2"
    ],
    "category": "q-bio.BM",
    "original_relevant_count": 1
  },
  {
    "query": "How can Transformer architectures be used to predict multiple Enzyme Commission numbers by identifying specific residue fragments in protein sequences?",
    "relevant_papers": [
      "2501.05644v2"
    ],
    "category": "q-bio.BM",
    "original_relevant_count": 1
  },
  {
    "query": "How to combine BERT sentence embeddings and TF-IDF features using an ensemble approach for detecting plagiarism in Marathi language texts?",
    "relevant_papers": [
      "2501.05260v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Effective methods for Marathi plagiarism detection using a weighted ensemble of machine learning models with semantic BERT and statistical TF-IDF representations.",
    "relevant_papers": [
      "2501.05260v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "L2 convergence rates and stability of linear Q-learning with function approximation using epsilon-softmax behavior policy and adaptive temperature",
    "relevant_papers": [
      "2501.19254v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "stochastic approximation analysis of linear Q-learning divergence issues under Markovian noise without Bellman completeness or near-optimality assumptions",
    "relevant_papers": [
      "2501.19254v4"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used for reference-free and multi-dimensional evaluation of counterspeech to better align with human judgment?",
    "relevant_papers": [
      "2501.17581v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Automated metrics for counterspeech generation evaluation using chain-of-thought prompting and auto-calibration to measure coherence and aggressiveness.",
    "relevant_papers": [
      "2501.17581v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to use large language models for active knowledge retrieval and data alignment to improve performance in transfer learning across different domains?",
    "relevant_papers": [
      "2501.17802v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "LLM-enhanced knowledge augmentation methods that integrate external data libraries with target domain information using feature space and probability measure harmonization.",
    "relevant_papers": [
      "2501.17802v3"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How do cognitive forcing functions and different explanation types affect human-AI collaborative performance and trust in high-stakes decision-making scenarios?",
    "relevant_papers": [
      "2501.16627v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "Comparing the impact of visual explanations and cognitive forcing functions on automation bias and user engagement in human-AI decision support systems.",
    "relevant_papers": [
      "2501.16627v1"
    ],
    "category": "cs.HC",
    "original_relevant_count": 1
  },
  {
    "query": "neurosymbolic knowledge base for environmental social and governance analysis using concept parsing and semi-supervised label propagation techniques",
    "relevant_papers": [
      "2501.15720v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to extract actionable sustainability information from corporate disclosures using a structured knowledge base of triplets and hierarchical taxonomies",
    "relevant_papers": [
      "2501.15720v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can large language models be used as proxies to reduce the cognitive load and communication requirements of preference elicitation in combinatorial auctions?",
    "relevant_papers": [
      "2501.14625v2"
    ],
    "category": "cs.GT",
    "original_relevant_count": 1
  },
  {
    "query": "Using LLM-based pipelines and DNF proper learning to accelerate preference elicitation for efficient resource allocation in natural language communication sandboxes.",
    "relevant_papers": [
      "2501.14625v2"
    ],
    "category": "cs.GT",
    "original_relevant_count": 1
  },
  {
    "query": "improving best-of-n sampling in large language models using pairwise judge reward models and knockout tournaments for mathematical reasoning",
    "relevant_papers": [
      "2501.13007v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "how to use pairwise comparison and chain of thought reasoning in reward models for better candidate selection in math problems",
    "relevant_papers": [
      "2501.13007v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can masking high perplexity tokens in training data help mitigate catastrophic forgetting and improve cross-domain generalization during LLM fine-tuning?",
    "relevant_papers": [
      "2501.14315v7"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "The effect of LLM-generated synthetic data on reducing performance degradation in non-target tasks compared to fine-tuning with ground truth human data.",
    "relevant_papers": [
      "2501.14315v7"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to identify both latent driving forces and direct causal relations among observed variables in climate time-series using causal representation learning?",
    "relevant_papers": [
      "2501.12500v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Causal discovery and representation learning framework for uncovering hidden dynamic processes and observable causal structures from non-parametric temporal data.",
    "relevant_papers": [
      "2501.12500v2"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How can debate and scalable oversight be used to improve weak-to-strong generalization for aligning future superhuman artificial intelligence models?",
    "relevant_papers": [
      "2501.13124v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Can a weak model extract trustworthy information from a strong model through debate to provide better supervision for strong model alignment?",
    "relevant_papers": [
      "2501.13124v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can zero-knowledge proofs be used to verify the effectiveness and compatibility of LoRA weights without exposing proprietary model parameters to the user?",
    "relevant_papers": [
      "2501.13965v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "Privacy-preserving verification techniques for low-rank adaptation weights in distributed machine learning using multi-party inference and succinct cryptographic proofs.",
    "relevant_papers": [
      "2501.13965v1"
    ],
    "category": "cs.CR",
    "original_relevant_count": 1
  },
  {
    "query": "How do binary decision biases and sampling methods in large language models affect the accuracy of agent-based financial market simulations?",
    "relevant_papers": [
      "2501.16356v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "Investigating randomness and decision-making biases in GPT models for fair integration into financial agent-based modeling and simulations.",
    "relevant_papers": [
      "2501.16356v1"
    ],
    "category": "cs.LG",
    "original_relevant_count": 1
  },
  {
    "query": "How to measure conditional feature importance using adversarial random forests for generative modeling on tabular datasets with mixed feature types",
    "relevant_papers": [
      "2501.11178v1"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "Explainable AI methods for estimating on-manifold conditional feature importance through distributions learned by adversarial random forest generative models",
    "relevant_papers": [
      "2501.11178v1"
    ],
    "category": "stat.ML",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking vision language model safety using a dataset of multimodal prompts where harmful intent is only clear when combining text and images",
    "relevant_papers": [
      "2501.10057v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "multilingual evaluation of multimodal safety in vision language models across fine grained hazard categories like self harm and drug use",
    "relevant_papers": [
      "2501.10057v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to optimize large scale machine learning training pipelines for ads recommendation models using shared input generation and TPU acceleration",
    "relevant_papers": [
      "2501.10546v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "Techniques for improving embedding table lookups and handling training preemptions in production deep learning systems for advertising and auction scoring",
    "relevant_papers": [
      "2501.10546v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "How does language-adaptive fine-tuning with the AfriBERTa model improve sentiment analysis performance for low-resource languages such as Hausa?",
    "relevant_papers": [
      "2501.11023v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating the effectiveness of language-adaptive fine-tuning and the NaijaSenti dataset for sentiment classification tasks in the Hausa language.",
    "relevant_papers": [
      "2501.11023v1"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Methods for adapting decoder-only large language models to perform bidirectional representation learning and text infilling through unified training objectives.",
    "relevant_papers": [
      "2501.08648v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How can combining bidirectional and causal attention in a generative decoder improve token-level representations and context-aware text generation?",
    "relevant_papers": [
      "2501.08648v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating how large language models like Llama and Claude moderate negative emotions and maintain semantic consistency in social media response and continuation tasks.",
    "relevant_papers": [
      "2501.08102v6"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Comparison of emotional intensity and semantic coherence between human-authored and LLM-generated content in climate change discussions on Twitter and Reddit.",
    "relevant_papers": [
      "2501.08102v6"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models exhibit systematic provider bias when recommending cloud services in code generation tasks?",
    "relevant_papers": [
      "2501.07849v3"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "Empirical study and dataset for evaluating service provider preferences and digital monopoly risks in LLM-assisted programming.",
    "relevant_papers": [
      "2501.07849v3"
    ],
    "category": "cs.SE",
    "original_relevant_count": 1
  },
  {
    "query": "benchmarking vision language models for error detection and correction in handwritten student mathematical responses using the FERMAT dataset",
    "relevant_papers": [
      "2501.07244v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "how well do current vision language models perform at evaluating and reasoning over handwritten math problems compared to printed text",
    "relevant_papers": [
      "2501.07244v2"
    ],
    "category": "cs.CV",
    "original_relevant_count": 1
  },
  {
    "query": "How do large language models perform on recalling notable global and regional events across different time periods and multiple languages?",
    "relevant_papers": [
      "2501.07482v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Evaluating geographic disparities and socioeconomic correlations in large language model knowledge retrieval for worldwide events over the last decade.",
    "relevant_papers": [
      "2501.07482v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "Automated pipeline for converting general Word documents into presentation slides including AI-generated speech delivery and narrative synthesis.",
    "relevant_papers": [
      "2501.06497v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "How to evaluate the quality of AI-generated presentations using LLM-based metrics focusing on content relevance, coherence, and redundancy.",
    "relevant_papers": [
      "2501.06497v2"
    ],
    "category": "cs.CL",
    "original_relevant_count": 1
  },
  {
    "query": "trends in the use of causal inference methods and their impact on publication success in economics working papers since 1980",
    "relevant_papers": [
      "2501.06873v1"
    ],
    "category": "econ.GN",
    "original_relevant_count": 1
  },
  {
    "query": "how does causal narrative complexity and novelty in economic research affect citations and acceptance in top tier academic journals",
    "relevant_papers": [
      "2501.06873v1"
    ],
    "category": "econ.GN",
    "original_relevant_count": 1
  },
  {
    "query": "optimizing Mixture-of-Experts model inference on serverless platforms using Bayesian optimization to reduce costs and communication bottlenecks",
    "relevant_papers": [
      "2501.05313v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  },
  {
    "query": "distributed deployment of MoE models in serverless computing with expert popularity prediction and pipelined scatter-gather communication strategies",
    "relevant_papers": [
      "2501.05313v1"
    ],
    "category": "cs.DC",
    "original_relevant_count": 1
  }
]